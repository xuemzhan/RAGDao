æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†ï¼Œå¥½æ¯”åœ¨çƒ¹é¥ªå‰â€œæ¸…æ´—å’Œå¤„ç†è”¬èœâ€ã€‚å…¶ç›®æ ‡æ˜¯æ¶ˆé™¤åŸå§‹æ•°æ®ä¸­çš„â€œæ³¥åœŸå’Œæ‚è´¨â€ï¼ˆå™ªéŸ³ï¼‰ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºä¸€ç§æ ‡å‡†åŒ–çš„ã€æœºå™¨æ˜“äºâ€œæ¶ˆåŒ–â€çš„æ ¼å¼ã€‚åœ¨RAGä¸­ï¼Œâ€œå™ªéŸ³â€ä¸ä»…æŒ‡æ— ç”¨çš„å­—ç¬¦æˆ–æ ¼å¼ï¼Œæ›´åŒ…æ‹¬ä»»ä½•å¯èƒ½è¯¯å¯¼åµŒå…¥æ¨¡å‹è¿›è¡Œè¯­ä¹‰ç†è§£çš„å› ç´ ã€‚åµŒå…¥æ¨¡å‹ä¼šå°†æ–‡æœ¬æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´ï¼Œæ–‡æœ¬ä¸­å¾®å°çš„ã€æ— æ„ä¹‰çš„å·®å¼‚éƒ½å¯èƒ½å¯¼è‡´å‘é‡ä½ç½®çš„å·¨å¤§å˜åŒ–ï¼Œä»è€Œå½±å“æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚

> æ–‡æœ¬é¢„å¤„ç†ï¼ˆText Preprocessingï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€ä¸ªåŸºç¡€ä¸”å…³é”®çš„æ­¥éª¤ã€‚å…¶æ¦‚å¿µå’Œæ–¹æ³•åœ¨ä¸»æµNLPæ•™ç§‘ä¹¦ä¸­æœ‰è¯¦ç»†è®ºè¿°ï¼Œå¦‚Dan Jurafskyå’ŒJames H. Martinæ‰€è‘—çš„ã€ŠSpeech and Language Processingã€‹
> 

## **åˆ é™¤é‡å¤å†…å®¹ (Deduplication)**

é‡å¤å†…å®¹æ˜¯çŸ¥è¯†åº“çš„â€œè„‚è‚ªâ€ï¼Œä¸ä»…å¢åŠ äº†å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼Œæ›´ä¸¥é‡çš„æ˜¯å®ƒä¼šâ€œç¨€é‡Šâ€æ£€ç´¢ç»“æœã€‚å½“ç”¨æˆ·æŸ¥è¯¢æ—¶ï¼Œå¦‚æœæ£€ç´¢å™¨è¿”å›äº†ä¸‰ä»½å†…å®¹å®Œå…¨ç›¸åŒçš„æ–‡æ¡£ï¼Œè¿™ä¸ä»…å ç”¨äº†å®è´µçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œè¿˜å¯èƒ½è®©LLMåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶äº§ç”Ÿä¸å¿…è¦çš„é‡å¤ã€‚é‡å¤åˆ†ä¸ºä¸¤ç§ï¼š

- **å®Œå…¨é‡å¤ï¼š**Â æ–‡ä»¶æˆ–æ–‡æœ¬å†…å®¹ä¸€å­—ä¸å·®ã€‚
- **è¿‘ä¼¼/è¯­ä¹‰é‡å¤ï¼š**Â å†…å®¹é«˜åº¦ç›¸ä¼¼ï¼Œä¾‹å¦‚åŒä¸€ç¯‡æ–°é—»ç¨¿çš„ä¸åŒç‰ˆæœ¬ã€åŒä¸€äº§å“çš„ä¸åŒè§’åº¦æè¿°ç­‰ã€‚

### **æ–¹æ³•ä¸ç¤ºä¾‹ï¼š**

- **å®Œå…¨é‡å¤æ£€æµ‹ï¼š**Â ä½¿ç”¨å“ˆå¸Œç®—æ³•ï¼ˆå¦‚MD5, SHA256ï¼‰å¯¹æ–‡æœ¬å†…å®¹è®¡ç®—å“ˆå¸Œå€¼ã€‚å“ˆå¸Œå€¼ç›¸åŒåˆ™è§†ä¸ºå®Œå…¨é‡å¤ã€‚è¿™ç§æ–¹æ³•é€Ÿåº¦æå¿«ï¼Œé€‚åˆåˆæ­¥ç­›é€‰ã€‚
- **è¿‘ä¼¼é‡å¤æ£€æµ‹ï¼š**
    - **ç±»æ¯”ï¼š**Â è¿™å°±åƒæ˜¯ç»™æ¯ç¯‡æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªç‹¬ç‰¹çš„â€œè¯­ä¹‰æŒ‡çº¹â€ã€‚å¦‚æœä¸¤æšæŒ‡çº¹é«˜åº¦ç›¸ä¼¼ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºè¿™ä¸¤ç¯‡æ–‡æ¡£å†…å®¹ç›¸è¿‘ã€‚
    - **æ–¹æ³•ï¼š**Â å¯¹æ–‡æ¡£æˆ–å¤§å—æ–‡æœ¬è®¡ç®—åµŒå…¥å‘é‡ï¼Œç„¶åé€šè¿‡è®¡ç®—å‘é‡é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ¥åˆ¤æ–­å…¶å†…å®¹çš„ç›¸ä¼¼ç¨‹åº¦ã€‚è®¾å®šä¸€ä¸ªé˜ˆå€¼ï¼ˆä¾‹å¦‚0.98ï¼‰ï¼Œè¶…è¿‡è¯¥é˜ˆå€¼åˆ™è§†ä¸ºè¿‘ä¼¼é‡å¤ã€‚å¯¹äºæµ·é‡æ•°æ®ï¼Œä¸¤ä¸¤æ¯”è¾ƒï¼ˆO(nÂ²)ï¼‰æˆæœ¬è¿‡é«˜ï¼Œé€šå¸¸ä¼šé‡‡ç”¨ **MinHash + LSH (å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ)**ç­‰æŠ€æœ¯å…ˆè¿›è¡Œå¿«é€Ÿåˆ†ç»„ï¼Œå†åœ¨å°ç»„å†…è¿›è¡Œç²¾ç¡®æ¯”è¾ƒã€‚
- **å¯æ‰§è¡Œä»£ç ç¤ºä¾‹ (è¿‘ä¼¼é‡å¤æ£€æµ‹):**

```python
# å‡†å¤‡ç¯å¢ƒ:
# pip install sentence-transformers torch

from sentence_transformers import SentenceTransformer, util
import torch

# 1. åŠ è½½ä¸€ä¸ªé«˜æ•ˆçš„é¢„è®­ç»ƒåµŒå…¥æ¨¡å‹
# all-MiniLM-L6-v2 æ˜¯ä¸€ä¸ªåœ¨æ€§èƒ½å’Œé€Ÿåº¦ä¸Šå–å¾—è‰¯å¥½å¹³è¡¡çš„æµè¡Œæ¨¡å‹ã€‚
print("Loading embedding model...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("Model loaded.")

# 2. ç¤ºä¾‹æ–‡æ¡£åˆ—è¡¨ - ä½¿ç”¨2025å¹´çƒ­ç‚¹è¯é¢˜
documents = [
    "ChatGPTæ¨å‡ºäº†å…¨æ–°çš„AIåŠ©æ‰‹åŠŸèƒ½ï¼Œå¯ä»¥å¤„ç†å¤šæ¨¡æ€è¾“å…¥ã€‚",  # æ–‡æ¡£0
    "OpenAIå‘å¸ƒChatGPTæœ€æ–°ç‰ˆæœ¬ï¼Œæ”¯æŒå›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬çš„ç»¼åˆå¤„ç†ã€‚",  # æ–‡æ¡£1 (ä¸0è¯­ä¹‰é‡å¤)
    "ä»Šæ—¥åŒ—äº¬å¤©æ°”æ™´æœ—ï¼Œæ°”æ¸©é€‚å®œã€‚",  # æ–‡æ¡£2
    "äººå·¥æ™ºèƒ½é¢†åŸŸçš„æŠ•èµ„åœ¨2025å¹´åˆ›ä¸‹æ–°é«˜ã€‚",  # æ–‡æ¡£3 (ä¸»é¢˜ç›¸å…³ä½†å†…å®¹ä¸åŒ)
    "OpenAIçš„ChatGPTæ–°å¢å¤šæ¨¡æ€åŠŸèƒ½ï¼Œå¯åŒæ—¶ç†è§£æ–‡å­—å’Œå›¾åƒå†…å®¹ã€‚",  # æ–‡æ¡£4 (ä¸0, 1è¯­ä¹‰é‡å¤)
    "ç‰¹æ–¯æ‹‰å®£å¸ƒå…¶è‡ªåŠ¨é©¾é©¶æŠ€æœ¯å–å¾—é‡å¤§çªç ´ï¼ŒFSDç³»ç»Ÿå‡†ç¡®ç‡æå‡è‡³99.9%ã€‚",  # æ–‡æ¡£5
    "Elon Muskè¡¨ç¤ºç‰¹æ–¯æ‹‰çš„å®Œå…¨è‡ªåŠ¨é©¾é©¶åŠŸèƒ½å·²æ¥è¿‘å®Œç¾ï¼Œè¯†åˆ«å‡†ç¡®ç‡è¾¾åˆ°99.9%ã€‚",  # æ–‡æ¡£6 (ä¸5è¯­ä¹‰é‡å¤)
    "æ¯”ç‰¹å¸ä»·æ ¼çªç ´12ä¸‡ç¾å…ƒå¤§å…³ï¼Œåˆ›å†å²æ–°é«˜ã€‚",  # æ–‡æ¡£7
    "åŠ å¯†è´§å¸å¸‚åœºè¿æ¥ç‰›å¸‚ï¼ŒBTCé¦–æ¬¡çªç ´120,000ç¾å…ƒã€‚"  # æ–‡æ¡£8 (ä¸7è¯­ä¹‰é‡å¤)
]

# 3. è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„åµŒå…¥å‘é‡
# .encode() æ–¹æ³•å°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸ºä¸€ä¸ªå¼ é‡(tensor)ï¼Œå…¶ä¸­æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ–‡æ¡£çš„å‘é‡è¡¨ç¤ºã€‚
print("Encoding documents...")
embeddings = model.encode(documents, convert_to_tensor=True)
print("Encoding complete. Shape of embeddings:", embeddings.shape)

# 4. ä½¿ç”¨ util.cos_sim è®¡ç®—æ‰€æœ‰å‘é‡å¯¹ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
# è¿™ä¼šè¿”å›ä¸€ä¸ª N x N çš„çŸ©é˜µï¼Œå…¶ä¸­ matrix[i][j] æ˜¯æ–‡æ¡£iå’Œæ–‡æ¡£jçš„ç›¸ä¼¼åº¦ã€‚
cosine_scores = util.cos_sim(embeddings, embeddings)

# 5. æ‰¾å‡ºå¹¶æ ‡è®°è¿‘ä¼¼é‡å¤çš„æ–‡æ¡£
threshold = 0.95  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´
duplicate_indices = set() # ä½¿ç”¨é›†åˆæ¥å­˜å‚¨å¾…åˆ é™¤çš„é‡å¤æ–‡æ¡£ç´¢å¼•ï¼Œé¿å…é‡å¤æ·»åŠ 

print(f"\nFinding duplicates with threshold > {threshold}...")
# æˆ‘ä»¬éå†ç›¸ä¼¼åº¦çŸ©é˜µçš„ä¸ŠåŠéƒ¨åˆ† (ä¸åŒ…æ‹¬å¯¹è§’çº¿)
for i in range(len(documents)):
    for j in range(i + 1, len(documents)):
        # å¦‚æœä¸¤ä¸ªä¸åŒæ–‡æ¡£çš„ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼
        if cosine_scores[i][j] > threshold:
            print(f"Found duplicate: Doc {i} and Doc {j} (Score: {cosine_scores[i][j]:.4f})")
            # å°†ç´¢å¼•jçš„æ–‡æ¡£æ ‡è®°ä¸ºé‡å¤é¡¹ (ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„)
            duplicate_indices.add(j)

# 6. åˆ›å»ºä¸€ä¸ªå»é‡åçš„æ–‡æ¡£åˆ—è¡¨
deduplicated_documents = [doc for i, doc in enumerate(documents) if i not in duplicate_indices]

print("\nOriginal Documents:")
for i, doc in enumerate(documents):
    print(f"{i}: {doc}")

print("\nDeduplicated Documents:")
for i, doc in enumerate(deduplicated_documents):
    print(f"- {doc}")
```

### **å¯èƒ½ç¢°åˆ°çš„é—®é¢˜ï¼š**

- **é˜ˆå€¼é€‰æ‹©çš„è‰ºæœ¯ï¼š**Â è¿‘ä¼¼é‡å¤çš„ç›¸ä¼¼åº¦é˜ˆå€¼éš¾ä»¥ä¸€æ¦‚è€Œè®ºã€‚å¤ªé«˜ï¼Œä¼šæ¼æ‰å¾ˆå¤šè¯­ä¹‰é‡å¤çš„å†…å®¹ï¼›å¤ªä½ï¼Œåˆ™å¯èƒ½å°†ä¸»é¢˜ç›¸å…³ä½†å†…å®¹ä¸åŒçš„æ–‡æ¡£è¯¯åˆ¤ä¸ºé‡å¤ã€‚é€šå¸¸éœ€è¦ä»ä¸€ä¸ªè¾ƒé«˜çš„å€¼ï¼ˆå¦‚0.98ï¼‰å¼€å§‹ï¼Œé€šè¿‡äººå·¥æŠ½æŸ¥è¢«åˆ¤ä¸ºé‡å¤çš„æ ·æœ¬æ¥é€æ­¥é™ä½å’Œè°ƒæ•´ã€‚

## **æ ‡å‡†åŒ–æ–‡æœ¬æ ¼å¼ (Text Normalization)**

è¿™æ˜¯ä¸ºäº†æ¶ˆé™¤å› æ ¼å¼é—®é¢˜å¯¼è‡´çš„è¯­ä¹‰è¯¯è§£ã€‚å¯¹äºåµŒå…¥æ¨¡å‹æ¥è¯´ï¼Œ"RAG",Â "rag",Â "<p>R.A.G.</p>"Â å¯èƒ½è¢«ç¼–ç æˆè·ç¦»å¾ˆè¿œçš„å‘é‡ï¼Œå°½ç®¡å®ƒä»¬æŒ‡å‘åŒä¸€ä¸ªæ¦‚å¿µã€‚

- **å¤šè¯­è¨€ç¯å¢ƒçš„è€ƒé‡ï¼š**
    - **æŒ‘æˆ˜ï¼š**Â åœ¨å¤„ç†åŒ…å«å¤šç§è¯­è¨€çš„æ–‡æ¡£æ—¶ï¼Œå¦‚æœæ··åˆç´¢å¼•ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¯­è¨€é—´çš„è¯­ä¹‰å¹²æ‰°ã€‚
    - **ç­–ç•¥ï¼š**Â åœ¨é¢„å¤„ç†é˜¶æ®µï¼Œåº”å¼•å…¥è¯­è¨€è¯†åˆ«æ­¥éª¤ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨langdetectç­‰åº“ï¼‰ã€‚è¯†åˆ«å‡ºçš„è¯­è¨€ä¿¡æ¯åº”ä½œä¸ºä¸€ä¸ªå…³é”®çš„**å…ƒæ•°æ®**å­—æ®µè¿›è¡Œå­˜å‚¨ã€‚åœ¨æ£€ç´¢æ—¶ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„è¯­è¨€ï¼Œä¼˜å…ˆæˆ–ä»…åœ¨å¯¹åº”è¯­è¨€çš„æ–‡æ¡£å—ä¸­è¿›è¡Œæœç´¢ã€‚
- **å¯æ‰§è¡Œä»£ç ç¤ºä¾‹ (ç»¼åˆå¤„ç†å‡½æ•°):**

```python
# å‡†å¤‡ç¯å¢ƒ:
# pip install beautifulsoup4

import re
from bs4 import BeautifulSoup
import unicodedata

def normalize_text(text: str) -> str:
    """ä¸€ä¸ªå…¨é¢çš„æ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–å‡½æ•°"""
    
    # 1. ç§»é™¤HTMLæ ‡ç­¾
    # ä½¿ç”¨BeautifulSoupåº“æ¥è§£æå’Œç§»é™¤HTMLæ ‡è®°
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text(separator=" ") # ä½¿ç”¨ç©ºæ ¼ä½œä¸ºåˆ†éš”ç¬¦ï¼Œé¿å…å•è¯ç²˜è¿
    
    # 2. Unicodeå½’ä¸€åŒ–
    # 'NFKC'å½¢å¼ä¼šåˆå¹¶è®¸å¤šå…¼å®¹å­—ç¬¦ï¼Œä¾‹å¦‚å°†å…¨è§’å­—ç¬¦è½¬ä¸ºåŠè§’ï¼Œå¤„ç†è¿å­—ç­‰ã€‚
    text = unicodedata.normalize('NFKC', text)
    
    # 3. è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # 4. ç§»é™¤URLå’Œé‚®ç®± (ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    
    # 5. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ (ä¿ç•™å­—æ¯ã€æ•°å­—ã€å’ŒåŸºæœ¬æ ‡ç‚¹)
    # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼ä¼šä¿ç•™å­—æ¯(a-z), æ•°å­—(0-9), ç©ºæ ¼, ä»¥åŠ . , ! ? -
    # text = re.sub(r'[^a-z0-9\s.,!?-]', '', text)
    
    # 6. ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    # å°†å¤šä¸ªè¿ç»­çš„ç©ºæ ¼ã€æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# --- ç¤ºä¾‹æ–‡æœ¬ ---
raw_text = """
  <p>Hello World! ğŸ˜Š</p>
  Check our amazing site at https://example.com. 
  Contact us at help@example.com. 
  This is a full-width testï¼šï¼´ï¼¥ï¼³ï¼´ã€‚
"""
clean_text = normalize_text(raw_text)

print("--- Raw Text ---")
print(raw_text)
print("\n--- Cleaned & Normalized Text ---")
print(clean_text)
# è¾“å‡º: hello world! ğŸ˜Š check our amazing site at . contact us at . this is a full-width test:testã€‚
```

- **ä¸€ä¸ªå¸¸è§çš„é™·é˜±â€”â€”è¿‡åº¦æ¸…æ´—ï¼š** æ¸…æ´—è§„åˆ™å¿…é¡»æ ¹æ®é¢†åŸŸçŸ¥è¯†æ¥å®šåˆ¶ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤„ç†ä»£ç çŸ¥è¯†åº“æ—¶ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ `*`, `{`, `}`ï¼‰æˆ–è½¬æ¢å¤§å°å†™ä¼šå®Œå…¨ç ´åä»£ç çš„å«ä¹‰ã€‚åœ¨å¤„ç†é‡‘èæˆ–ç§‘å­¦æ–‡çŒ®æ—¶ï¼Œç§»é™¤é€—å·æˆ–ç ´æŠ˜å·ä¹Ÿå¯èƒ½æ”¹å˜æ•°å­—å’Œå…¬å¼çš„æ„ä¹‰ã€‚

ğŸ‘‰ **æ¸…æ´—å‰ï¼Œè¯·å…ˆæ€è€ƒä½ çš„æ•°æ®ç±»å‹**

## **è¯å¹²æå– (Stemming) ä¸è¯å½¢è¿˜åŸ (Lemmatization)**

### **ç±»æ¯”ï¼š**

- **è¯å¹²æå–ï¼š**å°±åƒä¸€ä¸ª**ç²—æš´çš„ç†å‘å¸ˆ**ï¼Œç”¨æ¨å­ï¼ˆè§„åˆ™ï¼‰ç›´æ¥æ¨æ‰å¤´å‘ï¼ˆåç¼€ï¼‰ï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œä½†å‘å‹ï¼ˆç»“æœï¼‰å¯èƒ½å¾ˆéš¾çœ‹ï¼ˆä¸æ˜¯ä¸€ä¸ªçœŸå®çš„å•è¯ï¼‰ã€‚ä¾‹å¦‚ï¼Œ"studies",Â "studying"Â ->Â "studi"ã€‚
- **è¯å½¢è¿˜åŸï¼š**åˆ™åƒä¸€ä½**ä¸“ä¸šçš„é€ å‹å¸ˆ**ï¼Œä»–ä¼šæŸ¥é˜…æ—¶å°šæ‚å¿—ï¼ˆå­—å…¸ï¼‰å’Œè€ƒè™‘ä½ çš„è„¸å‹ï¼ˆè¯æ€§ï¼‰ï¼Œä¸ºä½ è®¾è®¡ä¸€ä¸ªæœ€åˆé€‚çš„åŸå‹å‘å‹ï¼ˆè¯å…ƒï¼‰ã€‚ä¾‹å¦‚ï¼Œ"studies"Â (åè¯) ->Â "study",Â "studying"Â (åŠ¨è¯) ->Â "study"ã€‚

### **åœ¨RAGä¸­çš„æŠ‰æ‹©ï¼š**

- **å¼ºçƒˆæ¨èä½¿ç”¨è¯å½¢è¿˜åŸï¼Œç”šè‡³ä¸ä½¿ç”¨ã€‚**Â ç°ä»£çš„åµŒå…¥æ¨¡å‹æ˜¯åœ¨æµ·é‡åŸå§‹æ–‡æœ¬ä¸Šè®­ç»ƒçš„ï¼Œå®ƒä»¬æœ¬èº«å°±èƒ½å¾ˆå¥½åœ°ç†è§£runningå’Œrunä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚å¯¹è¾“å…¥è¿›è¡Œè¯å¹²æå–ï¼Œåè€Œä¼šäº§ç”Ÿrunnè¿™æ ·çš„éè‡ªç„¶è¯­è¨€è¯æ±‡ï¼Œä»è€Œâ€œæ±¡æŸ“â€è¾“å…¥ï¼Œå¯¼è‡´åµŒå…¥è´¨é‡ä¸‹é™ã€‚è¯å½¢è¿˜åŸèƒ½ä¿ç•™è¯æ±‡çš„è‡ªç„¶æ€§ï¼Œå¯¹åµŒå…¥æ¨¡å‹æ›´å‹å¥½ï¼Œä½†ä¹Ÿä¼šå¢åŠ å¤„ç†æ—¶é—´ã€‚å¯¹äºå¤§å¤šæ•°ç°ä»£RAGç³»ç»Ÿï¼Œ**æœ€å¸¸è§çš„åšæ³•æ˜¯åªè¿›è¡Œæ ‡å‡†åŒ–ï¼Œè€Œä¸è¿›è¡Œè¯å½¢è¿˜åŸæˆ–è¯å¹²æå–**ï¼Œå®Œå…¨ä¿¡ä»»åµŒå…¥æ¨¡å‹çš„èƒ½åŠ›ã€‚
- **å¯æ‰§è¡Œä»£ç ç¤ºä¾‹ (ä½¿ç”¨spaCyè¿›è¡Œè¯å½¢è¿˜åŸ):**

```python
# å‡†å¤‡ç¯å¢ƒ:
# pip install spacy
# python -m spacy download en_core_web_sm

import spacy

# åŠ è½½spaCyçš„å°å‹è‹±è¯­æ¨¡å‹
# æ¨¡å‹ä¼šè¿›è¡Œåˆ†è¯ã€è¯æ€§æ ‡æ³¨ç­‰ä¸€ç³»åˆ—NLPå¤„ç†
print("Loading spaCy model...")
nlp = spacy.load("en_core_web_sm")
print("Model loaded.")

text = "The researchers are studying the effects of studying on students' performance."
doc = nlp(text)

# token.lemma_ å±æ€§ä¼šè¿”å›è¯¥è¯çš„è¯å…ƒï¼ˆåŸºæœ¬å½¢å¼ï¼‰
lemmatized_tokens = [token.lemma_ for token in doc]

print("\n--- Original Text ---")
print(text)
print("\n--- Lemmatized Tokens ---")
print(lemmatized_tokens)
print("\n--- Lemmatized Text ---")
print(" ".join(lemmatized_tokens))
# è¾“å‡º: the researcher be study the effect of study on student 's performance .
```