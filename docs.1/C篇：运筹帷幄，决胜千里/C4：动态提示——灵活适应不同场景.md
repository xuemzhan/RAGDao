# C4：动态提示——灵活适应不同场景

“一招鲜，吃遍天”的静态提示在复杂应用中是行不通的。动态提示技术，是指根据查询的类型、检索到的信息的性质、甚至用户的历史行为，来动态地生成或选择最合适的提示模板。

## **动态提示的实现策略**

- **基于查询类型的提示路由 (Query-based Prompt Routing):**
    - **说明：** 在RAG流程的最开始，加入一个“查询分类”步骤。使用一个小型LLM或关键词规则，判断用户查询的意图。
    - **示例:**
        - **查询:** "Compare RAG and Fine-tuning." -> **分类:** Comparison -> **路由到:** 一个专门为生成对比表格而设计的Prompt模板。
        - **查询:** "How to set up a ChromaDB index?" -> **分类:** Instructional -> **路由到:** 一个引导模型生成分步指南的Prompt模板。
        - **查询:** "What is the definition of RAG?" -> **分类:** Factual QA -> **路由到:** 一个标准的、要求简洁回答的Prompt模板。
- **基于上下文性质的自适应提示 (Context-aware Adaptive Prompting):**
    - **说明：** Prompt的内容可以根据检索到的上下文的性质动态调整。
    - **示例:**
        - **场景：** 在执行了H1篇提到的“置信度评估”后，发现检索到的文档分数不高。
        - **动态调整：** 可以在Prompt的指令中动态加入一句话："Warning: The following context may only be partially relevant. Please answer cautiously and state any uncertainties."
- **结合对话历史的动态提示:**
    - **说明：** 在多轮对话中，可以根据之前的对话内容调整当前Prompt的指令。
    - **示例:** 如果用户在之前的对话中多次对长答案表示不满意（例如，追问“能简单点说吗？”），系统可以在后续的Prompt中自动加入指令："Keep your answer concise and under 50 words."

## **在提示中使用少量示例 (Few-Shot Prompting)**

- **详细阐述：** 这是引导模型输出特定格式或执行复杂推理的最有效方法之一。通过在提示中提供一两个完整的“输入-输出”范例，模型可以迅速“学会”你的要求。
- **[图片建议]:** 一张图展示Few-Shot Prompt的结构：上半部分是Instruction，中间部分是一个完整的Example Input -> Example Output对，用一个框框起来并标注“示例(Example)”，下半部分是Actual Input ->，等待模型生成。
- **可执行代码示例 (引导JSON输出的Few-Shot Prompt):**

```python
def create_json_extraction_prompt(context: str, question: str) -> str:
    """
    使用Few-Shot示例来引导模型输出JSON格式。
    """
    # 这个Few-Shot示例被硬编码在Prompt模板中
    prompt = f"""
    You are an expert entity extractor. Your task is to extract specific information from the provided context based on the user's question and format it as a valid JSON object.
		--- EXAMPLE START ---
		<context>
		The project 'Apollo' was started on Jan 15, 2023. The project manager is Alice. The budget for the Apollo project is $500,000.
		</context>
		<question>
		What are the details for the Apollo project?
		</question>
		<answer>
		{{
		"project_name": "Apollo",
		"start_date": "2023-01-15",
		"manager": "Alice",
		"budget": 500000
		}}
		</answer>
		--- EXAMPLE END ---
		--- TASK START ---
		<context>
		{context}
		</context>
		<question>
		{question}
		</question>
		<answer>
		"""
		return prompt

# --- 示例用法 ---
my_context = "Project 'Zeus' is managed by Bob and has a total budget of $1.2M. It was officially kicked off on March 3, 2024."
my_question = "Extract the details for the Zeus project."

final_prompt = create_json_extraction_prompt(my_context, my_question)
print(final_prompt)

# 期望LLM能够模仿示例的格式，输出:
# {
#   "project_name": "Zeus",
#   "start_date": "2024-03-03",
#   "manager": "Bob",
#   "budget": 1200000
# }
```