# H1：处理不充分或不相关的检索信息——学会“优雅地认输”

“知之为知之，不知为不知，是知也。”对于AI系统而言，能够承认自己的局限性，是建立用户信任的关键一步。当检索系统无法从知识库中找到与用户查询相关的高质量信息时，强行让LLM基于低质量或不相关的上下文进行“创作”，是导致“幻觉”和错误信息的主要根源之一。

## **制定无法检索高质量数据的策略**

- **巧妙设计——“相关性阈值”与“回退策略”:**
    - **类比：** 就像一个**严谨的侦探**。如果他在案发现场找不到**任何一条高于60分可信度**的线索（低于相关性阈值），他不会随意猜测一个凶手，而会向上级报告：“目前线索不足，无法得出结论”（触发回退策略）。
    - **实现方法：** 在检索或重排后，检查Top-1文档的相关性分数。如果低于预设的阈值（例如0.7），则不进入生成步骤，直接触发回退。
- **可执行代码示例 (逻辑示意):**

```python
# 准备环境:
# pip install sentence-transformers
from sentence_transformers import CrossEncoder

# 假设我们有一个重排器模型
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def get_answer_with_fallback(query: str, retrieved_docs: list[str], relevance_threshold: float = 0.5):
    """
    一个带有回退机制的RAG生成管道。

    Args:
        query (str): 用户查询。
        retrieved_docs (list[str]): 从召回阶段获取的文档列表。
        relevance_threshold (float): 重排分数的阈值，低于此值则认为信息不充分。

    Returns:
        str: 生成的答案或回退信息。
    """
    if not retrieved_docs:
        return "I'm sorry, I couldn't find any information related to your query."

    # 1. 使用重排器对召回的文档进行打分
    sentence_pairs = [[query, doc] for doc in retrieved_docs]
    scores = reranker.predict(sentence_pairs)
    
    # 将分数和文档打包并排序
    scored_docs = sorted(zip(scores, retrieved_docs), reverse=True)
    
    top_score, top_doc = scored_docs[0]
    print(f"\n--- Confidence Check ---")
    print(f"Top document score: {top_score:.4f}")
    print(f"Relevance threshold: {relevance_threshold}")

    # 2. 置信度评估 (核心逻辑)
    if top_score < relevance_threshold:
        print("Confidence is below threshold. Triggering fallback.")
        # 3. 触发回退策略
        return f"I found some information, but I'm not confident enough to provide a definitive answer. The most relevant piece of information I found is: '{top_doc}'"
    else:
        print("Confidence is high. Proceeding to generation.")
        # 4. 筛选高质量上下文并生成答案 (此处简化)
        # high_quality_docs = [doc for score, doc in scored_docs if score >= relevance_threshold]
        # prompt = f"Context: {high_quality_docs[0]}\n\nQuestion: {query}\n\nAnswer:"
        # answer = f"Based on the high-confidence context, the answer is derived from: '{high_quality_docs[0]}'"
        # return answer
        
        # 简化返回
        return f"Based on high-confidence information, here is the answer derived from: '{top_doc}'"

# --- 示例用法 ---
my_query = "What is the airspeed velocity of an unladen swallow?"

# 场景1: 检索到高质量文档
retrieved_high_quality = [
    "The airspeed velocity of an unladen European swallow is about 11 meters per second.",
    "A swallow is a type of bird."
]
answer1 = get_answer_with_fallback(my_query, retrieved_high_quality, relevance_threshold=0.1)
print("\nFinal Answer (Scenario 1):", answer1)

print("\n" + "="*50 + "\n")

# 场景2: 检索到低质量/不相关文档
retrieved_low_quality = [
    "Birds are animals that can fly.",
    "The swallows return to Capistrano every year."
]
answer2 = get_answer_with_fallback(my_query, retrieved_low_quality, relevance_threshold=0.1)
print("\nFinal Answer (Scenario 2):", answer2)
```

[**H1：处理不充分或不相关的检索信息** v2](https://www.notion.so/H1-v2-26055a58d45c80afbaa7f93f3b6d649c?pvs=21)