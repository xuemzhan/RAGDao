# === C篇：运筹帷幄，决胜千里/C5：提示校准——持续迭代与优化.md ===

# C5：提示校准——持续迭代与优化

提示工程是一个动态的、永不停止的优化过程。最好的提示，是通过不断的评估、反馈和实验打磨出来的。这就像**调校一辆赛车**。你不能在车库里凭空想象最佳的悬挂设置。你必须把车开到赛道上（**A/B测试**），记录每一圈的性能数据（**评估指标**），听取车手的反馈（**用户反馈**），然后回到车库进行微调，再回到赛道上测试。这是一个持续的“测试-分析-调整”的循环。

## **构建系统化的评估流程**

- **创建“黄金标准”评估集 (Golden Set):**
    - **说明：** 这是进行所有离线评估的基础。你需要人工整理一个覆盖各种查询类型和边缘案例的测试集。每个样本应包含：question, ideal_context (理想中应该被检索到的文档), ideal_answer (人工编写的完美答案), bad_answer_example (一个典型的不良回答范例)。
- **实施自动化离线评估:**
    - **流程:** 每当一个新版本的Prompt被提出时，都应该自动运行一个评估流水线：
        1. 使用评估集中的question和ideal_context，通过新Prompt生成一个generated_answer。
        2. 使用G篇中提到的RAGAS等框架，将generated_answer与ideal_answer和ideal_context进行比较，自动计算出Faithfulness, Answer Correctness等一系列量化指标。
        3. 将新Prompt的指标得分与当前生产环境中的Prompt（Baseline）进行对比。
    - **优点:** 这种自动化的“回归测试”可以在几分钟内告诉你新Prompt是否在关键指标上有所改进或退步，大大提高了迭代效率。

## **A/B测试与线上实验的深化**

- **详细阐述：** 这是在生产环境中科学地比较不同提示策略的“金标准”。
- **从A/B测试到“Prompt锦标赛” (Prompt Tournament):**
    - **说明：** A/B测试通常只比较两个版本。对于提示工程，你可能同时有好几个候选版本。可以采用“锦标赛”或“多臂老虎机”（Multi-armed Bandit）的思路。
    - **方法：** 同时在线上部署多个Prompt版本（例如，版本A, B, C, D），系统根据实时的用户反馈（如“点赞率”）动态地调整分配给每个版本的流量。表现好的版本会获得更多流量，表现差的则逐渐被淘汰。
- **b. 细粒度的指标追踪:**
    - **说明：** 不要只看总体的用户满意度。要将指标按**查询类型**进行细分。
    - **示例：** 你的新Prompt可能在处理“比较类”问题时表现优异，但在处理“事实问答类”问题时反而有所下降。通过细粒度分析，你可以发现这一点，并考虑为不同类型的查询使用不同的Prompt（即C4中提到的动态提示）。
- **流程图：**

```mermaid
flowchart TD
    A[用户流量] --> B{"流量分割 (50/50)"};
    B --> C[A组: 使用旧Prompt];
    B --> D[B组: 使用新Prompt];
    
    C --> E["收集A组指标<br>(如: 用户满意度)"];
    D --> F["收集B组指标<br>(如: 用户满意度)"];
    
    E --> G{统计分析};
    F --> G;
    
    G --> H[新Prompt是否显著更优?];
    H -- Yes --> I[全量部署新Prompt];
    H -- No --> J[保留旧Prompt或继续实验];
```

## **建立Prompt版本控制与管理平台**

- **将Prompt视为配置 (Prompt-as-Config):**
    - **说明：** 不要将Prompt硬编码在你的应用程序代码中。应该将它们作为独立的配置文件（如YAML, JSON）进行管理，并纳入Git等版本控制系统中。
    - **优点：**
        - **版本追溯：** 可以轻松地查看历史版本的Prompt，并进行回滚。
        - **非技术人员可参与：** 产品经理、运营人员等也可以在无需修改代码的情况下，参与到Prompt的优化中。
- **b. 构建Prompt Hub:**
    - **说明：** 在大型团队或企业中，可以建立一个内部的“Prompt Hub”或“Prompt注册中心”。
    - **功能：**
        1. **版本控制与存储：** 集中管理所有业务线的Prompt模板。
        2. **在线测试与评估：** 提供一个UI界面，让用户可以输入变量，快速测试不同版本的Prompt，并查看其离线评估得分。
        3. **A/B测试集成：** 与A/B测试框架打通，可以一键将某个版本的Prompt推送到线上进行实验。
        4. **性能监控：** 展示每个线上Prompt版本的实时性能指标（如用户满意度、生成延迟等）。
    - **价值：** 将零散的、个人化的Prompt调优工作，转变为一个系统化的、可协作的、数据驱动的工程流程。

# === C篇：运筹帷幄，决胜千里/C1：情境整合——为模型搭建清晰的舞台.md ===

# C1：情境整合——为模型搭建清晰的舞台

情境整合（Context Integration）研究的是如何将检索到的不同信息片段（上下文）、用户的原始查询以及对模型的指令，有效地组织和拼接成一个最终的提示。这就像是为一位演员（LLM）准备**剧本**。剧本的结构必须清晰，让演员一眼就能分清哪部分是**背景介绍（上下文）**，哪部分是**他的台词提示（问题）**，哪部分是**导演的特别指示（指令）**。如果剧本乱成一团，再好的演员也可能演砸。

## **建立清晰的信息层次结构**

一个结构化的Prompt能帮助LLM更好地理解不同信息模块的角色。关键在于建立清晰的信息层次：

- **系统指令 (System Instruction):** 位于最顶层，明确模型的角色、任务和核心行为准则。
- **检索上下文 (Retrieved Context):** 从外部知识库获取的相关信息，是模型回答问题的主要依据。
- **用户查询 (User Question):** 用户的原始问题，是模型需要解决的核心目标。
- **输出要求 (Output Requirements):** 对期望回答的格式、风格、长度等进行约束。

## **尝试不同的信息合并方式**

不同的LLM可能对提示中信息的排列顺序有不同的敏感度。没有绝对的最优解，但存在一些经过实践检验的有效模式。**常见模式举例：**

- **模式一：指令 -> 上下文 -> 问题 (最常用)**
    
    先告诉模型它的角色和规则，然后给它阅读材料，最后让它回答问题。符合人类的认知流程，逻辑清晰。
    
- **模式二：上下文 -> 问题 -> 指令**
    
    有时将指令放在最后，可以起到“强化提醒”的作用，尤其是在指令非常关键时（例如，“绝对禁止使用外部知识”）。
    

## **使用清晰的界限**

使用明确的分隔符，可以帮助模型在结构上区分不同的信息输入。这减少了模型将上下文内容误解为指令，或将问题误解为上下文的可能性。

- **巧妙设计——XML标签：**
    - **概念出处：** 使用XML标签来构建提示的实践，由Anthropic公司在其Claude模型的文档中大力推广，并被证明是一种非常有效和鲁棒的结构化提示方法。
    - **优点：** 结构化强，可嵌套，比简单的###或---更不容易产生歧义。
- **可执行代码示例 (使用XML标签构建Prompt):**

```python
def create_prompt_with_xml(instruction: str, context_list: list[str], question: str) -> str:
    """
    使用XML标签构建一个结构清晰的RAG Prompt。

    Args:
        instruction (str): 对LLM的核心指令。
        context_list (list[str]): 检索到的文档块列表。
        question (str): 用户的原始问题。

    Returns:
        str: 格式化后的完整Prompt。
    """
    # 将上下文文档列表格式化为带有索引的XML标签字符串
    # <document index="1">...</document>
    # <document index="2">...</document>
    context_str = "\n".join(
        f"<document index=\"{i+1}\">\n{doc}\n</document>" 
        for i, doc in enumerate(context_list)
    )
    
    # 将所有部分组合成最终的Prompt
    # 使用<instruction>, <context>, <question>, <answer>等标签清晰地划分区域
    prompt = f"""
    <instruction>
		{instruction}
		</instruction>
		<context>
		{context_str}
		</context>
		<question>
		{question}
		</question>
		<answer>
		"""
		return prompt
# --- 示例用法 ---
my_instruction = "Answer the user's question based strictly on the provided documents. Cite the document index for each piece of information using the format [index]."
my_docs = [
    "The first RAG paper was published by Lewis et al. in 2020.", 
    "RAG combines retrieval with generation to reduce hallucination."
]
my_question = "Who published the first RAG paper and what is its purpose?"

final_prompt = create_prompt_with_xml(my_instruction, my_docs, my_question)

print("--- Generated Prompt ---")
print(final_prompt)

# 模拟LLM可能的输出:
# The first RAG paper was published by Lewis et al. in 2020 [1]. 
# Its purpose is to combine retrieval with generation to reduce hallucination [2].
```

## **一个重要的陷阱——“迷失在中间 (Lost in the Middle)”：**

这个现象在2023年由多伦多大学、斯坦福大学和UC伯克利的研究人员在论文 "Lost in the Middle: How Language Models Use Long Contexts" 中被系统性地揭示。

- **现象：** 许多LLM在处理长上下文时，对开头和结尾部分的信息注意力最强，而对中间部分的信息容易“遗忘”。
- **应对策略：** 在将检索到的文档列表传入提示前，**将最相关的文档（例如，由重排器Reranker打分最高的文档）放在列表的开头或结尾**，而不是简单地按原始分数堆叠。**核心策略：优化文档排序 (Document Reordering)**
    - **方法：** 在将检索到的Top-K个文档送入Prompt之前，不要简单地按原始相关性分数（从高到低）排列。可以尝试以下策略：
        1. **“三明治”法：** 将最相关的1-2个文档放在上下文的**最开头**，将次相关的1-2个文档放在**最末尾**，其余的文档放在中间。
        2. **“反向”法：** 直接将文档按相关性**从低到高**排列。这强制模型在最后看到最关键的信息，可能会激发更好的表现。
    - **效果：** 这些看似简单的排序调整，在许多基准测试中已被证明能显著提升RAG在长上下文场景下的性能。

> 文档重排序对RAG性能的影响，在Neevon shoal等人2024年的论文 "Lost in the Middle no More: Improving Large Language Model Population Awareness" 等研究中有深入探讨。
>

# === C篇：运筹帷幄，决胜千里/C3：多文档综合策略 (Multi-Document Synthesis Strategies).md ===

# C3：多文档综合策略 (Multi-Document Synthesis Strategies)

RAG系统通常会检索到多个相关的文档块。这些块可能内容互补、部分重叠，甚至相互矛盾。提示需要指导模型如何成为一个优秀的“信息整合者”和“冲突调解员”。

## **常用综合方法**

- **信息摘要 (Summarization):** 指示模型先总结每个文档的关键点，再整合成一个连贯的答案。
- **观点对比 (Viewpoint Comparison):** 对于包含不同观点或数据的文档，指示模型识别并呈现这些差异。
- **证据链构建 (Evidence Chaining):** 对于需要多步推理的问题，指示模型将多个文档的信息串联成一个逻辑清晰的证据链条。
- **置信度评估 (Confidence Scoring):** 指示模型根据多个来源信息的一致性，来评估最终答案的置信度。

## **制定多源综合信息的策略**

- **示例指令：** "Read all the provided documents carefully. Synthesize the information from all relevant documents to construct a comprehensive and coherent answer. Do not simply copy-paste sentences from the documents. Your answer should integrate the key points into a smooth narrative."

## **实施解决冲突信息的技术**

- **巧妙设计——“报告冲突”指令：**
    - **方法：** 指示模型在发现冲突时不应自行选择一方，而应将冲突本身作为答案的一部分呈现给用户。
    - **示例指令：** "If you find conflicting information across different documents, do not try to resolve it. Instead, you must present both viewpoints and explicitly state that there is a contradiction in the provided sources. For example: 'Document [1] states that the project deadline is on Monday, however, Document [3] states that it is on Wednesday.'"
    - **优点：** 这种方法极大地增强了系统的透明度和可信度，将最终的判断权交给了用户。

## **高级模式：Map-Reduce与Refine**

对于需要处理大量文档（例如，超过LLM上下文窗口长度）或进行深度摘要的场景，可以采用更复杂的提示链策略。

- **Map-Reduce 策略：**
    - **流程：**
        1. **Map 阶段:** 将所有检索到的文档块，**并行地**、**逐个地**喂给LLM，让它对**每一个**文档块都进行一个独立的初步处理（例如，生成一个摘要或提取关键信息）。
        2. **Reduce 阶段:** 将所有从Map阶段得到的初步结果（例如，所有的摘要），合并在一起，再喂给LLM一次，让它对这些**摘要的摘要**进行最终的综合和提炼，得出最终答案。
    - **类比：** **公司年度报告**的撰写。每个部门（Map）先各自提交一份年度总结，然后总经办（Reduce）将所有部门的总结汇总，提炼成一份面向董事会的最终报告。
- **Refine 策略：**
    - **流程：**
        1. 先将第一个文档块喂给LLM，生成一个初步答案。
        2. 然后，将第二个文档块和**上一步的答案**一起喂给LLM，让它根据新信息**修正和完善**已有的答案。
        3. 依次迭代，直到处理完所有文档。
    - **优点：** 能够构建更连贯、更有逻辑深度的答案。
    - **缺点：** 串行处理，耗时较长。

# === C篇：运筹帷幄，决胜千里/C2：指令清晰——明确模型的任务与行为.md ===

# C2：指令清晰——明确模型的任务与行为

指令是提示的“灵魂”，它定义了模型的**角色、能力边界、行为准则和输出格式**。指令越明确、越无歧义，模型的输出就越可控、越符合预期。

## **提供如何使用信息的明确说明**

不要假设模型“知道”该怎么做。你需要通过指令，为其设定精确的“游戏规则”。

- **信息优先级:** 明确哪些信息源更可信（例如，“优先采信发布日期最新的文档”）。
- **冲突处理:** 当检索到矛盾信息时，明确告知处理策略（例如，“同时呈现两种观点并注明来源”）。
- **引用要求:** 明确是否需要标注信息来源，以及引用的具体格式。
- **推理边界:** 明确模型是应该**严格基于**所提供信息回答，还是可以进行一定程度的**合理推理**。
- **对比举例 (从模糊到精确):**
    - **模糊指令 ❌:** "Answer the question based on the context."
    - **稍好一点 ✅:** "Answer the question using *only* the provided context."
    - **专业级指令 ✨:** "You are a helpful assistant for our company's knowledge base. Your task is to answer user questions based *strictly* and *exclusively* on the provided context documents. Do not use any of your internal knowledge. If the answer cannot be found in the provided documents, you *must* respond with the exact phrase: 'I could not find an answer in the provided documents.'"

## **使用外部知识包括引用或归因的指导**

- **详细阐述：** 在许多应用中（如客服、研究），答案的可追溯性至关重要。你必须在提示中明确指示模型如何进行引用。
- **示例指令 (在Prompt中):**
    
    "When you use information from a document to form your answer, you must cite the document's index at the end of the sentence, like this: [1]. If a single sentence synthesizes information from multiple documents, cite all of them, like this: [1, 2]. Each document is provided with its index in the format '<document index=\"...\">."
    

## **指令设计的心理学原则**

- **a. 角色扮演 (Role-playing):**
    - **说明：** 在指令的开头，为LLM设定一个具体的专家角色。这不仅仅是装饰，它能有效地激活模型在预训练时学到的与该角色相关的知识、语言风格和推理模式。
    - **示例：**
        - **弱指令：** "Summarize the document."
        - **强指令：** "You are a senior financial analyst. Your task is to summarize the following financial report for a board meeting. Focus on key metrics like revenue, profit margin, and future outlook."
- **b. 正向指令优于反向指令 (Positive vs. Negative Instructions):**
    - **说明：** 尽量告诉模型**“做什么”**，而不是**“不要做什么”**。就像对一个孩子说“请走路”比“不要跑”更有效一样，LLM对正向、具体的指令响应得更好。
    - **示例：**
        - **反向指令 ❌:** "Don't write a very long answer."
        - **正向指令 ✅:** "Summarize the key points in three concise bullet points."
- **c. 强调关键约束 (Emphasize Key Constraints):**
    - **说明：** 如果某个指令至关重要（例如，“严格基于上下文”），不要只说一遍。可以在指令的开头和结尾都强调它，或者使用大写字母、星号等方式来引起模型的“注意”。
    - **示例：** "...Based *STRICTLY* on the provided context. Remember, do not use any external knowledge. This is very important."

# === C篇：运筹帷幄，决胜千里/C0：开篇，掌握RAG的提示工程.md ===

# 开篇：掌握RAG的提示工程

如果说检索器是RAG系统的“情报官”，负责搜集信息；那么提示（Prompt）就是系统的“总参谋长”，负责向“最高指挥官”（LLM）下达一份清晰、周密、无懈可击的作战计划。这份计划必须明确告知LLM：战场态势如何（检索到的上下文），作战目标是什么（用户的查询），以及必须遵守哪些交战规则（指令）。

一份精心设计的提示，能将一堆零散的情报（文本块），转化为一篇逻辑严谨、条理清晰、直击问题核心的行动报告。反之，一份模糊、有歧（yì）义的提示，则可能让最强大的LLM也感到困惑，导致它要么“将在外君令有所不受”（忽略上下文），要么“错误领会意图”（答非所问）。

提示工程（Prompt Engineering）是一门介于艺术与科学之间的学问。它既需要我们像科学家一样，通过实验和迭代来寻找最优的结构；也需要我们像艺术家一样，运用语言的精妙来激发模型的最大潜能。本章将作为您的“参谋长作战手册”，深入探讨RAG提示工程的五大核心环节：**情境整合**、**指令清晰**、**多文档处理**、**动态提示**和**持续校准**。

```mermaid
graph TD
    subgraph "提示工程 (Prompt Engineering) 的核心要素"
        A["检索到的上下文<br>(Context)"] --> F{构建最终提示};
        B["用户查询<br>(Question)"] --> F;
        C["核心指令<br>(Instruction)"] --> F;
        
        F --> G[最终的Prompt];
        G --> H["大型语言模型 (LLM)"];
        
        subgraph "C1: 情境整合"
            I[不同的模板结构]
            J["清晰的边界符 (如XML标签)"]
        end
        
        subgraph "C2: 指令清晰"
            K[定义角色和能力边界]
            L[强制引用和归因]
        end
        
        subgraph "C3: 多文档处理"
            M[综合多源信息]
            N[处理信息冲突]
        end

        subgraph "C4: 动态提示"
            O[自适应提示策略]
            P["少量示例 (Few-Shot)"]
        end

        subgraph "C5: 持续校准"
            Q[A/B测试]
            R[反馈与迭代]
        end

        I --> C; J --> C;
        K --> C; L --> C;
        M --> C; N --> C;
        O --> C; P --> C;
        Q --> G; R --> G;
    end
```

[**C1：情境整合——为模型搭建清晰的舞台**](https://www.notion.so/C1-26055a58d45c80c1b305d661c0160165?pvs=21)

[**C2：指令清晰——明确模型的任务与行为**](https://www.notion.so/C2-26055a58d45c802cb976c41dead8ab62?pvs=21)

[**C3：多文档综合策略 (Multi-Document Synthesis Strategies)**](https://www.notion.so/C3-Multi-Document-Synthesis-Strategies-26055a58d45c8044bf14e15af3604dd6?pvs=21)

[**C4：动态提示——灵活适应不同场景**](https://www.notion.so/C4-26055a58d45c80238fd7ffb58f94ea53?pvs=21)

[**C5：提示校准——持续迭代与优化**](https://www.notion.so/C5-26055a58d45c800eb2a1f1b6b033e00e?pvs=21)

- Anthropic. (2024). *Anthropic Cookbook: Using XML tags*. Retrieved from [**https://docs.anthropic.com/claude/docs/use-xml-tags**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdocs.anthropic.com%2Fclaude%2Fdocs%2Fuse-xml-tags)
- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. *Advances in Neural Information Processing Systems, 33*, 1877-1901.
- Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2023). Lost in the Middle: How Language Models Use Long Contexts. *arXiv preprint arXiv:2307.03172*.

# === C篇：运筹帷幄，决胜千里/C4：动态提示——灵活适应不同场景.md ===

# C4：动态提示——灵活适应不同场景

“一招鲜，吃遍天”的静态提示在复杂应用中是行不通的。动态提示技术，是指根据查询的类型、检索到的信息的性质、甚至用户的历史行为，来动态地生成或选择最合适的提示模板。

## **动态提示的实现策略**

- **基于查询类型的提示路由 (Query-based Prompt Routing):**
    - **说明：** 在RAG流程的最开始，加入一个“查询分类”步骤。使用一个小型LLM或关键词规则，判断用户查询的意图。
    - **示例:**
        - **查询:** "Compare RAG and Fine-tuning." -> **分类:** Comparison -> **路由到:** 一个专门为生成对比表格而设计的Prompt模板。
        - **查询:** "How to set up a ChromaDB index?" -> **分类:** Instructional -> **路由到:** 一个引导模型生成分步指南的Prompt模板。
        - **查询:** "What is the definition of RAG?" -> **分类:** Factual QA -> **路由到:** 一个标准的、要求简洁回答的Prompt模板。
- **基于上下文性质的自适应提示 (Context-aware Adaptive Prompting):**
    - **说明：** Prompt的内容可以根据检索到的上下文的性质动态调整。
    - **示例:**
        - **场景：** 在执行了H1篇提到的“置信度评估”后，发现检索到的文档分数不高。
        - **动态调整：** 可以在Prompt的指令中动态加入一句话："Warning: The following context may only be partially relevant. Please answer cautiously and state any uncertainties."
- **结合对话历史的动态提示:**
    - **说明：** 在多轮对话中，可以根据之前的对话内容调整当前Prompt的指令。
    - **示例:** 如果用户在之前的对话中多次对长答案表示不满意（例如，追问“能简单点说吗？”），系统可以在后续的Prompt中自动加入指令："Keep your answer concise and under 50 words."

## **在提示中使用少量示例 (Few-Shot Prompting)**

- **详细阐述：** 这是引导模型输出特定格式或执行复杂推理的最有效方法之一。通过在提示中提供一两个完整的“输入-输出”范例，模型可以迅速“学会”你的要求。
- **[图片建议]:** 一张图展示Few-Shot Prompt的结构：上半部分是Instruction，中间部分是一个完整的Example Input -> Example Output对，用一个框框起来并标注“示例(Example)”，下半部分是Actual Input ->，等待模型生成。
- **可执行代码示例 (引导JSON输出的Few-Shot Prompt):**

```python
def create_json_extraction_prompt(context: str, question: str) -> str:
    """
    使用Few-Shot示例来引导模型输出JSON格式。
    """
    # 这个Few-Shot示例被硬编码在Prompt模板中
    prompt = f"""
    You are an expert entity extractor. Your task is to extract specific information from the provided context based on the user's question and format it as a valid JSON object.
		--- EXAMPLE START ---
		<context>
		The project 'Apollo' was started on Jan 15, 2023. The project manager is Alice. The budget for the Apollo project is $500,000.
		</context>
		<question>
		What are the details for the Apollo project?
		</question>
		<answer>
		{{
		"project_name": "Apollo",
		"start_date": "2023-01-15",
		"manager": "Alice",
		"budget": 500000
		}}
		</answer>
		--- EXAMPLE END ---
		--- TASK START ---
		<context>
		{context}
		</context>
		<question>
		{question}
		</question>
		<answer>
		"""
		return prompt

# --- 示例用法 ---
my_context = "Project 'Zeus' is managed by Bob and has a total budget of $1.2M. It was officially kicked off on March 3, 2024."
my_question = "Extract the details for the Zeus project."

final_prompt = create_json_extraction_prompt(my_context, my_question)
print(final_prompt)

# 期望LLM能够模仿示例的格式，输出:
# {
#   "project_name": "Zeus",
#   "start_date": "2024-03-03",
#   "manager": "Bob",
#   "budget": 1200000
# }
```

