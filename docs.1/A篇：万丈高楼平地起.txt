# === Aç¯‡ï¼šä¸‡ä¸ˆé«˜æ¥¼å¹³åœ°èµ·/A3ï¼šå…ƒæ•°æ®ä¸°å¯Œ (Metadata Enrichment)â€”â€”èµ‹äºˆæ•°æ®å¯è¿‡æ»¤çš„â€œç»´åº¦â€.md ===

# A3ï¼šå…ƒæ•°æ®ä¸°å¯Œ (Metadata Enrichment)â€”â€”èµ‹äºˆæ•°æ®å¯è¿‡æ»¤çš„â€œç»´åº¦â€

å…ƒæ•°æ®ï¼ˆMetadataï¼‰æ˜¯â€œå…³äºæ•°æ®çš„æ•°æ®â€ã€‚å…ƒæ•°æ®æ˜¯æå‡æ£€ç´¢ç²¾åº¦çš„é‡è¦è¡¥å……ã€‚è®¸å¤šå‘é‡æ•°æ®åº“æ”¯æŒåœ¨è¿›è¡Œå‘é‡æœç´¢å‰ï¼Œå…ˆé€šè¿‡å…ƒæ•°æ®å¯¹æœç´¢ç©ºé—´è¿›è¡Œé¢„è¿‡æ»¤ï¼ˆPre-filteringï¼‰ï¼Œè¿™èƒ½æå¤§åœ°ç¼©å°åŒ¹é…èŒƒå›´ï¼Œè·å¾—æ›´ç²¾å‡†ã€æ›´å¿«é€Ÿçš„æ£€ç´¢ç»“æœã€‚åœ¨RAGä¸­ï¼Œä¸ºæ¯ä¸ªæ–‡æœ¬å—é™„åŠ å…ƒæ•°æ®ï¼Œå°±å¦‚åŒä¸ºå›¾ä¹¦é¦†é‡Œçš„æ¯ä¸€æœ¬ä¹¦éƒ½åˆ¶ä½œä¸€å¼ è¯¦ç»†çš„**ç´¢å¼•å¡**ï¼Œä¸Šé¢ä¸ä»…æœ‰ä¹¦åï¼Œè¿˜æœ‰ä½œè€…ã€å‡ºç‰ˆæ—¥æœŸã€ç±»åˆ«ã€å…³é”®è¯ç­‰ã€‚è¿™ä½¿å¾—æˆ‘ä»¬ä¸ä»…å¯ä»¥æ ¹æ®ä¹¦çš„å†…å®¹ï¼ˆè¯­ä¹‰æœç´¢ï¼‰ï¼Œè¿˜å¯ä»¥æ ¹æ®ç´¢å¼•å¡ä¸Šçš„è¿™äº›æ ‡ç­¾æ¥è¿›è¡Œç²¾ç¡®çš„ç­›é€‰å’ŒæŸ¥æ‰¾ï¼ˆå…ƒæ•°æ®è¿‡æ»¤ï¼‰ã€‚

## **æºå…ƒæ•°æ® (Source Metadata)**

ä¸€ä¸ªç”Ÿäº§çº§çš„RAGç³»ç»Ÿï¼Œå…¶å…ƒæ•°æ®åº”è‡³å°‘åŒ…å«ä»¥ä¸‹å†…å®¹ï¼Œå½¢æˆä¸€ä¸ªç»“æ„åŒ–çš„â€œ**å†…å®¹èº«ä»½è¯**â€ï¼š

- **a. æ¥æºä¸æ—¶é—´ä¿¡æ¯:**
    - source: æ–‡æ¡£æ¥æºçš„URLæˆ–æ–‡ä»¶è·¯å¾„ã€‚
    - created_at,Â last_modified: åˆ›å»ºå’Œæœ€åä¿®æ”¹æ—¶é—´æˆ³ã€‚
- **b. æƒè´£ä¸ç‰ˆæœ¬ä¿¡æ¯:**
    - author: æ–‡æ¡£ä½œè€…ã€‚
    - version: æ–‡æ¡£ç‰ˆæœ¬å·ï¼Œå¯¹äºå¤„ç†å†²çªå’Œæ›´æ–°è‡³å…³é‡è¦ã€‚
- **c. å†…å®¹åˆ†ç±»ä¿¡æ¯:**
    - category: ä¸»é¢˜åˆ†ç±»æˆ–æ ‡ç­¾ã€‚
    - keywords: äººå·¥æˆ–è‡ªåŠ¨æå–çš„å…³é”®è¯ã€‚
- **d. [æ–°å¢] ç»“æ„åŒ–ä¿¡æ¯:**
    - doc_structure: å—åœ¨æ–‡æ¡£ä¸­çš„é€»è¾‘ä½ç½®ï¼Œå¦‚ç« èŠ‚ã€å­ç« èŠ‚æ ‡é¢˜ã€‚
    - position: å—åœ¨æ®µè½ä¸­çš„ç‰©ç†ä½ç½®ï¼ˆå¦‚é¡µç ã€æ®µè½å·ï¼‰ã€‚
- **e. [æ–°å¢] å®ä½“è¯†åˆ«ç»“æœ:**
    - entities: è‡ªåŠ¨è¯†åˆ«å‡ºçš„äººåã€åœ°åã€ç»„ç»‡æœºæ„åç­‰ã€‚

### **è¯¦ç»†é˜è¿°ï¼š**Â è¿™æ˜¯æŒ‡ç›´æ¥æ¥æºäºæ–‡æ¡£æœ¬èº«æˆ–å…¶ç¯å¢ƒçš„æè¿°æ€§ä¿¡æ¯ã€‚

- **ä¾‹å­ï¼š**
    - source: æ–‡ä»¶åæˆ–URL (/path/to/my_doc.pdf,Â https://en.wikipedia.org/wiki/RAG)
    - created_at,Â last_modified: æ–‡ä»¶åˆ›å»ºæˆ–æœ€åä¿®æ”¹æ—¥æœŸ
    - author: æ–‡æ¡£ä½œè€…
    - doc_type: æ–‡æ¡£ç±»å‹ (PDF,Â Markdown,Â API_Doc)
    - page_number: å¯¹äºPDFç­‰æ–‡æ¡£ï¼Œå—æ‰€åœ¨çš„é¡µç 
    - chapter_title: å—æ‰€å±çš„ç« èŠ‚æ ‡é¢˜

## **å†…å®¹å…ƒæ•°æ® (Content-based Metadata)**

- **è¯¦ç»†é˜è¿°ï¼š**Â è¿™æ˜¯é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ä»å—çš„**å†…å®¹æœ¬èº«**æå–å‡ºçš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œæå¤§åœ°å¢å¼ºäº†æ•°æ®çš„å¯å‘ç°æ€§ã€‚
- **æ–¹æ³•ä¸ç¤ºä¾‹ï¼š**
    - **å®ä½“æå– (Entity Extraction):**Â ä½¿ç”¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ¨¡å‹ï¼ˆå¦‚spaCyï¼‰æ¥è‡ªåŠ¨è¯†åˆ«å’Œæ ‡æ³¨æ–‡æœ¬ä¸­çš„äººåã€åœ°åã€ç»„ç»‡ã€äº§å“åç­‰ã€‚
    - **æ‘˜è¦ (Summarization):**Â ä½¿ç”¨ä¸€ä¸ªå°å‹LLMä¸ºæ¯ä¸ªå—ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ‘˜è¦ã€‚
    - **ç”Ÿæˆé—®é¢˜ (Question Generation):**Â ä½¿ç”¨LLMï¼Œè®©å®ƒé’ˆå¯¹æ¯ä¸ªå—ç”Ÿæˆå‡ ä¸ªæœ€å¯èƒ½è¢«é—®åˆ°çš„é—®é¢˜ã€‚
- **å¯æ‰§è¡Œä»£ç ç¤ºä¾‹ (ä½¿ç”¨spaCyè¿›è¡Œå®ä½“æå–):**
    
    ```python
    # å‡†å¤‡ç¯å¢ƒ (spaCyå·²åœ¨A1ä¸­å®‰è£…å’Œä¸‹è½½)
    import spacy
    
    # åŠ è½½spaCyæ¨¡å‹ (å¦‚æœä¹‹å‰æ²¡åŠ è½½)
    try:
        nlp
    except NameError:
        print("Loading spaCy model for NER...")
        nlp = spacy.load("en_core_web_sm")
        print("Model loaded.")
    
    text_chunk = "Apple Inc., led by Tim Cook, is planning to build a new campus in Austin, Texas for $1 billion."
    doc = nlp(text_chunk)
    
    # ä»å¤„ç†åçš„æ–‡æ¡£ä¸­æå–å®ä½“
    # ent.text æ˜¯å®ä½“æ–‡æœ¬, ent.label_ æ˜¯å®ä½“ç±»å‹
    entities = [{"text": ent.text, "label": ent.label_} for ent in doc.ents]
    
    # å°†è¿™äº›å®ä½“ä½œä¸ºå…ƒæ•°æ®
    chunk_with_metadata = {
        "text_content": text_chunk,
        "metadata": {
            "source": "press_release_q4_2024.txt",
            "entities": entities
        }
    }
    
    import json
    print(json.dumps(chunk_with_metadata, indent=2))
    # è¾“å‡º:
    # {
    #   "text_content": "Apple Inc., led by Tim Cook, is planning to build a new campus in Austin, Texas for $1 billion.",
    #   "metadata": {
    #     "source": "press_release_q4_2024.txt",
    #     "entities": [
    #       { "text": "Apple Inc.", "label": "ORG" },
    #       { "text": "Tim Cook", "label": "PERSON" },
    #       { "text": "Austin", "label": "GPE" },
    #       { "text": "Texas", "label": "GPE" },
    #       { "text": "$1 billion", "label": "MONEY" }
    #     ]
    #   }
    # }
    ```

# === Aç¯‡ï¼šä¸‡ä¸ˆé«˜æ¥¼å¹³åœ°èµ·/A2ï¼šåˆ†å—ç­–ç•¥ (Chunking)â€”â€”åœ¨ä¸Šä¸‹æ–‡ä¸ç²¾åº¦é—´å¯»æ±‚å¹³è¡¡.md ===

åˆ†å—ï¼ˆChunkingï¼‰æ˜¯å°†å¤§å‹æ–‡æ¡£åˆ†è§£ä¸ºæ›´å°ã€æ›´æ˜“äºç®¡ç†çš„æ–‡æœ¬ç‰‡æ®µï¼ˆChunksï¼‰çš„è¿‡ç¨‹ã€‚è¿™æ˜¯RAGä¸­æœ€å…³é”®çš„æ­¥éª¤ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒç›´æ¥å®šä¹‰äº†æ£€ç´¢çš„æœ€å°å•ä½ã€‚

æƒ³è±¡ä¸€ä¸‹ç”¨**æ”¾å¤§é•œ**çœ‹ä¸€å¹…å·¨å¤§çš„ç”»ã€‚

- **å°å— (Small Chunks):**Â å°±åƒç”¨ä¸€ä¸ª**é«˜å€æ”¾å¤§é•œ**ã€‚ä½ å¯ä»¥éå¸¸æ¸…æ™°åœ°çœ‹åˆ°ç”»ä½œçš„æ¯ä¸€ä¸ªç¬”è§¦å’Œç»†èŠ‚ï¼ˆé«˜ç²¾åº¦ï¼‰ï¼Œä½†å¾ˆå®¹æ˜“è¿·å¤±æ–¹å‘ï¼Œä¸çŸ¥é“è¿™ä¸ªç»†èŠ‚åœ¨æ•´å¹…ç”»ä¸­çš„ä½ç½®ï¼ˆä¸Šä¸‹æ–‡ä¸¢å¤±ï¼‰ã€‚
- **å¤§å— (Large Chunks):**Â å°±åƒç”¨ä¸€ä¸ª**ä½å€æ”¾å¤§é•œ**ã€‚ä½ å¯ä»¥çœ‹åˆ°ä¸€ä¸ªå®Œæ•´çš„åœºæ™¯ï¼Œç†è§£äººç‰©å…³ç³»ï¼ˆä¸Šä¸‹æ–‡å®Œæ•´ï¼‰ï¼Œä½†å¯èƒ½ä¼šåŒ…å«å¾ˆå¤šæ— å…³çš„èƒŒæ™¯ï¼Œæ— æ³•èšç„¦äºä½ çœŸæ­£æƒ³çœ‹çš„é‚£ä¸ªç»†èŠ‚ï¼ˆå™ªéŸ³å¹²æ‰°ï¼‰ã€‚

ç†æƒ³çš„â€œå—â€åº”è¯¥æ˜¯ä¸€ä¸ª**â€œåŸå­åŒ–çš„è¯­ä¹‰å•å…ƒâ€**â€”â€”æ—¢è¶³å¤Ÿå°ä»¥ä¿æŒä¸»é¢˜çš„å•ä¸€æ€§ï¼Œåˆè¶³å¤Ÿå¤§ä»¥åŒ…å«ç†è§£è‡ªèº«æ‰€éœ€çš„å®Œæ•´ä¸Šä¸‹æ–‡ã€‚

![image.png](attachment:59c117ce-d22a-468c-9ba3-56b4269c44dd:image.png)

```mermaid
graph TD
    subgraph "é€‰æ‹©åˆ†å—ç­–ç•¥çš„å†³ç­–æµç¨‹"
        Q1{æ–‡æ¡£æ˜¯å¦æœ‰<br>æ¸…æ™°çš„ç»“æ„?} -->|æ˜¯| S1[ç»“æ„åŒ–åˆ†å—<br>å¦‚æŒ‰Markdownæ ‡é¢˜];
        Q1 -->|å¦| Q2{æ˜¯å¦éœ€è¦<br>æœ€ç®€å•çš„å®ç°?};
        Q2 -->|æ˜¯| S2[å›ºå®šå¤§å°åˆ†å—<br>å¦‚RecursiveCharacterTextSplitter];
        Q2 -->|å¦| S3[è¯­ä¹‰åˆ†å—<br>æŒ‰è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†å‰²];
        
        S1 --> E{è®¾ç½®åˆç†çš„<br>å—é—´é‡å };
        S2 --> E;
        S3 --> E;
        E --> F[å®Œæˆåˆ†å—];
    end
```

## **å›ºå®šå¤§å°åˆ†å— (Fixed-Size Chunking)**

è¿™æ˜¯æœ€ç®€å•ç›´æ¥çš„åˆ†å—æ–¹æ³•ã€‚å®ƒä¸è€ƒè™‘æ–‡æœ¬çš„å†…åœ¨ç»“æ„ï¼Œè€Œæ˜¯æŒ‰ç…§é¢„è®¾çš„é•¿åº¦æ¥åˆ‡åˆ†ã€‚LangChainä¸­çš„RecursiveCharacterTextSplitteræ˜¯æ­¤æ–¹æ³•çš„ä¼˜ç§€å®ç°ï¼Œå®ƒä¼šæ™ºèƒ½åœ°å°è¯•æŒ‰æœ‰æ„ä¹‰çš„è¾¹ç•Œï¼ˆå¦‚æ®µè½\n\nã€å¥å­.ï¼‰è¿›è¡Œé€’å½’åˆ‡åˆ†ï¼Œæ¯”çº¯ç²¹æŒ‰å­—ç¬¦æ•°â€œä¸€åˆ€åˆ‡â€è¦å¥½å¾—å¤šã€‚

- **å¯æ‰§è¡Œä»£ç ç¤ºä¾‹ (ä½¿ç”¨LangChain):**

```python
# å‡†å¤‡ç¯å¢ƒ:
# pip install langchain tiktoken

from langchain.text_splitter import RecursiveCharacterTextSplitter
import tiktoken

# ç¤ºä¾‹æ–‡æœ¬, åŒ…å«ä¸åŒçš„ç»“æ„
long_text = """
Introduction to Retrieval-Augmented Generation (RAG).

RAG is a powerful technique for enhancing Large Language Models (LLMs). It combines a retriever component, which fetches relevant information from a knowledge base, and a generator component, which is typically an LLM that produces an answer based on the retrieved context. This approach significantly reduces the risk of hallucination.

How it works:
1. User submits a query.
2. The retriever searches the knowledge base.
3. Relevant documents are passed to the LLM.
4. The LLM generates a grounded answer.
"""

# ä½¿ç”¨tiktokenç¼–ç å™¨æ¥è®¡ç®—tokenæ•°ï¼Œè¿™æ¯”ç®€å•åœ°è®¡ç®—å­—ç¬¦æ•°æ›´å‡†ç¡®åœ°åæ˜ LLMçš„æ¶ˆè€—ã€‚
# 'cl100k_base'æ˜¯OpenAIæ¨¡å‹å¸¸ç”¨çš„ç¼–ç å™¨ã€‚
tokenizer = tiktoken.get_encoding('cl100k_base')
def tiktoken_len(text):
    return len(tokenizer.encode(text))

# åˆå§‹åŒ–åˆ†å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(
    # chunk_size: æ¯ä¸ªå—çš„æœ€å¤§tokenæ•°ã€‚
    chunk_size=70, 
    # chunk_overlap: ä¸¤ä¸ªç›¸é‚»å—ä¹‹é—´é‡å çš„tokenæ•°ã€‚
    # è¿™æœ‰åŠ©äºä¿æŒå¥å­åœ¨å—è¾¹ç•Œå¤„çš„è¿ç»­æ€§ã€‚
    chunk_overlap=10,
    # length_function: ç”¨äºè®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•°ã€‚
    length_function=tiktoken_len,
    # separators: åˆ†å‰²å™¨ä¼šä¾æ¬¡å°è¯•ç”¨è¿™äº›åˆ†éš”ç¬¦æ¥åˆ†å‰²æ–‡æœ¬ã€‚
    # å®ƒä¼šä¼˜å…ˆå°è¯•ç”¨ "\n\n" (æ®µè½) åˆ†å‰²ï¼Œå¦‚æœåˆ†å‰²åçš„å—ä»ç„¶å¤ªå¤§ï¼Œ
    # å°±ä¼šåœ¨å—å†…éƒ¨å°è¯•ç”¨ "\n" (è¡Œ) åˆ†å‰²ï¼Œä»¥æ­¤ç±»æ¨ã€‚
    separators=["\n\n", "\n", " ", ""] 
)

chunks = text_splitter.split_text(long_text)

print(f"Original text length: {tiktoken_len(long_text)} tokens")
print(f"Split into {len(chunks)} chunks.\n")

for i, chunk in enumerate(chunks):
    print(f"--- Chunk {i+1} (Length: {tiktoken_len(chunk)} tokens) ---")
    print(chunk.replace("\n", " ")) # æ›¿æ¢æ¢è¡Œç¬¦ä»¥ä¾¿äºæ˜¾ç¤º
    print("-" * (len(chunk) + 12))
```

- **å¯èƒ½ç¢°åˆ°çš„é—®é¢˜ï¼š**
    
    **ä¸Šä¸‹æ–‡å‰²è£‚ï¼š**å³ä½¿æ˜¯RecursiveCharacterTextSplitterï¼Œåœ¨å—å¤§å°è®¾ç½®å¾—ä¸åˆç†æ—¶ï¼Œä¹Ÿå¯èƒ½å°†ä¸€ä¸ªå®Œæ•´çš„å¥å­æˆ–é€»è¾‘å•å…ƒå¼ºè¡Œæ‹†å¼€ï¼Œè¿™æ˜¯è¯¥æ–¹æ³•å›ºæœ‰çš„é£é™©ã€‚
    

## **å†…å®¹æ„ŸçŸ¥åˆ†å— (Content-Aware Chunking)**

å†…å®¹æ„ŸçŸ¥åˆ†å—æ˜¯ä¸€ç§æ›´æ™ºèƒ½çš„åˆ†å—æ–¹æ³•ï¼Œå®ƒè¯•å›¾ç†è§£å¹¶åˆ©ç”¨æ–‡æ¡£çš„å†…åœ¨ç»“æ„å’Œè¯­ä¹‰æ¥åˆ›å»ºæ›´å…·æ„ä¹‰çš„å—ã€‚è¿™ç§ç­–ç•¥æ—¨åœ¨å…‹æœå›ºå®šå¤§å°åˆ†å—å¯èƒ½å¯¼è‡´çš„ä¸Šä¸‹æ–‡å‰²è£‚é—®é¢˜ã€‚

### **ç»“æ„åŒ–åˆ†å— (Structured Chunking):**

- **åŸç†ï¼š**Â è¿™ç§æ–¹æ³•åˆ©ç”¨æ–‡æ¡£çš„æ˜¾å¼ç»“æ„æ ‡è®°ï¼ˆå¦‚æ ‡é¢˜ã€ç« èŠ‚ã€åˆ—è¡¨ã€è¡¨æ ¼ç­‰ï¼‰æ¥è¿›è¡Œåˆ†å—ã€‚å®ƒå‡è®¾æ–‡æ¡£çš„ç»“æ„æœ¬èº«å°±ä»£è¡¨äº†å†…å®¹çš„é€»è¾‘åˆ’åˆ†ã€‚ä¾‹å¦‚ï¼ŒMarkdownæ–‡æ¡£å¯ä»¥æŒ‰#ã€##ç­‰æ ‡é¢˜çº§åˆ«è¿›è¡Œåˆ†å‰²ï¼›HTMLæˆ–XMLæ–‡æ¡£å¯ä»¥æŒ‰<p>ã€<div>ã€<table>ç­‰æ ‡ç­¾è¿›è¡Œåˆ†å‰²ã€‚
- **ä¼˜ç‚¹ï¼š**Â ç”Ÿæˆçš„å—å…·æœ‰å¾ˆå¼ºçš„é€»è¾‘å®Œæ•´æ€§å’Œä¸»é¢˜å•ä¸€æ€§ï¼Œéå¸¸é€‚åˆå¤„ç†æ ¼å¼è§„èŒƒã€ç»“æ„æ¸…æ™°çš„æ–‡æ¡£ï¼ˆå¦‚ä¹¦ç±ã€æŠ€æœ¯æ–‡æ¡£ã€æ³•å¾‹æ¡æ–‡ç­‰ï¼‰ã€‚æ£€ç´¢æ—¶ï¼Œç›´æ¥è¿”å›ä¸€ä¸ªæ ‡é¢˜ä¸‹çš„æ‰€æœ‰å†…å®¹é€šå¸¸èƒ½æä¾›é«˜è´¨é‡çš„ä¸Šä¸‹æ–‡ã€‚
- **ç¼ºç‚¹ï¼š**Â ä¾èµ–äºæ–‡æ¡£çš„è‰¯å¥½ç»“æ„ã€‚å¯¹äºéç»“æ„åŒ–æˆ–æ ¼å¼æ··ä¹±çš„æ–‡æœ¬ï¼Œè¿™ç§æ–¹æ³•å°†å¤±æ•ˆã€‚
- **é€‚ç”¨åœºæ™¯ï¼š**Â ç»“æ„åŒ–çš„æ–‡æ¡£ï¼Œå¦‚Markdownæ ¼å¼çš„æ–‡æ¡£åº“ã€PDFæŠ¥å‘Šï¼ˆé€šè¿‡è§£æç»“æ„ï¼‰ã€APIæ–‡æ¡£ç­‰ã€‚

### **è¯­ä¹‰åˆ†å— (Semantic Chunking):**

- **åŸç†ï¼š**Â è¿™æ˜¯æœ€å…ˆè¿›çš„åˆ†å—ç­–ç•¥ä¹‹ä¸€ã€‚å®ƒä¸ä¾èµ–äºæ˜¾å¼ç»“æ„ï¼Œè€Œæ˜¯é€šè¿‡è®¡ç®—å¥å­æˆ–æ®µè½ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ¥å†³å®šåˆ†å‰²ç‚¹ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå½“ä¸€ä¸ªæ–°å¥å­ä¸å½“å‰å—çš„è¯­ä¹‰å…³è”åº¦ä½äºæŸä¸ªé¢„è®¾é˜ˆå€¼æ—¶ï¼Œå°±è®¤ä¸ºè¯é¢˜å‘ç”Ÿäº†è½¬å˜ï¼Œä»è€Œåœ¨æ­¤åˆ›å»ºä¸€ä¸ªæ–°çš„å—ã€‚è¿™é€šå¸¸é€šè¿‡æ–‡æœ¬åµŒå…¥ï¼ˆembeddingsï¼‰å’Œèšç±»ç®—æ³•ï¼ˆå¦‚K-meansã€Mean-Shiftï¼‰æˆ–æ»‘åŠ¨çª—å£å†…ç›¸ä¼¼åº¦å˜åŒ–æ£€æµ‹æ¥å®ç°ã€‚
- **ä¼˜ç‚¹ï¼š**Â èƒ½å¤Ÿå¤„ç†éç»“æ„åŒ–å’ŒåŠç»“æ„åŒ–æ–‡æœ¬ï¼Œè‡ªåŠ¨è¯†åˆ«å†…å®¹çš„é€»è¾‘è¾¹ç•Œå’Œä¸»é¢˜è½¬æ¢ç‚¹ï¼Œç”Ÿæˆçš„å—åœ¨è¯­ä¹‰ä¸Šæ›´åŠ è¿è´¯å’Œèšç„¦ã€‚
- **ç¼ºç‚¹ï¼š**Â è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œéœ€è¦ç”Ÿæˆå¤§é‡åµŒå…¥å¹¶è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼›é˜ˆå€¼è®¾ç½®å¯èƒ½éœ€è¦ç»éªŒè°ƒæ•´ï¼›å¯¹äºéå¸¸åˆ†æ•£æˆ–å¤šä¸»é¢˜çš„æ–‡æ¡£å¯èƒ½æ•ˆæœä¸ä½³ã€‚
- **é€‚ç”¨åœºæ™¯ï¼š**Â å¯¹è¯è®°å½•ã€æ–‡ç« æ‘˜è¦ã€è¯„è®ºã€åšå®¢æ–‡ç« ã€èŠå¤©è®°å½•ç­‰éç»“æ„åŒ–æ–‡æœ¬ï¼Œä»¥åŠä»»ä½•éš¾ä»¥é€šè¿‡ç»“æ„åŒ–æ–¹å¼åˆ†å—çš„æ–‡æœ¬ã€‚

![image.png](attachment:ab62666b-74d7-484a-84fe-8a6193a4d3d8:image.png)

### **å¯æ‰§è¡Œä»£ç ç¤ºä¾‹ (ä½¿ç”¨LangChainå®ç°ç»“æ„åŒ–åˆ†å—):**

```python
# å‡†å¤‡ç¯å¢ƒ:
# pip install langchain tiktoken pypdf "unstructured[all-extra]" # unstructuredç”¨äºPDFè§£æç­‰é«˜çº§ç»“æ„åŒ–æå–
# æ³¨æ„: Unstructuredå®‰è£…å¯èƒ½éœ€è¦ä¸€äº›ç³»ç»Ÿä¾èµ–ï¼Œç‰¹åˆ«æ˜¯å¯¹äºPDFå’Œå›¾åƒå¤„ç†ã€‚

from langchain.text_splitter import MarkdownHeaderTextSplitter

markdown_document = """
# äº§å“ä»‹ç»
## æ ¸å¿ƒåŠŸèƒ½
äº§å“Xæ‹¥æœ‰å¤šé¡¹åˆ›æ–°åŠŸèƒ½ï¼ŒåŒ…æ‹¬å®æ—¶æ•°æ®åˆ†æã€æ™ºèƒ½æ¨èç³»ç»Ÿå’Œç”¨æˆ·è‡ªå®šä¹‰ç•Œé¢ã€‚
è¿™äº›åŠŸèƒ½æ—¨åœ¨æå‡ç”¨æˆ·ä½“éªŒå’Œå·¥ä½œæ•ˆç‡ã€‚

### å®æ—¶æ•°æ®åˆ†æ
æˆ‘ä»¬çš„æ•°æ®åˆ†ææ¨¡å—èƒ½å¤Ÿå¤„ç†TBçº§åˆ«çš„æ•°æ®ï¼Œå¹¶åœ¨æ¯«ç§’çº§å†…æä¾›æ´å¯Ÿã€‚

## æŠ€æœ¯æ¶æ„
äº§å“åŸºäºå¾®æœåŠ¡æ¶æ„ï¼Œåç«¯é‡‡ç”¨Pythonå’ŒGoè¯­è¨€å¼€å‘ï¼Œå‰ç«¯ä½¿ç”¨Reactã€‚
æ•°æ®åº“é€‰æ‹©PostgreSQLå’ŒMongoDBä»¥æ»¡è¶³ä¸åŒæ•°æ®éœ€æ±‚ã€‚

# å®šä»·ç­–ç•¥
æˆ‘ä»¬æä¾›å¤šç§è®¢é˜…æ–¹æ¡ˆï¼ŒåŒ…æ‹¬åŸºç¡€ç‰ˆã€ä¸“ä¸šç‰ˆå’Œä¼ä¸šç‰ˆã€‚
"""

# å®šä¹‰æ ‡é¢˜å’Œå—çš„æ˜ å°„
headers_to_split_on = [
    ("#", "Header1"),
    ("##", "Header2"),
    ("###", "Header3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on,
    strip_headers=False # ä¿ç•™æ ‡é¢˜åœ¨å—å†…
)

md_chunks = markdown_splitter.split_text(markdown_document)

print(f"Original markdown document length: {len(markdown_document)} characters")
print(f"Split into {len(md_chunks)} chunks.\n")

for i, chunk in enumerate(md_chunks):
    print(f"--- Markdown Chunk {i+1} ---")
    print(f"Metadata: {chunk.metadata}") # åŒ…å«æå–çš„æ ‡é¢˜ä¿¡æ¯
    print(chunk.page_content)
    print("-" * 30)

# è¯­ä¹‰åˆ†å—é€šå¸¸æ¶‰åŠæ›´å¤æ‚çš„æ­¥éª¤ï¼Œå¦‚ç”Ÿæˆembeddingså’Œèšç±»ï¼Œ
# è¶…å‡ºç›´æ¥ä½¿ç”¨ä¸€ä¸ªLangChain Splitterçš„èŒƒç•´ï¼Œæ›´å€¾å‘äºè‡ªå®šä¹‰å®ç°æˆ–é«˜çº§åº“ã€‚
# è¿™é‡Œä»…ç¤ºæ„æ€§è¯´æ˜ï¼Œæ— ç›´æ¥å¯è¿è¡Œä»£ç ã€‚
# from sentence_transformers import SentenceTransformer, util
# import numpy as np
#
# model = SentenceTransformer('all-MiniLM-L6-v2')
# sentences = long_text.split('.') # ç®€å•æŒ‰å¥å·åˆ†å‰²
# embeddings = model.encode(sentences, convert_to_tensor=True)
#
# # è®¡ç®—ç›¸é‚»å¥å­ç›¸ä¼¼åº¦ï¼Œæ ¹æ®ç›¸ä¼¼åº¦éª¤é™ç‚¹è¿›è¡Œåˆ†å‰²
# similarities = util.cos_sim(embeddings[:-1], embeddings[1:]).diag()
# split_points = np.where(similarities < threshold)[0] + 1
# # ç„¶åæ ¹æ®split_pointsç»„åˆå¥å­å½¢æˆè¯­ä¹‰å—
```

### **å¯èƒ½ç¢°åˆ°çš„é—®é¢˜ï¼š**

- **ç»“æ„åŒ–åˆ†å—ï¼š**
    - **æ–‡æ¡£ç»“æ„ä¸è§„èŒƒï¼š**Â å¦‚æœæ–‡æ¡£çš„æ ¼å¼ä¸ä¸€è‡´æˆ–å­˜åœ¨å¤§é‡é”™è¯¯ï¼Œç»“æ„åŒ–åˆ†å—å°†éš¾ä»¥å¥æ•ˆï¼Œç”šè‡³å¯èƒ½äº§ç”Ÿé”™è¯¯çš„åˆ†å—ã€‚
    - **è¿‡åº¦ä¾èµ–ï¼š**Â è¿‡åº¦ä¾èµ–ç»“æ„å¯èƒ½å¯¼è‡´ä¸¢å¤±ä¸€äº›æœªåœ¨ç»“æ„ä¸­ä½“ç°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
- **è¯­ä¹‰åˆ†å—ï¼š**
    - **è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼š**Â ç”ŸæˆåµŒå…¥å’Œè®¡ç®—ç›¸ä¼¼åº¦éœ€è¦æ¶ˆè€—å¤§é‡è®¡ç®—èµ„æºå’Œæ—¶é—´ï¼Œå¯¹äºè¶…å¤§å‹æ–‡æ¡£å¯èƒ½ä¸å®ç”¨ã€‚
    - **é˜ˆå€¼æ•æ„Ÿï¼š**Â è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼çš„è®¾å®šå¯¹åˆ†å—ç»“æœå½±å“å¾ˆå¤§ï¼Œéœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†è¿›è¡Œè°ƒä¼˜ï¼Œæ²¡æœ‰æ™®é€‚çš„æœ€ä½³å€¼ã€‚
    - **æ¨¡å‹ä¾èµ–ï¼š**Â åˆ†å—è´¨é‡é«˜åº¦ä¾èµ–äºæ‰€é€‰åµŒå…¥æ¨¡å‹æ•è·è¯­ä¹‰çš„èƒ½åŠ›ã€‚

## **å—é—´é‡å  (Chunk Overlap)**

æ— è®ºé‡‡ç”¨ä½•ç§åˆ†å—ç­–ç•¥ï¼Œå°¤å…¶æ˜¯åœ¨å›ºå®šå¤§å°åˆ†å—ä¸­ï¼Œå—çš„è¾¹ç•Œå¤„éƒ½å¯èƒ½å‘ç”Ÿä¸Šä¸‹æ–‡å‰²è£‚ï¼Œå¯¼è‡´ä¸€ä¸ªå®Œæ•´çš„è¯­ä¹‰å•å…ƒè¢«æ— æ„ä¸­æ‹†åˆ†åˆ°ä¸¤ä¸ªç›¸é‚»çš„å—ä¸­ã€‚é‡å æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ç¼“è§£æªæ–½ã€‚å®ƒè®©ä¸€ä¸ªå—çš„ç»“å°¾éƒ¨åˆ†ï¼ŒåŒæ—¶æˆä¸ºä¸‹ä¸€ä¸ªå—çš„èµ·å§‹éƒ¨åˆ†ï¼Œä»è€Œåœ¨æ£€ç´¢æ—¶æä¾›ä¸€ç§â€œå†—ä½™â€çš„ä¸Šä¸‹æ–‡ã€‚

- **ç›®çš„ï¼š**Â ä¸»è¦ç›®çš„æ˜¯ç¡®ä¿å³ä½¿æ£€ç´¢å‘½ä¸­äº†æŸä¸ªå—çš„è¾¹ç•Œéƒ¨åˆ†ï¼Œä¹Ÿèƒ½é€šè¿‡é‡å åŒ…å«å‰/åå—çš„å…³é”®ä¿¡æ¯ï¼Œé¿å…ä¸Šä¸‹æ–‡ä¸¢å¤±ã€‚å®ƒå°±åƒåœ¨æ¯ä¸ªå—çš„è¾¹ç¼˜åŠ ä¸Šä¸€ä¸ªâ€œå®‰å…¨è¾¹é™…â€ã€‚
- **å¦‚ä½•å·¥ä½œï¼š**Â å½“ä¸€ä¸ªæ–‡æ¡£è¢«åˆ†å‰²æˆNä¸ªå—æ—¶ï¼Œæ¯ä¸ªå—Chunk_iå°†åŒ…å«å®ƒè‡ªèº«çš„å†…å®¹ä»¥åŠChunk_{i-1}çš„æœ«å°¾overlap_sizeéƒ¨åˆ†ã€‚
- **ä¼˜ç‚¹ï¼š**Â ç®€å•æ˜“è¡Œï¼Œèƒ½æœ‰æ•ˆç¼“è§£è¾¹ç•Œä¸Šä¸‹æ–‡ä¸¢å¤±é—®é¢˜ï¼Œæé«˜æ£€ç´¢åˆ°å®Œæ•´è¯­ä¹‰å•å…ƒçš„æ¦‚ç‡ã€‚
- **ç¼ºç‚¹ï¼š**Â å¼•å…¥äº†æ•°æ®å†—ä½™ï¼Œä¼šå¢åŠ å‘é‡æ•°æ®åº“çš„å­˜å‚¨ç©ºé—´å’Œç´¢å¼•å¤§å°ã€‚æ£€ç´¢æ—¶å¯èƒ½è¿”å›åŒ…å«å¤§é‡é‡å¤å†…å®¹çš„å¤šä¸ªå—ï¼Œå¢åŠ LLMå¤„ç†çš„tokensæ•°é‡ã€‚

### **ä¸€ä¸ªå·§å¦™çš„è®¾è®¡â€”â€”â€œæ£€ç´¢æ—¶ä¸Šä¸‹æ–‡æ‰©å±•â€ (Retrieval-time Context Expansion):**

è¿™æ˜¯ä¸€ç§æ›´é«˜çº§çš„é‡å æ€æƒ³ï¼Œå®ƒå°†é‡å çš„å®ç°ä»åˆ†å—é˜¶æ®µæ¨è¿Ÿåˆ°æ£€ç´¢é˜¶æ®µï¼Œä»è€Œä¼˜åŒ–äº†å­˜å‚¨å’Œçµæ´»æ€§ã€‚

1. **åˆ†å—ï¼ˆæ— é‡å æˆ–æœ€å°é‡å ï¼‰ï¼š**Â å°†æ–‡æ¡£åˆ‡åˆ†æˆç‹¬ç«‹çš„ã€å°½å¯èƒ½åŸå­åŒ–çš„å— [C1, C2, C3, C4, ...]ã€‚è¿™äº›å—åœ¨åˆ›å»ºæ—¶å¯ä»¥ä¸åŒ…å«æˆ–åªåŒ…å«éå¸¸å°çš„é‡å ã€‚
2. **æ£€ç´¢ï¼š**Â å‡å¦‚ç”¨æˆ·çš„æŸ¥è¯¢å‘½ä¸­äº†å— C3ã€‚
3. **åŠ¨æ€æ‰©å±•ï¼š**Â åœ¨å°†C3é€å…¥LLMä¹‹å‰ï¼Œä¸æ˜¯ç›´æ¥ä½¿ç”¨C3ï¼Œè€Œæ˜¯æ ¹æ®é¢„è®¾çš„ç­–ç•¥ï¼ŒåŠ¨æ€åœ°å°†å®ƒçš„å‰ä¸€ä¸ªå—C2å’Œåä¸€ä¸ªå—C4æ‹¼æ¥èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªæ›´å®Œæ•´çš„ä¸Šä¸‹æ–‡ Context = C2 + C3 + C4ã€‚è¿™ä¸ªâ€œæ‰©å±•â€çš„èŒƒå›´ï¼ˆä¾‹å¦‚ï¼Œæ‰©å±•å¤šå°‘ä¸ªå‰/åå—ï¼‰å¯ä»¥åœ¨æ£€ç´¢æ—¶åŠ¨æ€è°ƒæ•´ï¼Œç”šè‡³æ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§æˆ–ç›¸å…³æ€§åˆ†æ•°è¿›è¡Œæ™ºèƒ½åˆ¤æ–­ã€‚
    - **ä¼˜ç‚¹ï¼š**
        - **ç´¢å¼•ç®€æ´æ€§ï¼š**Â å‘é‡æ•°æ®åº“ä¸­å­˜å‚¨çš„æ¯ä¸ªå—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œæ²¡æœ‰æˆ–åªæœ‰å¾ˆå°‘çš„å†—ä½™ï¼ŒèŠ‚çœäº†å­˜å‚¨ç©ºé—´å’Œç´¢å¼•æ„å»ºæ—¶é—´ã€‚
        - **ç”Ÿæˆé˜¶æ®µçµæ´»æ€§ï¼š**Â å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚ï¼Œåœ¨æ£€ç´¢åˆ°ç›¸å…³å—åï¼Œçµæ´»åœ°å†³å®šå‘LLMæä¾›å¤šå¤§èŒƒå›´çš„ä¸Šä¸‹æ–‡ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœC3éå¸¸ç›¸å…³ï¼Œå¯èƒ½åªæ‰©å±•C2å’ŒC4ï¼›å¦‚æœC3ç›¸å…³æ€§ä¸€èˆ¬ï¼Œå¯èƒ½æ‰©å±•C1-C5ä»¥æä¾›æ›´å¹¿é˜”çš„èƒŒæ™¯ï¼‰ã€‚
        - **é¿å…å†—ä½™ï¼š**Â å‡å°‘äº†LLMå¤„ç†é‡å¤ä¿¡æ¯çš„å¯èƒ½æ€§ï¼Œé™ä½äº†LLMçš„æ¨ç†æˆæœ¬ã€‚
    - **ç¼ºç‚¹ï¼š**
        - **å®ç°å¤æ‚æ€§å¢åŠ ï¼š**Â éœ€è¦é¢å¤–çš„é€»è¾‘æ¥ç®¡ç†å—ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶åœ¨æ£€ç´¢ååŠ¨æ€åœ°è¿›è¡Œæ‹¼æ¥ã€‚è¿™å¯èƒ½éœ€è¦å¯¹æ–‡æ¡£ç»“æ„æœ‰æ›´æ·±çš„ç†è§£æˆ–ç»´æŠ¤é¢å¤–çš„å…ƒæ•°æ®ï¼ˆå¦‚å—çš„é¡ºåºIDï¼‰ã€‚
        - **éœ€è¦é«˜æ•ˆçš„å—æŸ¥æ‰¾ï¼š**Â åŠ¨æ€æ‰©å±•éœ€è¦èƒ½å¤Ÿå¿«é€Ÿå®šä½è¢«æ£€ç´¢å—çš„ç›¸é‚»å—ã€‚

### **å¯æ‰§è¡Œä»£ç ç¤ºä¾‹ (LangChainå®ç°é‡å ):**

åœ¨å›ºå®šå¤§å°åˆ†å—çš„ç¤ºä¾‹ä¸­å·²ç»åŒ…å«äº†chunk_overlapå‚æ•°çš„ä½¿ç”¨ã€‚è¿™é‡Œä¸å†é‡å¤å±•ç¤ºï¼Œè€Œæ˜¯å¼ºè°ƒå…¶åœ¨ä¸åŒç­–ç•¥ä¸­çš„é€šç”¨æ€§ã€‚

![image.png](attachment:fef6c6aa-6a38-4109-a4b0-d3ffaa41a847:image.png)

### **å¯èƒ½ç¢°åˆ°çš„é—®é¢˜ï¼š**

- **é‡å å¤§å°çš„é€‰æ‹©ï¼š**Â è¿‡å°çš„é‡å å¯èƒ½æ— æ³•æœ‰æ•ˆç¼“è§£ä¸Šä¸‹æ–‡å‰²è£‚ï¼Œè¿‡å¤§çš„é‡å åˆ™ä¼šå¼•å…¥è¿‡å¤šå†—ä½™ï¼Œå¢åŠ LLMçš„è¾“å…¥é•¿åº¦å’Œå¤„ç†æˆæœ¬ï¼Œç”šè‡³å¯èƒ½ç¨€é‡Šæ ¸å¿ƒä¿¡æ¯ã€‚
- **é‡å å†—ä½™ï¼š**Â æ— è®ºæ˜¯å›ºå®šé‡å è¿˜æ˜¯æ£€ç´¢æ—¶æ‰©å±•ï¼Œéƒ½å­˜åœ¨ä¸€å®šç¨‹åº¦çš„å†—ä½™ã€‚å¦‚æœLLMåœ¨å¤„ç†è¿™äº›é‡å éƒ¨åˆ†æ—¶æ²¡æœ‰è¢«æ˜ç¡®å‘ŠçŸ¥æˆ–å¤„ç†å¾—å½“ï¼Œå¯èƒ½ä¼šå½±å“å…¶æ€§èƒ½æˆ–å¯¼è‡´é‡å¤ä¿¡æ¯ã€‚
- **æ£€ç´¢æ—¶ä¸Šä¸‹æ–‡æ‰©å±•çš„é¢å¤–å¼€é”€ï¼š**Â å°½ç®¡åœ¨å­˜å‚¨ä¸Šæ›´é«˜æ•ˆï¼Œä½†åœ¨æ£€ç´¢å’Œç”Ÿæˆé˜¶æ®µï¼ŒåŠ¨æ€æ‹¼æ¥çš„é€»è¾‘ä¼šå¸¦æ¥ä¸€äº›é¢å¤–çš„è®¡ç®—å’Œç®¡ç†å¼€é”€ã€‚

# === Aç¯‡ï¼šä¸‡ä¸ˆé«˜æ¥¼å¹³åœ°èµ·/A4ï¼šç´¢å¼•æŠ€æœ¯ (Indexing)â€”â€”æ„å»ºé€šå¾€çŸ¥è¯†çš„é«˜é€Ÿå…¬è·¯.md ===

# A4ï¼šç´¢å¼•æŠ€æœ¯ (Indexing)â€”â€”æ„å»ºé€šå¾€çŸ¥è¯†çš„é«˜é€Ÿå…¬è·¯

ç»è¿‡äº†æ¸…æ´—ã€åˆ†å—å’Œå…ƒæ•°æ®ä¸°å¯Œï¼Œæˆ‘ä»¬çš„çŸ¥è¯†åº“ç°åœ¨å°±åƒä¸€ä¸ªè£…æ»¡äº†æ— æ•°å¼ é«˜è´¨é‡å†…å®¹å¡ç‰‡çš„å·¨å¤§ä»“åº“ã€‚ä½†æ˜¯ï¼Œå¦‚æœè¿™äº›å¡ç‰‡åªæ˜¯éšæ„å †æ”¾ï¼Œé‚£ä¹ˆæ¯æ¬¡éœ€è¦æŸ¥æ‰¾ä¿¡æ¯æ—¶ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ä¸€å¼ ä¸€å¼ åœ°ç¿»æ‰¾ï¼Œè¿™æ— å¼‚äºå¤§æµ·æé’ˆã€‚ç´¢å¼•ï¼ˆIndexingï¼‰æŠ€æœ¯ï¼Œå°±æ˜¯ä¸ºè¿™ä¸ªæ— åºçš„ä»“åº“ï¼Œå»ºç«‹ä¸€å¥—é«˜æ•ˆã€æ™ºèƒ½çš„**å›¾ä¹¦ç®¡ç†ç³»ç»Ÿ**ã€‚æƒ³è±¡ä¸€ä¸‹è¿›å…¥ä¸€ä¸ª**å›½å®¶çº§çš„å›¾ä¹¦é¦†**ã€‚

- **æ²¡æœ‰ç´¢å¼•ï¼š**Â ä½ ä¼šè¢«æ•°ç™¾ä¸‡å†Œè—ä¹¦æ·¹æ²¡ï¼Œæƒ³æ‰¾ä¸€æœ¬å…³äºâ€œé‡å­ç‰©ç†â€çš„ä¹¦ï¼Œå¯èƒ½éœ€è¦èŠ±ä¸Šå‡ å¹´æ—¶é—´ã€‚
- **æœ‰äº†ç´¢å¼•ï¼š**Â ä½ å¯ä»¥èµ°åˆ°ç”µè„‘å‰ï¼Œä½¿ç”¨å›¾ä¹¦é¦†çš„**æ£€ç´¢ç³»ç»Ÿï¼ˆç´¢å¼•ï¼‰**ã€‚
    - ä½ å¯ä»¥é€šè¿‡**ä¹¦åã€ä½œè€…ï¼ˆå…³é”®è¯ï¼‰è¿›è¡Œç²¾ç¡®æŸ¥æ‰¾ï¼Œç³»ç»Ÿä¼šç«‹åˆ»å‘Šè¯‰ä½ å®ƒåœ¨å“ªä¸ªä¹¦æ¶çš„å“ªä¸€å±‚ã€‚è¿™å°±æ˜¯ç¨€ç–ç´¢å¼•**ã€‚
    - ä½ ä¹Ÿå¯ä»¥æ¨¡ç³Šåœ°æœç´¢â€œå…³äºæ—¶ç©ºæ—…è¡Œçš„ç†è®ºâ€ï¼Œç³»ç»Ÿä¸ä»…ä¼šæ¨èç§‘å¹»å°è¯´ï¼Œè¿˜å¯èƒ½æ¨èâ€œå¹¿ä¹‰ç›¸å¯¹è®ºâ€å’Œâ€œå¼¦ç†è®ºâ€ç­‰ç›¸å…³ç‰©ç†å­¦è‘—ä½œï¼Œå› ä¸ºå®ƒç†è§£äº†ä½ æŸ¥è¯¢èƒŒåçš„**æ¦‚å¿µå’Œæ„å›¾**ã€‚è¿™å°±æ˜¯**å¯†é›†ç´¢å¼•**ã€‚

ç´¢å¼•æŠ€æœ¯ï¼Œå°±æ˜¯å°†æˆ‘ä»¬å¤„ç†å¥½çš„æ•°æ®å—ï¼ˆChunksï¼‰ç»„ç»‡èµ·æ¥ï¼Œæ„å»ºèµ·ä¸€æ¡æ¡é€šå¾€æ­£ç¡®ä¿¡æ¯çš„é«˜é€Ÿå…¬è·¯ï¼Œä½¿å¾—æ£€ç´¢è¿‡ç¨‹ä»â€œéå†æœç´¢â€çš„ä¹¡é—´å°è·¯ï¼Œå‡çº§ä¸ºâ€œå³æ—¶å®šä½â€çš„è¶…çº§å¹²çº¿ã€‚

```mermaid
graph TD
    A[å¤„ç†å¥½çš„æ–‡æœ¬å—<br>+ å…ƒæ•°æ®] --> B{ç´¢å¼•æ„å»ºæµç¨‹};
    
    subgraph "A4: ç´¢å¼•æŠ€æœ¯"
        B --> C{é€‰æ‹©ç´¢å¼•ç±»å‹}
        C --> D["1.ç¨€ç–ç´¢å¼•<br>(é”®è¯é©±åŠ¨)"]
        C --> E["2.å¯†é›†ç´¢å¼•<br>(è¯­ä¹‰é©±åŠ¨)"]
        C --> F["3.æ··åˆç´¢å¼•<br>(åŒç®¡é½ä¸‹)"]

        D -- "å…³é”®è¯ç´¢å¼• (e.g., BM25)" --> G((ç´¢å¼•åº“));
        E -- "å‘é‡ç´¢å¼• (e.g., FAISS, HNSW)" --> G;
        F -- "ç»“åˆä¸¤ç§ç´¢å¼•" --> G;
    end
    
    H[ç”¨æˆ·æŸ¥è¯¢] --> I{æ£€ç´¢å™¨};
    I --> G;
    G -- "è¿”å›ç›¸å…³ç»“æœ" --> I;
```

## **ç¨€ç–ç´¢å¼• (Sparse Indexing) - å…³é”®è¯çš„ç²¾å‡†å®šä½**

ç¨€ç–ç´¢å¼•æ˜¯ä¼ ç»Ÿä¿¡æ¯æ£€ç´¢çš„åŸºçŸ³ã€‚å®ƒé€šè¿‡ä¸€ä¸ªâ€œå€’æ’ç´¢å¼•â€ï¼ˆInverted Indexï¼‰æ¥å®ç°ï¼Œå³è®°å½•æ¯ä¸ªè¯å‡ºç°åœ¨å“ªäº›æ–‡æ¡£ä¸­ã€‚BM25 (Best Matching 25) æ˜¯ç›®å‰æœ€æµè¡Œå’Œæœ€æœ‰æ•ˆçš„ç¨€ç–æ£€ç´¢ç®—æ³•ã€‚

- **å·¥ä½œåŸç†ï¼ˆç›´è§‚ç†è§£ï¼‰ï¼š**Â BM25åŸºäºä¸¤ä¸ªæ ¸å¿ƒæ€æƒ³æ¥ç»™æ–‡æ¡£æ‰“åˆ†ï¼š
    1. **è¯é¢‘ (Term Frequency - TF):**Â ä¸€ä¸ªè¯åœ¨æŸæ–‡æ¡£ä¸­å‡ºç°æ¬¡æ•°è¶Šå¤šï¼Œè¯¥è¯ä¸è¯¥æ–‡æ¡£çš„ç›¸å…³æ€§å°±è¶Šé«˜ã€‚
    2. **é€†æ–‡æ¡£é¢‘ç‡ (Inverse Document Frequency - IDF):**Â ä¸€ä¸ªè¯åœ¨**æ‰€æœ‰**æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°è¶Šå°‘ï¼ˆè¶Šç½•è§ï¼‰ï¼Œå®ƒå¯¹äºåŒºåˆ†æ–‡æ¡£çš„é‡è¦æ€§å°±è¶Šå¤§ã€‚ä¾‹å¦‚ï¼Œâ€œçš„â€ã€â€œæ˜¯â€è¿™ç§è¯å‡ ä¹æ¯ç¯‡æ–‡æ¡£éƒ½æœ‰ï¼ŒIDFå€¼å¾ˆä½ï¼›è€Œâ€œå¤¸å…‹â€è¿™ä¸ªè¯å¾ˆç½•è§ï¼ŒIDFå€¼å°±å¾ˆé«˜ã€‚

> BM25 (Best Matching 25) æ˜¯ä¿¡æ¯æ£€ç´¢é¢†åŸŸçš„ä¸€ä¸ªç»å…¸æ’åºå‡½æ•°ï¼Œç”±Stephen E. Robertsonç­‰äººåœ¨20ä¸–çºª90å¹´ä»£çš„TRECä¼šè®®é¡¹ç›®ä¸­å¼€å‘ï¼Œæ˜¯TF-IDFæ¨¡å‹çš„é‡å¤§æ”¹è¿›ã€‚
> 
- **ç±»æ¯”ï¼šä¼ ç»Ÿçš„å›¾ä¹¦é¦†å¡ç‰‡ç›®å½•**

ç¨€ç–ç´¢å¼•å°±åƒä¸€ä¸ªå·¨å¤§çš„**å¡ç‰‡ç›®å½•æŸœ**ã€‚æ¯ä¸ªæŠ½å±‰æŒ‰å…³é”®è¯ï¼ˆå¦‚â€œç‰©ç†å­¦â€ã€â€œå†å²â€ï¼‰çš„å­—æ¯é¡ºåºæ’åˆ—ã€‚ä½ æ‹‰å¼€â€œç‰©ç†å­¦â€çš„æŠ½å±‰ï¼Œé‡Œé¢æ˜¯ä¸€æ’å¡ç‰‡ï¼Œæ¯å¼ å¡ç‰‡éƒ½æŒ‡å‘ä¸€æœ¬åŒ…å«â€œç‰©ç†å­¦â€è¿™ä¸ªè¯çš„ä¹¦ç±ã€‚æŸ¥æ‰¾ç²¾ç¡®çš„æœ¯è¯­éå¸¸å¿«ã€‚

- **ä»£ç ç¤ºä¾‹ (ä½¿ç”¨Â rank_bm25)**

```python
# å‡†å¤‡ç¯å¢ƒ:
# pip install rank_bm25 numpy
# å¦‚æœéœ€è¦å¤„ç†ä¸­æ–‡ç­‰éè‹±è¯­æ–‡æœ¬ï¼Œå»ºè®®å®‰è£…jiebaè¿›è¡Œåˆ†è¯
# pip install jieba

import numpy as np
from rank_bm25 import BM25Okapi
# import jieba # å¦‚æœå¤„ç†ä¸­æ–‡ï¼Œå–æ¶ˆæ­¤è¡Œæ³¨é‡Š

# 1. å‡†å¤‡æˆ‘ä»¬çš„æ–‡æ¡£åº“ (Corpus)
corpus = [
    "A RAG system combines a retriever with a language model.",
    "The retriever component is crucial for finding relevant context.",
    "BM25 is a popular sparse retriever algorithm.",
    "Hybrid search combines sparse and dense retrieval methods."
]

# 2. å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯ (Tokenization)
# å¯¹äºè‹±æ–‡ï¼Œç®€å•çš„æŒ‰ç©ºæ ¼åˆ†å‰²å³å¯ã€‚
# å¯¹äºä¸­æ–‡ï¼Œéœ€è¦ä½¿ç”¨ä¸“é—¨çš„åˆ†è¯å™¨ã€‚
tokenized_corpus = [doc.lower().split(" ") for doc in corpus]
# def tokenize_chinese(doc):
#     return list(jieba.cut(doc))
# tokenized_corpus = [tokenize_chinese(doc) for doc in corpus]

# 3. åˆ›å»º BM25 ç´¢å¼•å¯¹è±¡
# BM25Okapi æ˜¯BM25ç®—æ³•çš„ä¸€ä¸ªæ ‡å‡†å®ç°
bm25 = BM25Okapi(tokenized_corpus)
print("BM25 index created successfully.")

# 4. å®šä¹‰ä¸€ä¸ªæŸ¥è¯¢å¹¶è¿›è¡Œåˆ†è¯
query = "sparse retriever"
tokenized_query = query.lower().split(" ")
print(f"\nPerforming query: '{query}'")

# 5. ä½¿ç”¨ get_top_n æ–¹æ³•è·å–æœ€ç›¸å…³çš„ N ä¸ªæ–‡æ¡£
# BM25ä¼šè®¡ç®—æŸ¥è¯¢ä¸­çš„æ¯ä¸ªè¯ä¸è¯­æ–™åº“ä¸­æ‰€æœ‰æ–‡æ¡£çš„ç›¸å…³æ€§åˆ†æ•°
top_n_docs = bm25.get_top_n(tokenized_query, corpus, n=2)

print("\n--- Top 2 Relevant Documents (BM25) ---")
for i, doc in enumerate(top_n_docs):
    print(f"{i+1}: {doc}")

# ä½ ä¹Ÿå¯ä»¥è·å–æ‰€æœ‰æ–‡æ¡£çš„åˆ†æ•°
doc_scores = bm25.get_scores(tokenized_query)
print("\n--- All Document Scores (BM25) ---")
for i, score in enumerate(doc_scores):
    print(f"Doc {i+1}: {score:.4f}")
```

## **å¯†é›†ç´¢å¼• (Dense Indexing) - è¯­ä¹‰çš„ä¸–ç•Œ:**

å¯†é›†ç´¢å¼•ä¸å…³å¿ƒè¡¨é¢çš„å…³é”®è¯ï¼Œè€Œæ˜¯å…³å¿ƒæ–‡æœ¬èƒŒåæ·±å±‚çš„**è¯­ä¹‰**ã€‚å®ƒé€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆåµŒå…¥æ¨¡å‹ï¼‰å°†æ¯ä¸ªæ–‡æœ¬å—è½¬æ¢ä¸ºä¸€ä¸ªé«˜ç»´æµ®ç‚¹æ•°å‘é‡ã€‚æŸ¥è¯¢æ—¶ï¼Œç”¨æˆ·çš„æŸ¥è¯¢ä¹Ÿè¢«è½¬æ¢ä¸ºä¸€ä¸ªå‘é‡ï¼Œç³»ç»Ÿé€šè¿‡è®¡ç®—å‘é‡é—´çš„è·ç¦»ï¼ˆå¦‚ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰æ¥æ‰¾åˆ°è¯­ä¹‰ä¸Šæœ€æ¥è¿‘çš„å†…å®¹ã€‚

ç”±äºåœ¨é«˜ç»´ç©ºé—´ä¸­è¿›è¡Œç²¾ç¡®æœç´¢ï¼ˆæš´åŠ›æœç´¢ï¼‰å­˜åœ¨â€œç»´åº¦è¯…å’’â€ï¼Œæ‰€æœ‰ç°ä»£å‘é‡æ•°æ®åº“éƒ½ä½¿ç”¨è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰ç®—æ³•æ¥æ„å»ºç´¢å¼•ï¼Œä»¥å®ç°æ¯«ç§’çº§çš„å¿«é€Ÿæœç´¢ã€‚

å¯†é›†ç´¢å¼•å°±åƒä¸€ä½**ç»éªŒä¸°å¯Œä¸”æœ‰è¯»å¿ƒæœ¯çš„å›¾ä¹¦ç®¡ç†å‘˜**ã€‚ä½ ä¸ç”¨å‘Šè¯‰ä»–å®Œæ•´çš„ä¹¦åï¼Œä½ åªéœ€è¦æè¿°ä½ æƒ³äº†è§£çš„**æ¦‚å¿µ**ï¼ˆä¾‹å¦‚ï¼Œâ€œæˆ‘æƒ³äº†è§£é‚£ç§èƒ½è®©é£èˆ¹æ¯”å…‰è¿˜å¿«çš„æ—…è¡Œæ–¹

- **ä»£ç ç¤ºä¾‹ (ä½¿ç”¨Â faiss-cpu)**

```python
# å‡†å¤‡ç¯å¢ƒ:
# pip install faiss-cpu sentence-transformers numpy
# faiss-cpu æ˜¯Facebook AIçš„å‘é‡æœç´¢åº“çš„CPUç‰ˆæœ¬ï¼Œéå¸¸é«˜æ•ˆã€‚

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# 1. å‡†å¤‡æ–‡æ¡£åº“ (ä¸BM25ç¤ºä¾‹ç›¸åŒ)
corpus = [
    "A RAG system combines a retriever with a language model.",
    "The retriever component is crucial for finding relevant context.",
    "BM25 is a popular sparse retriever algorithm.",
    "Hybrid search combines sparse and dense retrieval methods.",
    "An automobile is a wheeled motor vehicle used for transportation." # æ–°å¢ä¸€ä¸ªè¯­ä¹‰ç›¸å…³çš„ä¾‹å­
]

# 2. åŠ è½½åµŒå…¥æ¨¡å‹å¹¶è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„åµŒå…¥å‘é‡
print("Loading embedding model and encoding corpus...")
model = SentenceTransformer('all-MiniLM-L6-v2')
doc_embeddings = model.encode(corpus)
print("Encoding complete. Shape of embeddings:", doc_embeddings.shape)

# 3. æ„å»ºFAISSç´¢å¼•
# è·å–å‘é‡çš„ç»´åº¦
d = doc_embeddings.shape[1] 
# IndexFlatL2 æ˜¯ä¸€ä¸ªåŸºç¡€çš„ã€ä½¿ç”¨L2è·ç¦»è¿›è¡Œç²¾ç¡®æœç´¢çš„ç´¢å¼•ã€‚
# å®ƒæ²¡æœ‰ä½¿ç”¨ANNï¼Œä½†å¯¹äºå°å‹æ•°æ®é›†ï¼Œæ˜¯å¾ˆå¥½çš„å…¥é—¨å’ŒåŸºå‡†ã€‚
# åœ¨Dç¯‡ä¸­ï¼Œæˆ‘ä»¬ä¼šæ¢è®¨æ›´é«˜çº§çš„ANNç´¢å¼•å¦‚IndexIVFFlatå’ŒIndexHNSWFlatã€‚
index = faiss.IndexFlatL2(d)

# 4. æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²ç»è®­ç»ƒ (å¯¹äºIndexFlatL2ï¼Œæ— éœ€è®­ç»ƒ)
print("\nIs FAISS index trained?", index.is_trained)

# 5. å°†æ–‡æ¡£å‘é‡æ·»åŠ åˆ°ç´¢å¼•ä¸­
index.add(doc_embeddings)
print(f"Number of vectors in the index: {index.ntotal}")

# 6. å®šä¹‰ä¸€ä¸ªæŸ¥è¯¢å¹¶è¿›è¡ŒåµŒå…¥
# æ³¨æ„è¿™ä¸ªæŸ¥è¯¢ä½¿ç”¨äº†"car"ï¼Œè€Œä¸æ˜¯è¯­æ–™åº“ä¸­çš„"automobile"
query_text = "What is a car?"
query_embedding = model.encode([query_text]) # ç¼–ç æ—¶éœ€è¦ä¼ å…¥åˆ—è¡¨
print(f"\nPerforming semantic query: '{query_text}'")

# 7. ä½¿ç”¨ index.search() æ–¹æ³•è¿›è¡Œæœç´¢
k = 2 # æˆ‘ä»¬æƒ³æ‰¾æœ€ç›¸ä¼¼çš„2ä¸ªç»“æœ
distances, indices = index.search(query_embedding, k)

print("\n--- Top 2 Relevant Documents (FAISS/Dense) ---")
for i in range(k):
    doc_index = indices[0][i]
    distance = distances[0][i]
    print(f"{i+1}: Document '{corpus[doc_index]}' (Distance: {distance:.4f})")
# æœŸæœ›çœ‹åˆ°å…³äº"automobile"çš„æ–‡æ¡£è¢«é«˜åˆ†å¬å›ï¼Œå› ä¸ºå®ƒä¸"car"è¯­ä¹‰æœ€æ¥è¿‘ã€‚
```

## **æ··åˆæ£€ç´¢ (Hybrid Search)â€”â€”é±¼ä¸ç†ŠæŒå…¼å¾—**

- **è¯¦ç»†é˜è¿°ï¼š**Â æ—¢ç„¶ç¨€ç–ç´¢å¼•å’Œå¯†é›†ç´¢å¼•å„æœ‰ä¼˜åŠ£ï¼Œæœ€å¼ºå¤§çš„ç­–ç•¥å°±æ˜¯å°†å®ƒä»¬ç»“åˆèµ·æ¥ã€‚æ··åˆæ£€ç´¢åŒæ—¶åˆ©ç”¨å…³é”®è¯åŒ¹é…çš„ç²¾ç¡®æ€§å’Œè¯­ä¹‰æœç´¢çš„æ³›åŒ–èƒ½åŠ›ã€‚
- **å·§å¦™è®¾è®¡â€”â€”å€’æ•°æ’åºèåˆ (Reciprocal Rank Fusion - RRF):**
    - **é—®é¢˜ï¼š**Â BM25çš„å¾—åˆ†ï¼ˆä¾‹å¦‚ï¼Œ0-40ï¼‰å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ-1åˆ°1ï¼‰çš„èŒƒå›´å’Œåˆ†å¸ƒå®Œå…¨ä¸åŒï¼Œæ— æ³•ç›´æ¥ç›¸åŠ ã€‚
    - **RRFçš„è§£å†³æ–¹æ¡ˆï¼š**Â RRFå®Œå…¨å¿½ç•¥åŸå§‹åˆ†æ•°ï¼Œåªå…³å¿ƒæ¯ä¸ªæ–‡æ¡£åœ¨å„è‡ªæ£€ç´¢ç»“æœåˆ—è¡¨ä¸­çš„**æ’å**ã€‚
    - **ç®—æ³•ï¼š**
        1. åˆ†åˆ«ä»BM25å’Œå‘é‡æœç´¢å¾—åˆ°ä¸¤ä¸ªæ’ååˆ—è¡¨ã€‚
        2. å¯¹äºçŸ¥è¯†åº“ä¸­çš„æ¯ä¸ªæ–‡æ¡£dï¼Œè®¡ç®—å…¶RRFåˆ†æ•°ï¼šRRF_Score(d) = 1 / (k + rank_bm25(d)) + 1 / (k + rank_vector(d))
        3. rank(d)æ˜¯æ–‡æ¡£dåœ¨æŸä¸ªåˆ—è¡¨ä¸­çš„æ’åã€‚å¦‚æœæ–‡æ¡£æœªå‡ºç°åœ¨åˆ—è¡¨ä¸­ï¼Œåˆ™è¯¥é¡¹ä¸º0ã€‚kæ˜¯ä¸€ä¸ªå°çš„å¸¸æ•°ï¼ˆé€šå¸¸è®¾ä¸º60ï¼‰ï¼Œç”¨äºé™ä½é«˜æ’åæ–‡æ¡£çš„æƒé‡å½±å“ã€‚
        4. æŒ‰æœ€ç»ˆçš„RRFåˆ†æ•°é‡æ–°æ’åºæ‰€æœ‰æ–‡æ¡£ã€‚

> Reciprocal Rank Fusion æ˜¯ä¸€ç§æ•°æ®èåˆæ–¹æ³•ï¼Œåœ¨Cormack, Clarke, and Buettcheräº2009å¹´TRECä¼šè®®çš„è®ºæ–‡ "Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods" ä¸­è¢«æå‡ºå¹¶è¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚
> 

```mermaid
flowchart TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B{BM25 æ£€ç´¢};
    A --> C{å‘é‡æ£€ç´¢};
    
    B --> D[BM25 æ’ååˆ—è¡¨<br>DocA: 1<br>DocC: 2<br>DocE: 3];
    C --> E[å‘é‡æ£€ç´¢ æ’ååˆ—è¡¨<br>DocC: 1<br>DocB: 2<br>DocA: 3];
    
    subgraph "RRF èåˆ (k=60)"
        F["è®¡ç®— DocA åˆ†æ•°<br> 1 /(60+1) + 1/(60+3)"]
        G["è®¡ç®— DocB åˆ†æ•°<br>0 + 1/(60+2)"]
        H["è®¡ç®— DocC åˆ†æ•°<br>1/(60+2) + 1/(60+1)"]
        I["è®¡ç®— DocE åˆ†æ•°<br>1/(60+3) + 0"]
    end

    D --> F;
    E --> F;
    E --> G;
    D --> H;
    E --> H;
    D --> I;

    J[é‡æ–°æ’åºæ‰€æœ‰æ–‡æ¡£<br>1. DocC<br>2. DocA<br>3. DocB<br>4. DocE] --> K[æœ€ç»ˆæ··åˆæ’åç»“æœ];

```

- å¯è¡Œæ€§ä»£ç 

```python
# å‡†å¤‡ç¯å¢ƒ:
# ç¡®ä¿ rank_bm25, faiss-cpu, sentence-transformers éƒ½å·²å®‰è£…

# --- æˆ‘ä»¬å°†å¤ç”¨å‰é¢ä¸¤ä¸ªç¤ºä¾‹ä¸­åˆ›å»ºçš„å¯¹è±¡å’Œæ•°æ® ---
# å‡è®¾ bm25, index, model, corpus éƒ½å·²å­˜åœ¨äºå½“å‰ç¯å¢ƒä¸­

def hybrid_search(query: str, bm25_index, faiss_index, embedding_model, corpus_docs, k=5):
    """
    æ‰§è¡Œæ··åˆæœç´¢å¹¶ä½¿ç”¨RRFè¿›è¡Œèåˆã€‚

    Args:
        query (str): ç”¨æˆ·æŸ¥è¯¢ã€‚
        bm25_index: è®­ç»ƒå¥½çš„BM25ç´¢å¼•å¯¹è±¡ã€‚
        faiss_index: æ·»åŠ äº†å‘é‡çš„FAISSç´¢å¼•å¯¹è±¡ã€‚
        embedding_model: sentence-transformeræ¨¡å‹ã€‚
        corpus_docs (list): åŸå§‹æ–‡æ¡£åˆ—è¡¨ã€‚
        k (int): æœ€ç»ˆè¿”å›çš„ç»“æœæ•°é‡ã€‚

    Returns:
        list: ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ [(score, doc), ...]ï¼ŒæŒ‰RRFåˆ†æ•°é™åºæ’åˆ—ã€‚
    """
    # --- 1. ç¨€ç–æ£€ç´¢ (BM25) ---
    tokenized_query = query.lower().split(" ")
    bm25_scores = bm25_index.get_scores(tokenized_query)
    
    # --- 2. å¯†é›†æ£€ç´¢ (FAISS) ---
    query_embedding = embedding_model.encode([query])
    # FAISSè¿”å›è·ç¦»ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (è¿™é‡Œç”¨è´Ÿè·ç¦»ç®€å•æ¨¡æ‹Ÿ)
    # å¹¶ä¸”éœ€è¦è·å–æ‰€æœ‰æ–‡æ¡£çš„è·ç¦»ï¼Œè¿™åœ¨IndexFlatL2ä¸­æ˜¯ä½æ•ˆçš„
    # æ›´ä¼˜çš„åšæ³•æ˜¯æœç´¢ä¸€ä¸ªè¾ƒå¤§çš„Nï¼Œç„¶ååªå¤„ç†è¿™Nä¸ªç»“æœ
    # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å‡è®¾å¯ä»¥æ‹¿åˆ°æ‰€æœ‰åˆ†æ•°
    # çœŸå®åœºæ™¯ä¸­ï¼Œä½ ä¼šç”¨ faiss_index.search()
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æ‰‹åŠ¨è®¡ç®—æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦
    all_doc_embeddings = embedding_model.encode(corpus_docs)
    dense_scores = util.cos_sim(query_embedding, all_doc_embeddings)[0].numpy()

    # --- 3. RRF èåˆ ---
    # RRFå¸¸æ•°k
    RRF_K = 60
    
    # å°†åˆ†æ•°è½¬æ¢ä¸ºæ’å (æ³¨æ„ï¼šæ’åä»1å¼€å§‹)
    # argsort() è¿”å›çš„æ˜¯ä»å°åˆ°å¤§çš„ç´¢å¼•ï¼Œæ‰€ä»¥éœ€è¦åè½¬
    bm25_ranks = np.argsort(bm25_scores)[::-1].argsort() + 1
    dense_ranks = np.argsort(dense_scores)[::-1].argsort() + 1
    
    rrf_scores = {}
    for i in range(len(corpus_docs)):
        rrf_scores[i] = (1 / (RRF_K + bm25_ranks[i])) + (1 / (RRF_K + dense_ranks[i]))

    # --- 4. æ’åºå¹¶è¿”å›ç»“æœ ---
    sorted_docs_indices = sorted(rrf_scores.keys(), key=lambda i: rrf_scores[i], reverse=True)
    
    final_results = []
    for i in sorted_docs_indices[:k]:
        final_results.append({
            "doc": corpus_docs[i],
            "rrf_score": rrf_scores[i],
            "bm25_score": bm25_scores[i],
            "dense_score": dense_scores[i]
        })
        
    return final_results

# --- ç¤ºä¾‹ç”¨æ³• ---
# å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ—¢éœ€è¦å…³é”®è¯ä¹Ÿéœ€è¦è¯­ä¹‰çš„æŸ¥è¯¢
# "retriever algorithm" åŒ…å«äº†å…³é”®è¯ "retriever", "algorithm" å’Œè¯­ä¹‰
my_query = "retriever algorithm"
hybrid_results = hybrid_search(my_query, bm25, index, model, corpus)

print(f"\n--- Hybrid Search Results for query: '{my_query}' ---")
for result in hybrid_results:
    print(f"Doc: {result['doc']}")
    print(f"  - RRF Score: {result['rrf_score']:.6f} (BM25 Score: {result['bm25_score']:.2f}, Dense Score: {result['dense_score']:.2f})")

# æœŸæœ›çœ‹åˆ° BM25 å’Œ Dense retriever ç›¸å…³çš„æ–‡æ¡£éƒ½æ’åœ¨é å‰çš„ä½ç½®
```

## **é«˜çº§ç­–ç•¥ï¼šåˆ†å±‚ç´¢å¼• (Hierarchical Indexing)**

å½“çŸ¥è¯†åº“è§„æ¨¡è¾¾åˆ°æ•°ç™¾ä¸‡ç”šè‡³ä¸Šäº¿çº§åˆ«æ—¶ï¼Œç›´æ¥åœ¨æ‰€æœ‰æ–‡æ¡£å—ä¸Šè¿›è¡Œä¸€æ¬¡æ€§æ£€ç´¢ï¼Œå³ä½¿æœ‰ANNç´¢å¼•ï¼Œä¹Ÿå¯èƒ½é¢ä¸´æ€§èƒ½ç“¶é¢ˆå’Œç²¾åº¦ä¸‹é™çš„é—®é¢˜ã€‚

- **ä¸¤é˜¶æ®µæ£€ç´¢æµç¨‹:**
    1. **æ„å»ºåŒé‡ç´¢å¼•ï¼š**
        - **ç´¢å¼•ä¸€ï¼ˆæ‘˜è¦å±‚ï¼‰ï¼š**Â ä¸ºçŸ¥è¯†åº“ä¸­çš„æ¯ä¸€ç¯‡**å®Œæ•´æ–‡æ¡£**ï¼ˆæˆ–å¤§çš„ç« èŠ‚ï¼‰ï¼Œéƒ½ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ã€‚ç„¶åï¼Œåªå¯¹è¿™äº›**æ‘˜è¦**è¿›è¡ŒåµŒå…¥å’Œç´¢å¼•ã€‚
        - **ç´¢å¼•äºŒï¼ˆå†…å®¹å±‚ï¼‰ï¼š**Â æŒ‰ç…§å¸¸è§„æ–¹æ³•ï¼Œå¯¹æ‰€æœ‰æ–‡æ¡£çš„**å…·ä½“å—**è¿›è¡ŒåµŒå…¥å’Œç´¢å¼•ã€‚
    2. **æ‰§è¡Œä¸¤æ­¥æœç´¢ï¼š**
        - **ç¬¬ä¸€æ­¥ï¼ˆç²—ç­›ï¼‰ï¼š**Â ç”¨æˆ·çš„æŸ¥è¯¢é¦–å…ˆåœ¨**æ‘˜è¦å±‚ç´¢å¼•**ä¸­è¿›è¡Œæœç´¢ã€‚è¿™ä¸€æ­¥çš„ç›®æ ‡æ˜¯å¿«é€Ÿåœ°ä»æµ·é‡æ–‡æ¡£ä¸­ï¼Œç­›é€‰å‡ºå°‘æ•°å‡ ç¯‡ï¼ˆä¾‹å¦‚ï¼ŒTop 5ï¼‰æœ€å¯èƒ½ç›¸å…³çš„**å®Œæ•´æ–‡æ¡£**ã€‚
        - **ç¬¬äºŒæ­¥ï¼ˆç²¾ç­›ï¼‰ï¼š**Â æ¥ä¸‹æ¥ï¼Œç”¨æˆ·çš„æŸ¥è¯¢**åª**åœ¨è¿™5ç¯‡è¢«é€‰ä¸­çš„æ–‡æ¡£æ‰€åŒ…å«çš„**å…·ä½“å—**ï¼ˆå†…å®¹å±‚ç´¢å¼•çš„å­é›†ï¼‰ä¸­è¿›è¡Œæœç´¢ï¼Œä»è€Œæ‰¾åˆ°æœ€ç²¾ç¡®çš„ç­”æ¡ˆå—ã€‚

è¿™å°±åƒåœ¨**æŸ¥é˜…ä¸€ä¸ªåºå¤§çš„ç§‘å­¦æ–‡çŒ®åº“**ã€‚ä½ ä¸ä¼šç›´æ¥å»é˜…è¯»æ¯ä¸€ç¯‡è®ºæ–‡çš„å…¨æ–‡ã€‚ä½ ä¼šå…ˆå¿«é€Ÿæµè§ˆ**100ç¯‡è®ºæ–‡çš„æ‘˜è¦ï¼ˆç¬¬ä¸€æ­¥ï¼‰**ï¼Œæ‰¾å‡ºå…¶ä¸­ä½ è®¤ä¸ºæœ€ç›¸å…³çš„**5ç¯‡**ï¼Œç„¶åå†å»ç²¾è¯»è¿™**5ç¯‡è®ºæ–‡çš„å…¨æ–‡ï¼ˆç¬¬äºŒæ­¥ï¼‰**ã€‚è¿™ç§æ–¹æ³•æå¤§åœ°æé«˜äº†ç ”ç©¶æ•ˆç‡ã€‚

```mermaid
flowchart TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B{ç¬¬ä¸€æ­¥: æœç´¢æ‘˜è¦ç´¢å¼•};
    B --> C["æ‰¾åˆ°Top-5ç›¸å…³æ–‡æ¡£<br>(DocA, DocC, DocG, DocX, DocZ)"];
    C --> D{ç¬¬äºŒæ­¥: ä»…åœ¨é€‰å®šæ–‡æ¡£çš„<br>å—ç´¢å¼•ä¸­æœç´¢};
    D --> E[æ‰¾åˆ°æœ€ç»ˆçš„Top-K<br>ç›¸å…³æ–‡æ¡£å—];
    E --> F[é€å¾€LLM];
```

# === Aç¯‡ï¼šä¸‡ä¸ˆé«˜æ¥¼å¹³åœ°èµ·/A0ï¼šå¼€ç¯‡ï¼Œä¼˜åŒ–æ•°æ®å‡†å¤‡ä¸ç´¢å¼•.md ===

# å¼€ç¯‡: ä¼˜åŒ–æ•°æ®å‡†å¤‡ä¸ç´¢å¼•

> **æ•°æ®çš„â€œç²¾è€•ç»†ä½œâ€æ˜¯RAGæˆåŠŸçš„åŸºçŸ³**
> 

åœ¨æ„å»ºä»»ä½•ä¸€ä¸ªRAGç³»ç»Ÿæ—¶ï¼Œæˆ‘ä»¬æœ€å…ˆé¢å¯¹çš„ä¸æ˜¯ç‚«é…·çš„è¯­è¨€æ¨¡å‹ï¼Œä¹Ÿä¸æ˜¯å¤æ‚çš„æ£€ç´¢ç®—æ³•ï¼Œè€Œæ˜¯åŸå§‹ã€æ‚ä¹±ã€å½¢æ€å„å¼‚çš„æ•°æ®ã€‚æ‚¨å¯ä»¥å°†è‡ªå·±æƒ³è±¡æˆä¸€ä½ç±³å…¶æ—ä¸‰æ˜Ÿå¤§å¨ï¼Œè€Œæ•°æ®å°±æ˜¯æ‚¨çš„é£Ÿæã€‚å†é«˜è¶…çš„å¨è‰ºï¼Œä¹Ÿæ— æ³•å°†è…çƒ‚ã€åŠ£è´¨çš„é£Ÿæå˜æˆçé¦ä½³è‚´ã€‚åŒç†ï¼ŒRAGç³»ç»Ÿçš„æ•°æ®å‡†å¤‡ä¸ç´¢å¼•é˜¶æ®µï¼Œå°±æ˜¯å¯¹â€œé£Ÿæâ€è¿›è¡Œç²¾æŒ‘ç»†é€‰ã€æ¸…æ´—å¤„ç†ã€ç²¾ç»†åˆ‡å‰²çš„è¿‡ç¨‹ã€‚

è¿™ä¸ªé˜¶æ®µçš„å·¥ä½œï¼Œå¾€å¾€å æ®äº†æ•´ä¸ªRAGé¡¹ç›®70%ä»¥ä¸Šçš„ç²¾åŠ›ï¼Œä½†å®ƒä¹Ÿå†³å®šäº†ç³»ç»Ÿæ€§èƒ½çš„ä¸Šé™ã€‚ä¸€ä¸ªç»è¿‡â€œç²¾è€•ç»†ä½œâ€çš„çŸ¥è¯†åº“ï¼Œèƒ½å¤Ÿè®©åç»­çš„æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹äº‹åŠåŠŸå€ï¼›åä¹‹ï¼Œä¸€ä¸ªå……æ»¡å™ªéŸ³ã€ç»“æ„æ··ä¹±çš„çŸ¥è¯†åº“ï¼Œåˆ™ä¼šæˆä¸ºæ•´ä¸ªç³»ç»Ÿæ€§èƒ½çš„ç“¶é¢ˆï¼Œå³ä½¿æœ€å…ˆè¿›çš„LLMä¹Ÿæ— åŠ›å›å¤©ã€‚

æœ¬ç« å°†ä½œä¸ºæ‚¨çš„â€œå¤‡æ–™æŒ‡å—â€ï¼Œæ·±å…¥æ¢è®¨æ•°æ®å‡†å¤‡ä¸ç´¢å¼•çš„å››å¤§æ ¸å¿ƒæ”¯æŸ±ï¼š**æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†**ã€**åˆ†å—ç­–ç•¥**ã€**å…ƒæ•°æ®ä¸°å¯Œ**å’Œ**ç´¢å¼•æŠ€æœ¯**ã€‚æˆ‘ä»¬å°†ä»åŸºæœ¬æ¦‚å¿µå‡ºå‘ï¼Œè¾…ä»¥å¯ç›´æ¥è¿è¡Œçš„ä»£ç ç¤ºä¾‹ã€ç”ŸåŠ¨çš„ç±»æ¯”ã€å·§å¦™çš„è®¾è®¡å’Œå¸¸è§é—®é¢˜å‰–æï¼ŒåŠ©æ‚¨ä¸ºæ‚¨çš„RAGç³»ç»Ÿæ‰“ä¸‹æœ€åšå®çš„åŸºç¡€ã€‚

```mermaid
graph TD
    A[åŸå§‹æ•°æ®æº:PDF\HTML\DOCX...] --> B{æ•°æ®åŠ è½½ä¸æå–};
    B --> C[A1: æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†];
    C --> C1[åˆ é™¤é‡å¤å†…å®¹];
    C --> C2[æ ‡å‡†åŒ–æ–‡æœ¬æ ¼å¼];
    C --> C3[è¯å½¢è¿˜åŸ];
    C3 --> D[A2: åˆ†å—ç­–ç•¥];
    D --> D1[å›ºå®šå¤§å°åˆ†å—];
    D --> D2[å†…å®¹æ„ŸçŸ¥åˆ†å—];
    D --> D3[å—é—´é‡å ];
    D --> E[A3: å…ƒæ•°æ®ä¸°å¯Œ];
    E --> E1[æ·»åŠ æºå…ƒæ•°æ®];
    E --> E2[æå–å†…å®¹å…ƒæ•°æ®];
    E --> F[A4: ç´¢å¼•æŠ€æœ¯];
    F --> F1[å¯†é›†ç´¢å¼•<å‘é‡>];
    F --> F2[ç¨€ç–ç´¢å¼•<å…³é”®è¯>];
    F --> F3[æ··åˆç´¢å¼•];
    F3 --> G[æ„å»ºå®Œæˆçš„çŸ¥è¯†åº“ç´¢å¼•];

    subgraph "å‡†å¤‡é˜¶æ®µ"
        B
        C
        D
        E
        F
    end
```

- **A1: æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†â€”â€”ä¸ºçŸ¥è¯†åº“â€œæçº¯â€**
    - åˆ é™¤é‡å¤å†…å®¹ (Deduplication)
    - æ ‡å‡†åŒ–æ–‡æœ¬æ ¼å¼ (Text Normalization)
    - è¯å¹²æå–ä¸è¯å½¢è¿˜åŸ (Stemming & Lemmatization)
- **A2: åˆ†å—ç­–ç•¥ (Chunking)â€”â€”åœ¨ä¸Šä¸‹æ–‡ä¸ç²¾åº¦é—´å¯»æ±‚å¹³è¡¡**
    - å›ºå®šå¤§å°åˆ†å— (Fixed-Size Chunking)
    - å†…å®¹æ„ŸçŸ¥åˆ†å— (Content-Aware Chunking)
    - å—é—´é‡å  (Chunk Overlap)
- **A3: å…ƒæ•°æ®ä¸°å¯Œ (Metadata Enrichment)â€”â€”èµ‹äºˆæ•°æ®å¯è¿‡æ»¤çš„â€œç»´åº¦â€**
    - æºå…ƒæ•°æ® (Source Metadata)
    - å†…å®¹å…ƒæ•°æ® (Content-based Metadata)
- **A4: ç´¢å¼•æŠ€æœ¯ (Indexing)â€”â€”æ„å»ºé€šå¾€çŸ¥è¯†çš„é«˜é€Ÿå…¬è·¯**
    - ç´¢å¼•æ–¹æ³•çš„æ ¸å¿ƒåˆ†ç±»
    - æ··åˆæ£€ç´¢ (Hybrid Search)

[**A1ï¼šæ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†â€”â€”ä¸ºçŸ¥è¯†åº“â€œæçº¯â€**](https://www.notion.so/A1-25f55a58d45c8017ad72c49a99e55862?pvs=21)

[**A2ï¼šåˆ†å—ç­–ç•¥ (Chunking)â€”â€”åœ¨ä¸Šä¸‹æ–‡ä¸ç²¾åº¦é—´å¯»æ±‚å¹³è¡¡**](https://www.notion.so/A2-Chunking-25f55a58d45c8016a2dff1fd3d33b881?pvs=21)

[**A3ï¼šå…ƒæ•°æ®ä¸°å¯Œ (Metadata Enrichment)â€”â€”èµ‹äºˆæ•°æ®å¯è¿‡æ»¤çš„â€œç»´åº¦â€**](https://www.notion.so/A3-Metadata-Enrichment-25f55a58d45c8097b4ddcaa47ed715ef?pvs=21)

[**A4ï¼šç´¢å¼•æŠ€æœ¯ (Indexing)â€”â€”æ„å»ºé€šå¾€çŸ¥è¯†çš„é«˜é€Ÿå…¬è·¯**](https://www.notion.so/A4-Indexing-25f55a58d45c80bda53fe16e0b2a23d9?pvs=21)

### **å¼•ç”¨æ–‡çŒ® (References)**

- Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.Â *Advances in Neural Information Processing Systems, 33*, 9459-9474.
- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need.Â *Advances in Neural Information Processing Systems, 30*.

# === Aç¯‡ï¼šä¸‡ä¸ˆé«˜æ¥¼å¹³åœ°èµ·/A1ï¼šæ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†â€”â€”ä¸ºçŸ¥è¯†åº“â€œæçº¯â€.md ===

æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†ï¼Œå¥½æ¯”åœ¨çƒ¹é¥ªå‰â€œæ¸…æ´—å’Œå¤„ç†è”¬èœâ€ã€‚å…¶ç›®æ ‡æ˜¯æ¶ˆé™¤åŸå§‹æ•°æ®ä¸­çš„â€œæ³¥åœŸå’Œæ‚è´¨â€ï¼ˆå™ªéŸ³ï¼‰ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºä¸€ç§æ ‡å‡†åŒ–çš„ã€æœºå™¨æ˜“äºâ€œæ¶ˆåŒ–â€çš„æ ¼å¼ã€‚åœ¨RAGä¸­ï¼Œâ€œå™ªéŸ³â€ä¸ä»…æŒ‡æ— ç”¨çš„å­—ç¬¦æˆ–æ ¼å¼ï¼Œæ›´åŒ…æ‹¬ä»»ä½•å¯èƒ½è¯¯å¯¼åµŒå…¥æ¨¡å‹è¿›è¡Œè¯­ä¹‰ç†è§£çš„å› ç´ ã€‚åµŒå…¥æ¨¡å‹ä¼šå°†æ–‡æœ¬æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´ï¼Œæ–‡æœ¬ä¸­å¾®å°çš„ã€æ— æ„ä¹‰çš„å·®å¼‚éƒ½å¯èƒ½å¯¼è‡´å‘é‡ä½ç½®çš„å·¨å¤§å˜åŒ–ï¼Œä»è€Œå½±å“æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚

> æ–‡æœ¬é¢„å¤„ç†ï¼ˆText Preprocessingï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„ä¸€ä¸ªåŸºç¡€ä¸”å…³é”®çš„æ­¥éª¤ã€‚å…¶æ¦‚å¿µå’Œæ–¹æ³•åœ¨ä¸»æµNLPæ•™ç§‘ä¹¦ä¸­æœ‰è¯¦ç»†è®ºè¿°ï¼Œå¦‚Dan Jurafskyå’ŒJames H. Martinæ‰€è‘—çš„ã€ŠSpeech and Language Processingã€‹
> 

## **åˆ é™¤é‡å¤å†…å®¹ (Deduplication)**

é‡å¤å†…å®¹æ˜¯çŸ¥è¯†åº“çš„â€œè„‚è‚ªâ€ï¼Œä¸ä»…å¢åŠ äº†å­˜å‚¨å’Œè®¡ç®—æˆæœ¬ï¼Œæ›´ä¸¥é‡çš„æ˜¯å®ƒä¼šâ€œç¨€é‡Šâ€æ£€ç´¢ç»“æœã€‚å½“ç”¨æˆ·æŸ¥è¯¢æ—¶ï¼Œå¦‚æœæ£€ç´¢å™¨è¿”å›äº†ä¸‰ä»½å†…å®¹å®Œå…¨ç›¸åŒçš„æ–‡æ¡£ï¼Œè¿™ä¸ä»…å ç”¨äº†å®è´µçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œè¿˜å¯èƒ½è®©LLMåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶äº§ç”Ÿä¸å¿…è¦çš„é‡å¤ã€‚é‡å¤åˆ†ä¸ºä¸¤ç§ï¼š

- **å®Œå…¨é‡å¤ï¼š**Â æ–‡ä»¶æˆ–æ–‡æœ¬å†…å®¹ä¸€å­—ä¸å·®ã€‚
- **è¿‘ä¼¼/è¯­ä¹‰é‡å¤ï¼š**Â å†…å®¹é«˜åº¦ç›¸ä¼¼ï¼Œä¾‹å¦‚åŒä¸€ç¯‡æ–°é—»ç¨¿çš„ä¸åŒç‰ˆæœ¬ã€åŒä¸€äº§å“çš„ä¸åŒè§’åº¦æè¿°ç­‰ã€‚

### **æ–¹æ³•ä¸ç¤ºä¾‹ï¼š**

- **å®Œå…¨é‡å¤æ£€æµ‹ï¼š**Â ä½¿ç”¨å“ˆå¸Œç®—æ³•ï¼ˆå¦‚MD5, SHA256ï¼‰å¯¹æ–‡æœ¬å†…å®¹è®¡ç®—å“ˆå¸Œå€¼ã€‚å“ˆå¸Œå€¼ç›¸åŒåˆ™è§†ä¸ºå®Œå…¨é‡å¤ã€‚è¿™ç§æ–¹æ³•é€Ÿåº¦æå¿«ï¼Œé€‚åˆåˆæ­¥ç­›é€‰ã€‚
- **è¿‘ä¼¼é‡å¤æ£€æµ‹ï¼š**
    - **ç±»æ¯”ï¼š**Â è¿™å°±åƒæ˜¯ç»™æ¯ç¯‡æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªç‹¬ç‰¹çš„â€œè¯­ä¹‰æŒ‡çº¹â€ã€‚å¦‚æœä¸¤æšæŒ‡çº¹é«˜åº¦ç›¸ä¼¼ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºè¿™ä¸¤ç¯‡æ–‡æ¡£å†…å®¹ç›¸è¿‘ã€‚
    - **æ–¹æ³•ï¼š**Â å¯¹æ–‡æ¡£æˆ–å¤§å—æ–‡æœ¬è®¡ç®—åµŒå…¥å‘é‡ï¼Œç„¶åé€šè¿‡è®¡ç®—å‘é‡é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ¥åˆ¤æ–­å…¶å†…å®¹çš„ç›¸ä¼¼ç¨‹åº¦ã€‚è®¾å®šä¸€ä¸ªé˜ˆå€¼ï¼ˆä¾‹å¦‚0.98ï¼‰ï¼Œè¶…è¿‡è¯¥é˜ˆå€¼åˆ™è§†ä¸ºè¿‘ä¼¼é‡å¤ã€‚å¯¹äºæµ·é‡æ•°æ®ï¼Œä¸¤ä¸¤æ¯”è¾ƒï¼ˆO(nÂ²)ï¼‰æˆæœ¬è¿‡é«˜ï¼Œé€šå¸¸ä¼šé‡‡ç”¨ **MinHash + LSH (å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ)**ç­‰æŠ€æœ¯å…ˆè¿›è¡Œå¿«é€Ÿåˆ†ç»„ï¼Œå†åœ¨å°ç»„å†…è¿›è¡Œç²¾ç¡®æ¯”è¾ƒã€‚
- **å¯æ‰§è¡Œä»£ç ç¤ºä¾‹ (è¿‘ä¼¼é‡å¤æ£€æµ‹):**

```python
# å‡†å¤‡ç¯å¢ƒ:
# pip install sentence-transformers torch

from sentence_transformers import SentenceTransformer, util
import torch

# 1. åŠ è½½ä¸€ä¸ªé«˜æ•ˆçš„é¢„è®­ç»ƒåµŒå…¥æ¨¡å‹
# all-MiniLM-L6-v2 æ˜¯ä¸€ä¸ªåœ¨æ€§èƒ½å’Œé€Ÿåº¦ä¸Šå–å¾—è‰¯å¥½å¹³è¡¡çš„æµè¡Œæ¨¡å‹ã€‚
print("Loading embedding model...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("Model loaded.")

# 2. ç¤ºä¾‹æ–‡æ¡£åˆ—è¡¨ - ä½¿ç”¨2025å¹´çƒ­ç‚¹è¯é¢˜
documents = [
    "ChatGPTæ¨å‡ºäº†å…¨æ–°çš„AIåŠ©æ‰‹åŠŸèƒ½ï¼Œå¯ä»¥å¤„ç†å¤šæ¨¡æ€è¾“å…¥ã€‚",  # æ–‡æ¡£0
    "OpenAIå‘å¸ƒChatGPTæœ€æ–°ç‰ˆæœ¬ï¼Œæ”¯æŒå›¾åƒã€éŸ³é¢‘å’Œæ–‡æœ¬çš„ç»¼åˆå¤„ç†ã€‚",  # æ–‡æ¡£1 (ä¸0è¯­ä¹‰é‡å¤)
    "ä»Šæ—¥åŒ—äº¬å¤©æ°”æ™´æœ—ï¼Œæ°”æ¸©é€‚å®œã€‚",  # æ–‡æ¡£2
    "äººå·¥æ™ºèƒ½é¢†åŸŸçš„æŠ•èµ„åœ¨2025å¹´åˆ›ä¸‹æ–°é«˜ã€‚",  # æ–‡æ¡£3 (ä¸»é¢˜ç›¸å…³ä½†å†…å®¹ä¸åŒ)
    "OpenAIçš„ChatGPTæ–°å¢å¤šæ¨¡æ€åŠŸèƒ½ï¼Œå¯åŒæ—¶ç†è§£æ–‡å­—å’Œå›¾åƒå†…å®¹ã€‚",  # æ–‡æ¡£4 (ä¸0, 1è¯­ä¹‰é‡å¤)
    "ç‰¹æ–¯æ‹‰å®£å¸ƒå…¶è‡ªåŠ¨é©¾é©¶æŠ€æœ¯å–å¾—é‡å¤§çªç ´ï¼ŒFSDç³»ç»Ÿå‡†ç¡®ç‡æå‡è‡³99.9%ã€‚",  # æ–‡æ¡£5
    "Elon Muskè¡¨ç¤ºç‰¹æ–¯æ‹‰çš„å®Œå…¨è‡ªåŠ¨é©¾é©¶åŠŸèƒ½å·²æ¥è¿‘å®Œç¾ï¼Œè¯†åˆ«å‡†ç¡®ç‡è¾¾åˆ°99.9%ã€‚",  # æ–‡æ¡£6 (ä¸5è¯­ä¹‰é‡å¤)
    "æ¯”ç‰¹å¸ä»·æ ¼çªç ´12ä¸‡ç¾å…ƒå¤§å…³ï¼Œåˆ›å†å²æ–°é«˜ã€‚",  # æ–‡æ¡£7
    "åŠ å¯†è´§å¸å¸‚åœºè¿æ¥ç‰›å¸‚ï¼ŒBTCé¦–æ¬¡çªç ´120,000ç¾å…ƒã€‚"  # æ–‡æ¡£8 (ä¸7è¯­ä¹‰é‡å¤)
]

# 3. è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„åµŒå…¥å‘é‡
# .encode() æ–¹æ³•å°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸ºä¸€ä¸ªå¼ é‡(tensor)ï¼Œå…¶ä¸­æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ–‡æ¡£çš„å‘é‡è¡¨ç¤ºã€‚
print("Encoding documents...")
embeddings = model.encode(documents, convert_to_tensor=True)
print("Encoding complete. Shape of embeddings:", embeddings.shape)

# 4. ä½¿ç”¨ util.cos_sim è®¡ç®—æ‰€æœ‰å‘é‡å¯¹ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
# è¿™ä¼šè¿”å›ä¸€ä¸ª N x N çš„çŸ©é˜µï¼Œå…¶ä¸­ matrix[i][j] æ˜¯æ–‡æ¡£iå’Œæ–‡æ¡£jçš„ç›¸ä¼¼åº¦ã€‚
cosine_scores = util.cos_sim(embeddings, embeddings)

# 5. æ‰¾å‡ºå¹¶æ ‡è®°è¿‘ä¼¼é‡å¤çš„æ–‡æ¡£
threshold = 0.95  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´
duplicate_indices = set() # ä½¿ç”¨é›†åˆæ¥å­˜å‚¨å¾…åˆ é™¤çš„é‡å¤æ–‡æ¡£ç´¢å¼•ï¼Œé¿å…é‡å¤æ·»åŠ 

print(f"\nFinding duplicates with threshold > {threshold}...")
# æˆ‘ä»¬éå†ç›¸ä¼¼åº¦çŸ©é˜µçš„ä¸ŠåŠéƒ¨åˆ† (ä¸åŒ…æ‹¬å¯¹è§’çº¿)
for i in range(len(documents)):
    for j in range(i + 1, len(documents)):
        # å¦‚æœä¸¤ä¸ªä¸åŒæ–‡æ¡£çš„ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼
        if cosine_scores[i][j] > threshold:
            print(f"Found duplicate: Doc {i} and Doc {j} (Score: {cosine_scores[i][j]:.4f})")
            # å°†ç´¢å¼•jçš„æ–‡æ¡£æ ‡è®°ä¸ºé‡å¤é¡¹ (ä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„)
            duplicate_indices.add(j)

# 6. åˆ›å»ºä¸€ä¸ªå»é‡åçš„æ–‡æ¡£åˆ—è¡¨
deduplicated_documents = [doc for i, doc in enumerate(documents) if i not in duplicate_indices]

print("\nOriginal Documents:")
for i, doc in enumerate(documents):
    print(f"{i}: {doc}")

print("\nDeduplicated Documents:")
for i, doc in enumerate(deduplicated_documents):
    print(f"- {doc}")
```

### **å¯èƒ½ç¢°åˆ°çš„é—®é¢˜ï¼š**

- **é˜ˆå€¼é€‰æ‹©çš„è‰ºæœ¯ï¼š**Â è¿‘ä¼¼é‡å¤çš„ç›¸ä¼¼åº¦é˜ˆå€¼éš¾ä»¥ä¸€æ¦‚è€Œè®ºã€‚å¤ªé«˜ï¼Œä¼šæ¼æ‰å¾ˆå¤šè¯­ä¹‰é‡å¤çš„å†…å®¹ï¼›å¤ªä½ï¼Œåˆ™å¯èƒ½å°†ä¸»é¢˜ç›¸å…³ä½†å†…å®¹ä¸åŒçš„æ–‡æ¡£è¯¯åˆ¤ä¸ºé‡å¤ã€‚é€šå¸¸éœ€è¦ä»ä¸€ä¸ªè¾ƒé«˜çš„å€¼ï¼ˆå¦‚0.98ï¼‰å¼€å§‹ï¼Œé€šè¿‡äººå·¥æŠ½æŸ¥è¢«åˆ¤ä¸ºé‡å¤çš„æ ·æœ¬æ¥é€æ­¥é™ä½å’Œè°ƒæ•´ã€‚

## **æ ‡å‡†åŒ–æ–‡æœ¬æ ¼å¼ (Text Normalization)**

è¿™æ˜¯ä¸ºäº†æ¶ˆé™¤å› æ ¼å¼é—®é¢˜å¯¼è‡´çš„è¯­ä¹‰è¯¯è§£ã€‚å¯¹äºåµŒå…¥æ¨¡å‹æ¥è¯´ï¼Œ"RAG",Â "rag",Â "<p>R.A.G.</p>"Â å¯èƒ½è¢«ç¼–ç æˆè·ç¦»å¾ˆè¿œçš„å‘é‡ï¼Œå°½ç®¡å®ƒä»¬æŒ‡å‘åŒä¸€ä¸ªæ¦‚å¿µã€‚

- **å¤šè¯­è¨€ç¯å¢ƒçš„è€ƒé‡ï¼š**
    - **æŒ‘æˆ˜ï¼š**Â åœ¨å¤„ç†åŒ…å«å¤šç§è¯­è¨€çš„æ–‡æ¡£æ—¶ï¼Œå¦‚æœæ··åˆç´¢å¼•ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¯­è¨€é—´çš„è¯­ä¹‰å¹²æ‰°ã€‚
    - **ç­–ç•¥ï¼š**Â åœ¨é¢„å¤„ç†é˜¶æ®µï¼Œåº”å¼•å…¥è¯­è¨€è¯†åˆ«æ­¥éª¤ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨langdetectç­‰åº“ï¼‰ã€‚è¯†åˆ«å‡ºçš„è¯­è¨€ä¿¡æ¯åº”ä½œä¸ºä¸€ä¸ªå…³é”®çš„**å…ƒæ•°æ®**å­—æ®µè¿›è¡Œå­˜å‚¨ã€‚åœ¨æ£€ç´¢æ—¶ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„è¯­è¨€ï¼Œä¼˜å…ˆæˆ–ä»…åœ¨å¯¹åº”è¯­è¨€çš„æ–‡æ¡£å—ä¸­è¿›è¡Œæœç´¢ã€‚
- **å¯æ‰§è¡Œä»£ç ç¤ºä¾‹ (ç»¼åˆå¤„ç†å‡½æ•°):**

```python
# å‡†å¤‡ç¯å¢ƒ:
# pip install beautifulsoup4

import re
from bs4 import BeautifulSoup
import unicodedata

def normalize_text(text: str) -> str:
    """ä¸€ä¸ªå…¨é¢çš„æ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–å‡½æ•°"""
    
    # 1. ç§»é™¤HTMLæ ‡ç­¾
    # ä½¿ç”¨BeautifulSoupåº“æ¥è§£æå’Œç§»é™¤HTMLæ ‡è®°
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text(separator=" ") # ä½¿ç”¨ç©ºæ ¼ä½œä¸ºåˆ†éš”ç¬¦ï¼Œé¿å…å•è¯ç²˜è¿
    
    # 2. Unicodeå½’ä¸€åŒ–
    # 'NFKC'å½¢å¼ä¼šåˆå¹¶è®¸å¤šå…¼å®¹å­—ç¬¦ï¼Œä¾‹å¦‚å°†å…¨è§’å­—ç¬¦è½¬ä¸ºåŠè§’ï¼Œå¤„ç†è¿å­—ç­‰ã€‚
    text = unicodedata.normalize('NFKC', text)
    
    # 3. è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # 4. ç§»é™¤URLå’Œé‚®ç®± (ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    
    # 5. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ (ä¿ç•™å­—æ¯ã€æ•°å­—ã€å’ŒåŸºæœ¬æ ‡ç‚¹)
    # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼ä¼šä¿ç•™å­—æ¯(a-z), æ•°å­—(0-9), ç©ºæ ¼, ä»¥åŠ . , ! ? -
    # text = re.sub(r'[^a-z0-9\s.,!?-]', '', text)
    
    # 6. ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    # å°†å¤šä¸ªè¿ç»­çš„ç©ºæ ¼ã€æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# --- ç¤ºä¾‹æ–‡æœ¬ ---
raw_text = """
  <p>Hello World! ğŸ˜Š</p>
  Check our amazing site at https://example.com. 
  Contact us at help@example.com. 
  This is a full-width testï¼šï¼´ï¼¥ï¼³ï¼´ã€‚
"""
clean_text = normalize_text(raw_text)

print("--- Raw Text ---")
print(raw_text)
print("\n--- Cleaned & Normalized Text ---")
print(clean_text)
# è¾“å‡º: hello world! ğŸ˜Š check our amazing site at . contact us at . this is a full-width test:testã€‚
```

- **ä¸€ä¸ªå¸¸è§çš„é™·é˜±â€”â€”è¿‡åº¦æ¸…æ´—ï¼š** æ¸…æ´—è§„åˆ™å¿…é¡»æ ¹æ®é¢†åŸŸçŸ¥è¯†æ¥å®šåˆ¶ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤„ç†ä»£ç çŸ¥è¯†åº“æ—¶ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ `*`, `{`, `}`ï¼‰æˆ–è½¬æ¢å¤§å°å†™ä¼šå®Œå…¨ç ´åä»£ç çš„å«ä¹‰ã€‚åœ¨å¤„ç†é‡‘èæˆ–ç§‘å­¦æ–‡çŒ®æ—¶ï¼Œç§»é™¤é€—å·æˆ–ç ´æŠ˜å·ä¹Ÿå¯èƒ½æ”¹å˜æ•°å­—å’Œå…¬å¼çš„æ„ä¹‰ã€‚

ğŸ‘‰ **æ¸…æ´—å‰ï¼Œè¯·å…ˆæ€è€ƒä½ çš„æ•°æ®ç±»å‹**

## **è¯å¹²æå– (Stemming) ä¸è¯å½¢è¿˜åŸ (Lemmatization)**

### **ç±»æ¯”ï¼š**

- **è¯å¹²æå–ï¼š**å°±åƒä¸€ä¸ª**ç²—æš´çš„ç†å‘å¸ˆ**ï¼Œç”¨æ¨å­ï¼ˆè§„åˆ™ï¼‰ç›´æ¥æ¨æ‰å¤´å‘ï¼ˆåç¼€ï¼‰ï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œä½†å‘å‹ï¼ˆç»“æœï¼‰å¯èƒ½å¾ˆéš¾çœ‹ï¼ˆä¸æ˜¯ä¸€ä¸ªçœŸå®çš„å•è¯ï¼‰ã€‚ä¾‹å¦‚ï¼Œ"studies",Â "studying"Â ->Â "studi"ã€‚
- **è¯å½¢è¿˜åŸï¼š**åˆ™åƒä¸€ä½**ä¸“ä¸šçš„é€ å‹å¸ˆ**ï¼Œä»–ä¼šæŸ¥é˜…æ—¶å°šæ‚å¿—ï¼ˆå­—å…¸ï¼‰å’Œè€ƒè™‘ä½ çš„è„¸å‹ï¼ˆè¯æ€§ï¼‰ï¼Œä¸ºä½ è®¾è®¡ä¸€ä¸ªæœ€åˆé€‚çš„åŸå‹å‘å‹ï¼ˆè¯å…ƒï¼‰ã€‚ä¾‹å¦‚ï¼Œ"studies"Â (åè¯) ->Â "study",Â "studying"Â (åŠ¨è¯) ->Â "study"ã€‚

### **åœ¨RAGä¸­çš„æŠ‰æ‹©ï¼š**

- **å¼ºçƒˆæ¨èä½¿ç”¨è¯å½¢è¿˜åŸï¼Œç”šè‡³ä¸ä½¿ç”¨ã€‚**Â ç°ä»£çš„åµŒå…¥æ¨¡å‹æ˜¯åœ¨æµ·é‡åŸå§‹æ–‡æœ¬ä¸Šè®­ç»ƒçš„ï¼Œå®ƒä»¬æœ¬èº«å°±èƒ½å¾ˆå¥½åœ°ç†è§£runningå’Œrunä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚å¯¹è¾“å…¥è¿›è¡Œè¯å¹²æå–ï¼Œåè€Œä¼šäº§ç”Ÿrunnè¿™æ ·çš„éè‡ªç„¶è¯­è¨€è¯æ±‡ï¼Œä»è€Œâ€œæ±¡æŸ“â€è¾“å…¥ï¼Œå¯¼è‡´åµŒå…¥è´¨é‡ä¸‹é™ã€‚è¯å½¢è¿˜åŸèƒ½ä¿ç•™è¯æ±‡çš„è‡ªç„¶æ€§ï¼Œå¯¹åµŒå…¥æ¨¡å‹æ›´å‹å¥½ï¼Œä½†ä¹Ÿä¼šå¢åŠ å¤„ç†æ—¶é—´ã€‚å¯¹äºå¤§å¤šæ•°ç°ä»£RAGç³»ç»Ÿï¼Œ**æœ€å¸¸è§çš„åšæ³•æ˜¯åªè¿›è¡Œæ ‡å‡†åŒ–ï¼Œè€Œä¸è¿›è¡Œè¯å½¢è¿˜åŸæˆ–è¯å¹²æå–**ï¼Œå®Œå…¨ä¿¡ä»»åµŒå…¥æ¨¡å‹çš„èƒ½åŠ›ã€‚
- **å¯æ‰§è¡Œä»£ç ç¤ºä¾‹ (ä½¿ç”¨spaCyè¿›è¡Œè¯å½¢è¿˜åŸ):**

```python
# å‡†å¤‡ç¯å¢ƒ:
# pip install spacy
# python -m spacy download en_core_web_sm

import spacy

# åŠ è½½spaCyçš„å°å‹è‹±è¯­æ¨¡å‹
# æ¨¡å‹ä¼šè¿›è¡Œåˆ†è¯ã€è¯æ€§æ ‡æ³¨ç­‰ä¸€ç³»åˆ—NLPå¤„ç†
print("Loading spaCy model...")
nlp = spacy.load("en_core_web_sm")
print("Model loaded.")

text = "The researchers are studying the effects of studying on students' performance."
doc = nlp(text)

# token.lemma_ å±æ€§ä¼šè¿”å›è¯¥è¯çš„è¯å…ƒï¼ˆåŸºæœ¬å½¢å¼ï¼‰
lemmatized_tokens = [token.lemma_ for token in doc]

print("\n--- Original Text ---")
print(text)
print("\n--- Lemmatized Tokens ---")
print(lemmatized_tokens)
print("\n--- Lemmatized Text ---")
print(" ".join(lemmatized_tokens))
# è¾“å‡º: the researcher be study the effect of study on student 's performance .
```

