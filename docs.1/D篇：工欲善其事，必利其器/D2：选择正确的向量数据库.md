# D2: 选择正确的向量数据库 (Choosing the Right Vector DB)

## **决策的艺术：权衡的四个维度**

选择向量数据库并非单纯的技术选型，而是一场结合业务需求、团队能力、预算和未来规划的综合决策。

- **性能与功能：** 是否需要毫秒级延迟？是否需要复杂的元数据过滤和混合检索？
- **运维复杂度：** 团队是否有能力和精力去部署、维护和调优一个复杂的分布式系统？
- **成本：** 开源自建的初期硬件成本 vs. 全托管服务的长期订阅费用。
- **生态系统与社区：** 是否有活跃的社区支持？与你使用的技术栈（如LangChain）集成是否方便？

## **数据库类别详解**

- **全托管服务 (Fully-Managed Services):**
    - **代表：** Pinecone, Zilliz Cloud
    - **类比：** **入住五星级酒店**。你只需拎包入住（调用API），所有基础设施、安保、清洁（运维、扩展、调优）都由酒店方负责。
    - **优点：** 极简的开发体验，性能稳定，无需运维。
    - **缺点：** 商业闭源，有厂商锁定风险，长期成本可能较高。
- **开源云原生 (Open-Source Cloud-Native):**
    - **代表：** Milvus, Qdrant, Weaviate
    - **类比：** **自己买地建别墅**。你可以完全按自己的想法设计（深度定制），拥有完全的自主权（数据主权），但从打地基到装修水电都需要自己操心（部署和运维）。
    - **优点：** 功能强大，社区活跃，可私有化部署。
    - **缺点：** 需要自行部署、运维和调优，有一定学习曲线。
- **轻量级嵌入式 (Lightweight & Embedded):**
    - **代表：** Chroma, LanceDB
    - **类比：** **买一个高品质的帐篷**。搭建极其方便（易于集成），适合周末露营（快速原型和中小型项目），但无法抵御飓风（海量数据和高并发）。
    - **优点：** 与Python生态无缝集成，零配置启动。
    - **缺点：** 扩展性和并发性能有限。

## **可执行代码示例 (使用Chroma)**

```python
# 准备环境:
# pip install chromadb sentence-transformers

import chromadb
from sentence_transformers import SentenceTransformer

# 1. 初始化一个ChromaDB客户端。
#    - `chromadb.Client()` 创建一个临时的、内存中的数据库，关闭程序后数据会丢失。
#    - `chromadb.PersistentClient(path="/path/to/db")` 会将数据持久化到磁盘。
client = chromadb.Client()

# 2. 创建或获取一个"collection"。
#    你可以把它想象成SQL数据库中的一张表。
#    如果名为"my_rag_collection"的集合已存在，则获取它；否则，创建它。
collection = client.get_or_create_collection(name="my_rag_collection")

# 3. 准备要添加到数据库的文档
docs = [
    "The Eiffel Tower is in Paris.",
    "The Great Wall of China is a series of fortifications.",
    "Paris is the capital of France."
]
doc_ids = ["doc1", "doc2", "doc3"] # 每个文档都需要一个唯一的ID

# 4. 使用嵌入模型将文档转换为向量
print("Encoding documents...")
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(docs).tolist() # ChromaDB需要列表格式
print("Encoding complete.")

# 5. 将文档、嵌入向量和元数据添加到集合中
#    元数据是可选的，但对于过滤非常有用。
collection.add(
    embeddings=embeddings,
    documents=docs,
    metadatas=[
        {"source": "wiki", "category": "landmark"},
        {"source": "history_book", "category": "landmark"},
        {"source": "wiki", "category": "city_info"}
    ],
    ids=doc_ids
)

print("\nDocuments added to the collection.")

# 6. 执行查询
query_text = "What is the capital of France?"
print(f"\nPerforming query: '{query_text}'")

# 将查询文本也转换为向量
query_embedding = model.encode(query_text).tolist()

# 使用.query()方法进行相似性搜索
results = collection.query(
    query_embeddings=[query_embedding], # 查询向量（可以批量查询）
    n_results=2, # 指定返回最相似的2个结果
    # (可选) 在查询时进行元数据过滤
    # where={"category": "city_info"}
)

print("\n--- Query Results ---")
# 结果是一个包含多个列表的字典
for i, doc in enumerate(results['documents'][0]):
    distance = results['distances'][0][i]
    metadata = results['metadatas'][0][i]
    print(f"Result {i+1}:")
    print(f"  - Document: {doc}")
    print(f"  - Distance: {distance:.4f} (越小越相似)")
    print(f"  - Metadata: {metadata}")

# 期望输出:
# Result 1: Paris is the capital of France.
# Result 2: The Eiffel Tower is in Paris.
```