# B2：混合检索方法——集各家之所长

单一的检索方法总有其盲点。语义检索（密集）擅长理解意图，但可能忽略具体关键词；关键词检索（稀疏）能精确匹配术语，却不懂同义词。混合检索（Hybrid Search）就是将这两种方法结合，实现1 + 1 > 2的效果。

## **实施重新排序 (Rerank)——两阶段检索策略**

- **详细阐述：** 这是提升检索**精度**的“杀手锏”。想象在大海里**捞鱼**。
    1. **第一阶段（召回）：** 你先用一张**大网（ANN或BM25）**，迅速地捞起一大堆东西，里面有鱼、有虾，也有水草和石头。此阶段的目标是“快”和“全”，保证目标鱼都在网里。
    2. **第二阶段（重排）：** 然后，你在船上，用**双手（交叉编码器）**，把捞上来的东西一件一件地仔细检查，挑出真正想要的鱼，然后按照大小、品质排好序。此阶段的目标是“准”和“精”。
- **核心模型——交叉编码器 (Cross-Encoder):**
    - **工作原理：** 它将查询和文档**拼接**在一起（[CLS] query [SEP] document [SEP]），然后像一个情感分类模型一样，对这个“拼接句”的相关性进行打分。由于它能同时“看到”查询和文档的全部内容，可以捕捉到更细微的词语交互，因此其排序精度远高于标准的嵌入模型（双编码器）。
- **重排策略的实践考量：**
    - **候选集大小 (Candidate Set Size):** 召回多少文档送入重排器是一个关键的权衡。通常在 k=50 到 k=100 之间。太小，可能在召回阶段就漏掉了正确答案；太大，会显著增加重排阶段的延迟。
    - **模型的选择:**
        - **轻量级模型：** ms-marco-MiniLM-L-6-v2 等模型速度快，适合对延迟要求高的在线场景。
        - **重量级模型：** bge-reranker-large, Cohere Rerank 等模型精度更高，但计算成本也更高，可能适用于离线任务或对质量要求极高的场景。
    - **微调重排器 (Fine-tuning a Reranker)：**与微调嵌入模型类似，你也可以在自己的领域数据上微调重排器。重排器的微调通常更简单，因为它是一个分类任务（判断[query, doc]对是否相关）。你需要的数据集是 (query, relevant_doc) 和 (query, irrelevant_doc) 对。微调后的重排器能更精准地理解你所在领域的“相关性”。
- **可执行代码示例 (使用sentence-transformers的Cross-Encoder):**

```python
# 准备环境:
# pip install sentence-transformers torch

from sentence_transformers.cross_encoder import CrossEncoder

# 1. 加载一个预训练的重排模型
# ms-marco-MiniLM-L-6-v2 是一个在微软MS MARCO数据集上训练的、轻量且高效的模型。
print("Loading Cross-Encoder model...")
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
print("Model loaded.")

# 2. 假设这是第一阶段召回的结果
query = "What is the capital of France?"
# 召回的文档可能语义相关，但精确度不一
retrieved_documents = [
    "The Eiffel Tower is located in Paris.", # 相关但非直接答案
    "Berlin is the capital of Germany.", # 不相关
    "France is a country in Western Europe. Paris serves as its capital, a major global center for art and fashion.", # 最佳答案
    "Paris is a beautiful city." # 相关但信息量不足
]

# 3. 准备输入对：[查询, 文档]
sentence_pairs = [[query, doc] for doc in retrieved_documents]

# 4. 使用 reranker.predict() 计算每个对的相关性分数
# 分数越高，表示相关性越强。
print("\nCalculating reranking scores...")
scores = reranker.predict(sentence_pairs)

# 5. 将分数与文档配对，并按分数降序排序
scored_docs = sorted(zip(scores, retrieved_documents), reverse=True)

print("\n--- Results after Reranking ---")
for score, doc in scored_docs:
    # 使用 f-string 格式化输出，确保对齐
    print(f"Score: {score:9.4f}\t Document: {doc}")
```

## **使用查询扩展技术 (Query Expansion)**

用户输入的查询往往很短，或用词不一（“RAG效果不好” vs. “如何提升检索增强生成系统的准确率”）。查询扩展旨在用更丰富、更多样的表达方式来“重写”或“扩展”原始查询，以提高召回率。

- **巧妙设计——LLM即扩展器：** 利用一个LLM（可以是系统中的生成模型，也可以是一个更小更快的模型），根据用户的原始查询，生成多个变体查询。然后将原始查询和这些生成的查询**一起**发送给检索器，将所有结果汇总后进行去重和重排。

### **查询扩展的多种方法：**

- **LLM生成变体 (LLM-based Expansion):**
    - **说明：** 这是最灵活、最强大的方法。利用LLM生成多个语义相似但表述不同的查询。
    - **风险：** **查询漂移 (Query Drift)**，即生成的查询偏离了用户的原始意图。
- **Multi-Query Retriever：**这是LangChain等框架中封装好的一种策略。它使用LLM根据用户输入，一次性生成多个**不同视角**的查询。例如，对于查询“RAG的优缺点”，LLM可能生成：“RAG的优点是什么？”、“RAG的缺点是什么？”、“RAG适合什么场景？”。然后对这些子查询并行检索，合并结果。
- **引用历史查询 (Pseudo-Relevance Feedback):**
    - **说明：** 一种经典的、无需LLM的扩展技术。
    - **流程：**
        1. 用原始查询进行第一次检索。
        2. 假设排名最前的1-2个文档是高度相关的。
        3. 从这1-2个文档中提取出最重要、最独特的关键词。
        4. 将这些关键词**添加**到原始查询中，形成一个更长、更具体的新查询，然后进行第二次检索。
    - **优点：** 简单、快速，不增加外部API调用成本。