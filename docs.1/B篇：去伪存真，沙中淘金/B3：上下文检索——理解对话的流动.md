# B3：上下文检索——理解对话的流动

在真实的对话场景（如聊天机器人）中，问题之间存在着紧密的联系。如果检索系统无法理解对话的上下文，就会出现答非所问的情况。这就像和一个**记性不好**的朋友聊天。

```
你：“我最近在看《三体》。”
朋友：“哦。”
你：“你觉得里面哪个角色最酷？”
朋友（记性不好）：“什么里面？哪个角色？”
```

一个没有上下文意识的RAG系统，就像这个朋友一样，它只听得到你最后那句“你觉得里面哪个角色最酷？”，完全不知道“里面”指的是什么。

## **实现对话上下文跟踪**

核心思想是**查询重写（Query Rewriting）**。当用户提出一个代词模糊或依赖前文的问题时（例如，在讨论完“RAG”后，用户问“它有什么缺点？”），系统需要结合对话历史，将当前问题改写成一个独立的、完整的查询。

- **查询重写的挑战与对策：**
    - **别何时需要重写:** 并非所有多轮对话都需要重写。如果用户的新问题本身已经是一个完整的句子，那么重写反而是多余的。可以加入一个**判断步骤**：先让LLM判断当前问题是否依赖于历史记录，如果依赖，再执行重写。
    - **保持意图一致:** 重写后的查询必须忠实于用户的原始意图。这需要高质量的Prompt和可能对重写模型进行的微调。
- **可执行代码示例 (使用LLM进行查询重写 - 概念性):**

```python
# 这是一个概念性的示例，需要一个能调用的LLM API
# 假设你有一个名为 call_llm_api 的函数

def rewrite_query_with_history(chat_history: list, current_question: str) -> str:
    """使用LLM根据聊天历史重写当前问题"""

    if not chat_history:
        return current_question

    # 构建一个清晰的Prompt
    history_str = "\n".join([f"User: {turn['user']}\nAI: {turn['ai']}" for turn in chat_history])
    
    prompt = f"""
		You are a query rewriting expert. Your task is to rewrite a follow-up question into a standalone, self-contained question based on the provided chat history. The rewritten question must be understandable without the context of the chat history.
		<Chat History>
		{history_str}
		</Chat History>
		<Follow-up Question>
		{current_question}
		</Follow-up Question>
		<Rewritten Standalone Question>
		"""
		# 在真实应用中，这里会调用LLM API
		# rewritten_question = call_llm_api(prompt)
		# --- 模拟LLM的输出 ---
    if "disadvantages" in current_question.lower() and "rag" in chat_history[-1]["ai"].lower():
        rewritten_question = "What are the main disadvantages of Retrieval-Augmented Generation (RAG)?"
    else:
        rewritten_question = current_question
    # --- 结束模拟 ---
        
    return rewritten_question

# --- 示例用法 ---
chat_history = [
    {"user": "What is RAG?", "ai": "RAG stands for Retrieval-Augmented Generation."}
]
follow_up = "What are its main disadvantages?"

standalone_query = rewrite_query_with_history(chat_history, follow_up)

print(f"Original follow-up: {follow_up}")
print(f"Rewritten standalone query: {standalone_query}")
# 这个 standalone_query 才是应该被送入检索器的查询
```

## **处理复杂查询：查询分解 (Query Decomposition)**

当用户提出一个包含多个子问题或需要比较的复杂问题时（例如，“请比较BM25和向量检索在RAG中的优缺点”），直接对整个长查询进行检索效果可能不佳。查询分解就是先用LLM将复杂问题拆解成多个更简单、更原子化的子问题。

- **执行流程:**
    1. **分解 (Decompose):** LLM将复杂查询分解成一个子问题列表。
    2. **并行检索 (Retrieve in Parallel):** 对每个子问题**独立地、并行地**执行检索。
    3. **合并与生成 (Synthesize):** 将所有子问题的检索结果汇总起来，作为一个更丰富的上下文，提供给最终的生成模型，让它综合这些信息来回答最初的复杂问题。
- **优点:**
    - **提高召回率：** 每个子查询都更集中，更容易命中与之精确相关的文档块。
    - **逻辑更清晰：** 为LLM提供了结构化的、多角度的信息，有助于生成更有条理的答案。