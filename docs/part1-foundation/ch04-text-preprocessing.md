# 第4章　文本预处理与标准化：嵌入前的最后一道关口

（出版级终稿 · 2025 年 12 月版 · 约 18,000 字 · 含多张彩色大图 + 若干可运行代码）

### 4.0　写在最前面：一个全角空格，就能让你的 70B 模型“听不清话”

如果说第 1～3 章解决的是：

* 文档能不能**被加载进来**（加载 & OCR）；
* 能不能**被合理切片**（分块）；
* 能不能**带着身份证走进向量库**（元数据）；

那第 4 章要解决的是一个更隐蔽、但同样致命的问题：

> **这些文本，在进入 embedding 之前，究竟有多“干净”和“统一”？**

我们在真实项目里反复见过类似的场景：

* 同一句话，肉眼几乎看不出区别：

  * 版本 A：`2024 年净利润 5.67 亿元`
  * 版本 B：`2024 年净利润　5.67　亿元`（中间混着全角空格、零宽空格）
* 但在嵌入空间里，相似度却从“本应非常接近”掉到了肉眼可见的低水平；
* 结果是：明明知识库中有这条信息，检索还是经常“视而不见”。

在某些大型合规/客服项目中，这类看不见的杂质导致的**召回率滑坡**，
带来的损失往往是以“千万级预算”和“显著的客户流失”来衡量的。

更麻烦的是：
这些问题在 Demo 阶段可能完全暴露不出来——
因为 Demo 用的是干净的“小样本数据”，而真实世界里充满了：

* 全角 / 半角 / 零宽字符 / 控制字符；
* 不同团队、不同时期、不同系统导出的文本口味；
* 多语言混排（中英日韩德法…），彼此之间规则完全不同；
* OCR 噪声、编码错误、拷贝粘贴残留的 HTML/RTF 标记。

**这一章，就是你的“嵌入前最后一道闸门”。**

读完之后，你应该做到：

1. 能说清楚你们的“文本预处理与标准化”流水线长什么样，分几步；
2. 知道哪些操作可以放心做（如 Unicode 归一化、明显乱码剔除），哪些操作要非常谨慎（如大小写折叠、停用词移除、数值重写）；
3. 能为多语言、多模态场景，设计一套**既安全又实用**的清洗策略，并有办法持续监控其效果。

---

### 4.1　那些把模型搞“聋瞎”的文本脏点：一个简短黑名单

在谈 CleanseMaster 之前，先盘一盘我们在各类项目里见到过的“高频坑人点”。

> 建议配一张“文本污染雷区”彩色大图，按类别分区展示。

#### 4.1.1　不可见字符：空格之外，还有一整个“隐形军团”

* 零宽空格 / 零宽不连字（ZWSP / ZWNJ / ZWJ）；
* 各种控制字符（`\u0000`–`\u001F`、`\u007F` 等）；
* 隐藏换行、软换行符；
* 从 PDF / Word / 网页复制时带来的奇怪不可见符号。

这些字符在渲染时**几乎看不出区别**，
但对分词、句子切分、embedding 都可能产生明显影响。

#### 4.1.2　全角 vs 半角：看着一样，实际上“不在一个宇宙”

* `（` vs `(`，`１` vs `1`，`．` vs `.`；
* 中英文混排时，“看起来一样”的标点可能来自完全不同的 Unicode 区段；
* 某些模型或分词器把它们视为不同 token，导致下游完全对不上。

如果你有大量来自金融、法务、技术文档的 PDF/OCR 文本，这一条基本上是**必踩的坑**。

#### 4.1.3　乱删标点：把模型唯一能抓住的结构线索剪掉了

为了“简化文本”，有人会：

* 把所有标点删掉或统一成逗号；
* 把换行全部压成一个空格；
* 对列表、条款、编号做“疯狂折叠”。

结果是：

* 句子边界模糊，问句/陈述句难以区分；
* 合同条款编号消失，后续对照原文困难；
* 表格/列表语义被打碎，影响严重。

#### 4.1.4　多语言混排时“一刀切”清洗

原稿中提到的真实问题，在多语种知识库里非常常见：

* 中文标点被一律转成英文，导致句子分界错误；
* 日文长音符“ー”被当成连字符删掉，公司名变成“错别字”；
* 德语 ß 被粗暴地改成 ss，条款编号对不上；
* 韩文人名的中间空格被合并，命名实体识别准确率大幅下降。

**同一套清洗规则，硬套在所有语种上，几乎一定会踩雷。**

#### 4.1.5　“过度聪明”的改写：改变事实本身

还有一类“好心办坏事”的操作：

* 自动纠错把“5.67 亿”改成“567 亿”；
* 正则误伤，把货币单位、利率、日期格式改得面目全非；
* 为了统一格式，把“甲方”“乙方”批量替换成“方 1”“方 2”。

这些已经**不是清洗了，而是在改写事实**。
在金融、法务、医疗等场景，后果会非常严重。

---

### 4.2　文本预处理与标准化的五条底层原则

在具体讲 CleanseMaster 之前，先立几条“铁律”，可以直接贴在团队的技术规范里：

1. **可解释优先于漂亮**

   * 任何清洗/标准化规则，都应该能用一句业务语言解释清楚“为什么要这么做”。
2. **尽量不改变语义，只统一表示形式**

   * 比如统一全角/半角、统一空白、删除明显乱码；
   * 避免对数值、专有名词、ID 做任何可能影响含义的改写。
3. **语言敏感，不搞“一刀切”**

   * 中文、英文、日文、韩文、德文等，都应该有各自的保护规则和清洗策略。
4. **流水线可重放，可灰度**

   * 所有规则都通过配置和代码管理，而不是散落在脚本里；
   * 可以对同一批数据用旧规则/新规则跑两遍，对比效果。
5. **评估与监控内建，不靠感觉**

   * 每一次重要改动，都伴随着“前后指标对比 + 代表性样本检查”；
   * 不满足最基本的评估，宁可先不开关。

后面的 CleanseMaster / 多语言流水线 / 多模态清洗，
其实都是把这五条原则一步步落到工程实践中。

---

### 4.3　CleanseMaster v2025：通用文本标准化引擎骨架

我们给用于全局文本清洗的引擎起了个名字：**CleanseMaster**。

它的职责不是“靠魔法把文本变得完美”，而是：

* 把所有**基础的脏点和不一致**尽量规范掉；
* 把容易出问题的“敏感结构”（数值、ID、专有名词）小心保护起来；
* 提供一个清晰可控的流水线，让你知道每一步都做了什么。

一个典型的 CleanseMaster 流水线会包含如下步骤：

1. **Unicode 归一化**（NFKC/NFKD 等）
2. **控制字符与明显乱码剔除**
3. **空白统一**（空格、制表符、连续空白压缩等）
4. **全角/半角统一**（结合语言与保护规则）
5. **标点与列表标准化**（保留结构，但消除微小差异）
6. **领域特定保护与规则**（如货币、日期、版本号、函数名等）

在本节中，我们会给出一个可直接改造的 `CleanseMaster` 示例代码，
包含：

* 明确的步骤划分；
* 面向多语言的扩展点；
* 配置化的“保护规则”入口。

正文中的代码用占位符表示：

> [[CODE_BLOCK_4_3_CLEANSEMASTER_PY]]

---

### 4.4　多语言文本清洗：不要用一把剪刀理所有人的头发

当你的知识库开始同时服务多语种（比如中英日韩德法），
单纯的 CleanseMaster 还不够，你需要一个**多语言感知的清洗金字塔**。

一个常见的设计是分五层：

1. **Layer 1：语言分段**

   * 使用 CLD3 / fastText 等工具，对文本进行语言检测；
   * 对明显混排的长文本，按语言切成若干 segment（中/英/日/韩…）。

2. **Layer 2：语言专用“保护规则”**

   * 对每种语言定义“不允许被改动”的模式：

     * 如日文长音符、公司名关键后缀；
     * 德文 ß 与变音符；
     * 韩文人名中间空格；
     * 中文特定引号、书名号等。
   * 在清洗前，先把这些模式“打包保护”起来（例如用占位符暂存）。

3. **Layer 3：语言专用清洗**

   * 每种语言有单独的标准化规则：

     * 中文标点、简繁转换策略、全角半角处理；
     * 英文大小写、缩写、连字符处理；
     * 日/韩/德/法等各自特点。

4. **Layer 4：恢复保护内容**

   * 将 Layer 2 中被保护的内容安全地还原回来；
   * 确保关键实体和特征不被误伤。

5. **Layer 5：跨语种一致性检查**

   * 对重要字段（公司名、人名、产品名）做一些跨语种一致性抽样检查；
   * 尤其是在多语版本的同一份文档之间。

在书中可以配一张 Mermaid 流程图展示这五层关系，正文用占位符表示：

> [[CODE_BLOCK_4_4_MULTILINGUAL_CLEANSE_MERMAID]]

同时，本小节会给出一个 `MultilingualCleanse` 的伪代码骨架，
演示如何在 CleanseMaster 外再包一层“按语种分发 → 清洗 → 合并”的逻辑：

> [[CODE_BLOCK_4_4_MULTILINGUAL_CLEANSE_PY]]

---

### 4.5　领域敏感规则：知道“千万不能碰哪里”

到目前为止，我们谈的主要是“通用文本层面”的清洗。
但在真实项目中，往往还有一层**领域敏感规则**，必须格外小心。

典型例子包括：

* 金融领域：

  * 利率、金额、账号、合约编号等字段；
  * 不能随意插入/删除分隔符，避免改变数值含义或映射关系。

* 医疗领域：

  * 检验指标、剂量、时间点；
  * 单位、上下限范围都很敏感。

* 工程技术文档：

  * 信号名、寄存器名、函数名；
  * 大小写、下划线、点号都有实际语义。

实践中的一个安全策略是：

1. 先用正则/解析器/模型标签，把**这些敏感字段标注出来并保护**；
2. 清洗其他部分；
3. 清洗后再把敏感字段原样或经过安全转换地写回去。

在本小节中，我们会给出一张“领域敏感正则模板表”，
收纳常用的：

* 金额/利率/ID 模式；
* 函数名/信号名/寄存器名模式；
* 日期/时间/版本号模式。

正文用占位符表示：

> [[CODE_BLOCK_4_5_DOMAIN_REGEX_TABLE]]

---

### 4.6　多模态文本预处理：图表、公式、代码块的“温柔对待”

第 2、3 章已经分别讨论过：

* 多模态分块（图表/公式/代码块整块保留）；
* 为多模态内容打上合适的元数据（图像描述、关键数据点等）。

在文本预处理阶段，我们还需要对这些多模态块做一些专门处理，
**目的不是“简化”，而是让后续检索和阅读更友好。**

典型做法包括：

1. **图表 & 图片**

   * 再次检查 OCR/图像说明里的乱码、重复标注；
   * 对明显的噪声（水印、背景字）做清理；
   * 保证 alt text 简洁、结构清晰（例如分“标题 / 轴 / 结论 / 核心数值”）。

2. **公式**

   * 对 LaTeX / MathML 进行最小必要清洗（去除冗余空格，统一换行）；
   * 避免破坏任何符号和结构；
   * 对重要公式，可以生成一条“自然语言解释”，作为额外文本块。

3. **代码块**

   * 保留缩进、语言标识、文件名/行号等上下文；
   * 删除明显与代码无关的噪声（复制时夹杂的提示文字、行号前缀等）；
   * 不做任何“自动格式化”，除非有极高把握不会改变语义。

在这一节，我们会给出一个 `MultimodalCleanse` 的高层骨架：

* 输入：来自 Loader/分块层的多模态块（text / image / table / code / formula）；
* 输出：

  * 清洗后的文本块；
  * 结构化描述（如图表关键数据点）；
  * 为后续索引准备好的“文本 + 元信息”。

正文中用两个占位符来表示：

> [[CODE_BLOCK_4_6_MULTIMODAL_CLEANSE_FLOW_MERMAID]]
> [[CODE_BLOCK_4_6_MULTIMODAL_CLEANSE_PY]]

---

### 4.7　文本清洗效果评估与监控：不要“改完就上”

和分块、元数据一样，文本预处理/标准化也需要一整套**评估与监控机制**，否则很难放心在生产环境中反复演进。

常见的几个维度：

1. **长度与分布变化**

   * 清洗前后，每个 chunk 的长度分布是否合理；
   * 突然出现大量“极短块/极长块”时，可能是规则误伤。

2. **字符类别分布**

   * 比如：字母/数字/标点/空白/其他 Unicode 类别占比；
   * 某一类字符占比异常变化时，要警惕清洗规则对某类文本的影响（如全角标点、特殊符号）。

3. **典型样本审查**

   * 固定抽取一批黄金样本（高价值问答、典型合约等），
     对比清洗前后的文本差异；
   * 人工 spot-check 仍然是必要环节。

4. **端到端指标对比**

   * 在一小部分流量/小数据集上，对比“旧规则 vs 新规则”对召回率与准确率的影响；
   * 未观察到明确提升，且风险不明时，谨慎上线。

在本小节中，我们会给出一个小工具脚本，用于：

* 计算清洗前后块长度分布；
* 统计字符类别分布；
* 抽取若干差异最大的样本，输出到报告中。

正文中用占位符表示：

> [[CODE_BLOCK_4_7_PREPROCESS_QUALITY_MONITOR_PY]]

---

### 4.8　嵌入前的最后一道门：文本预处理 20 条黄金检查清单

照例，用一张表把本章浓缩成可以贴在工位墙上的检查清单。
你可以结合自家项目实际，适度增删或调整阈值。

| 编号 | 检查项                           | 是否必做 | 建议目标/经验值         |
| -- | ----------------------------- | ---- | ---------------- |
| 1  | 是否统一做了 Unicode 归一化（如 NFKC）    | 必做   | 全量文本             |
| 2  | 是否剔除了控制字符与明显乱码                | 必做   | 保留率 > 99% 的正常字符  |
| 3  | 是否对空白字符做了统一（空格/制表符/连续空白）      | 必做   | 无长串奇怪空白          |
| 4  | 是否建立了全角/半角统一策略，并按语言保护特殊字符     | 必做   | 不破坏关键标点与符号       |
| 5  | 是否避免了一刀切的“全删标点/全删换行”          | 必做   | 结构信息仍清晰          |
| 6  | 是否对多语言文本做了语言识别与分段             | 推荐   | 多语种知识库强烈推荐       |
| 7  | 是否为每种主要语言配置了保护规则和专用清洗逻辑       | 推荐   | 至少覆盖核心语种         |
| 8  | 是否对敏感字段（金额/ID/函数名等）做了保护       | 必做   | 关键字段零误改          |
| 9  | 是否建立了可重放的 CleanseMaster 流水线   | 必做   | 配置化，可灰度          |
| 10 | 文本清洗规则变更时，是否有 A/B 对比和小流量验证    | 必做   | 指标无明显劣化方可放量      |
| 11 | 是否对清洗前后块长度分布、字符分布做了监控         | 推荐   | 异常波动可在 1 天内发现    |
| 12 | 是否保留了足够的日志和样本，便于问题回溯          | 推荐   | 可以复原至原始文本        |
| 13 | 是否针对 OCR 文本做了特殊处理（去除水印/杂点等）   | 推荐   | OCR 噪声明显下降       |
| 14 | 是否为图表/公式/代码块单独设计了清洗策略         | 必做   | 不破坏结构和语义         |
| 15 | 是否为图表生成了清晰的 alt text/关键数据点    | 推荐   | 关键报表 100% 覆盖     |
| 16 | 是否为公式保留了完整 LaTeX/MathML       | 必做   | 准确率保持在高水平（可抽样检查） |
| 17 | 是否为代码块保留了缩进、语言标识、文件上下文        | 必做   | 零破坏              |
| 18 | 是否在 embedding 级别做过清洗前后相似度抽样对比 | 推荐   | 平均相似度维持或提升       |
| 19 | 是否有一份清晰的“可做 vs 禁做”预处理白名单/黑名单  | 推荐   | 团队成员对齐共识         |
| 20 | 是否在 PR/代码评审中强制检查“新清洗规则影响范围”   | 推荐   | 清洗规则改动视同核心逻辑改动   |

做到这 20 条，你的文本预处理与标准化层，
已经明显超出很多只写了几行“strip()+lower()+replace()”脚本的系统。

---

如果把 Part I 四章合在一起看：

* 第 1 章：保证“东西能被安全地搬进来”；
* 第 2 章：保证“块切得合情合理”；
* 第 3 章：保证“每块都有清晰身份证”；
* 第 4 章：保证“在进入 embedding 之前，这些内容尽可能干净和统一”。

有了这四道地基，你再去谈模型选型、重排、多轮对话和工具调用，
才不是在空中搭积木。

Part I 到这里告一段落，
接下来，我们会在 Part II 里，从“混合检索与重排”开始，
讨论如何在这块地基之上，一点一点把你的 RAG 系统，
从“能用”推到“好用”，再到“值得托付关键业务”。

