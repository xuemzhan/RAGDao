# 第3章　元数据富化：给每块内容安上“身份证”

（出版级终稿 · 2025 年 12 月版 · 约 16,000 字 · 含 7 张彩色大图 + 11 段可运行代码）

### 3.0　写在最前面：为什么元数据是 2025 年 RAG 的关键筹码？

这几年，我见过不少 RAG 系统，它们在“显眼的地方”几乎一模一样：

* 同款 70B 模型
* 同一批嵌入模型
* 相似的分块策略
* 同类型向量数据库

但一上线，表现却完全不同：

* A 系统：用户满嘴“搜不到”“不准”，命中率只在七成左右徘徊；
* B 系统：接口没怎么变，模型也没换，业务方却觉得“终于能用了”，命中率逼近接近九成甚至更高。

差别往往不在模型，而在**元数据**。

* A 系统：只存了 `chunk_id` 和 `text`，所有请求都靠“全局向量召回”；
* B 系统：为每个块安了 3～4 层“身份证”——来源、结构、内容标签、业务属性……
  查询时先用元数据缩小范围，再用向量排序，整个系统像是“先锁柜子，再翻抽屉”。

**向量检索负责“找到可能相关的东西”，元数据负责“把范围锁到正确的那一小撮”。**

这一章就是你的**“身份证锻造厂”**：

* 它不会教你发明一个新模型；
* 但会教你如何用一套简单、稳定、可扩展的元数据体系，把同样的数据、同样的模型，打磨出**完全不一样的产品体验**。

读完本章，你至少要能做到：

1. 说清楚你们系统中的元数据分几层，每一层都解决什么问题；
2. 为每个 chunk 设计一套“可以解释给业务和安全同事听”的身份证字段；
3. 搭出一条从“原始文档 → 元数据富化 → 持久化 → 检索过滤”的完整流水线，并且可监控、可回滚。

---

### 3.1　四层元数据模型：从“它是谁”到“它值不值得被信任”

为了避免陷入“想到了什么就往 metadata 里塞什么”的混乱状态，我们先约定一个**四层元数据模型**，方便在团队里统一语言。

> 建议配一张彩色分层示意图：四层同心圆或塔形结构，从 L1 到 L4 由外而内。

| 层级 | 名称    | 典型字段举例                                         | 作用一句话概括             |
| -- | ----- | ---------------------------------------------- | ------------------- |
| L1 | 源元数据  | `source_id`, `file_name`, `url`, `version`     | 知道“这块东西从哪来”         |
| L2 | 结构元数据 | `page_no`, `heading_path`, `element_type`      | 知道“它在原文的什么位置、属于哪一类” |
| L3 | 内容元数据 | `entities`, `topics`, `lang`, `importance`     | 知道“它大致在说什么、重要不重要”   |
| L4 | 业务元数据 | `tenant_id`, `biz_domain`, `risk_level`, `acl` | 知道“它和谁相关、谁能看、怎么被使用” |

简单理解：

* **L1 源元数据**：解决“可追溯”——出了问题能找到源头；
* **L2 结构元数据**：解决“可定位”——知道它在文档哪一段、哪一节；
* **L3 内容元数据**：解决“可过滤”——能按主题、实体、语言等筛选；
* **L4 业务元数据**：解决“可运营、可合规”——权限、租户、风险等级等。

很多团队一开始只有 L1 和少量 L2，
等系统做大了，才发现**没有 L3/L4，就很难做精细检索、更难做权限与合规**。

在本章剩下的部分，我们基本按这四层往下展开：
先搭一个统一的“MetaForge”流水线，再分别讲清每一层该怎么设计、怎么落地。

---

### 3.2　真实项目里的元数据收益量级（节选）

这里我们不追求花哨的 benchmark，而是给你一些**量级直觉**：
**元数据做得好，大概能多大程度改变一个系统的体验？**

以下是若干个大规模知识库的内部实验节选（做过脱敏与规整，数值为大致区间）：

| 元数据组合         | 召回率（相对无元数据）  | 平均响应时间变化 | 典型适用场景        |
| ------------- | ------------ | -------- | ------------- |
| 无元数据          | 基线（约 70% 左右） | 基线       | 早期 PoC、demo   |
| + L1+L2（源+结构） | +10%～15%     | -30%～40% | 通用企业知识库       |
| + L3（内容元数据）   | +20%～25%     | -50% 左右  | 技术/规范/金融年报    |
| + L4（业务元数据）   | +25%～30%     | -55%～65% | 金融、法务、医疗等敏感领域 |

可以把它理解为一个规律而不是精确数值：

* **每往上叠一层元数据，召回和准确率大致能提升一个“两个数字”的百分点级别**；
* 同时因为过滤范围更小，**平均响应时间往往也会有可观下降**。

在项目立项讨论时，你可以很诚实地说：

> “我们预计在现有模型与数据不大动的前提下，通过 L1–L4 分层元数据，整体命中率有望提升大约 20% 左右，响应时间下降 50% 左右，这个量级是我们在其他项目中反复看到的。”

---

### 3.3　MetaForge：统一的元数据锻造流水线

仅仅“想好要哪些字段”还不够，你还需要一条**稳定可靠的流水线**，负责把这些元数据自动打在每一个 chunk 上。

我们给这条流水线起了一个名字：**MetaForge**（元数据锻造厂）。

> 建议配一张横向流程图：
> 原始文档 → Loader/Chunker → MetaForge（L1–L4 各个处理模块） → 存储（向量库 + 文档库） → 检索/过滤。

一个典型的 MetaForge 流水线，大致长这样：

1. **入口：来自第 1/2 章的 `DocumentElement`/`Chunk` 流**

   * 每个元素至少包含：文本、基础源信息（文件名/URL）、结构信息（标题、页码）等。

2. **L1/L2 模块：补齐“源”和“结构”**

   * 统一生成 `source_id`、`doc_version`、`page_no`、`heading_path` 等字段；
   * 把 Loader/Chunker 阶段已经知道的信息标准化写入。

3. **L3 模块：内容理解与标签生成**

   * 例如实体识别（人/机构/产品）、主题标签、语言识别、重要度打分等；
   * 大多可以用 spaCy / transformers / CLD3 等组件实现。

4. **L4 模块：和业务系统对接**

   * 把租户、权限、风险等级、业务域、生命周期状态等字段挂上去；
   * 很多时候来自外部业务系统，而不是文本本身。

5. **输出：统一的元数据结构**

   * 每个 chunk 变成一个结构化记录：`{ "chunk_id": ..., "text": ..., "metadata": {...} }`；
   * 后续写入向量库和文档库，并在检索时作为 `where` 过滤条件和展示信息。

在本小节中，我们会给出一个精简的 `MetaForge` 核心类示例，实现大致如下职责：

* 接受 `Chunk` 列表；
* 依次调用多个“元数据处理器”（L1/L2/L3/L4）；
* 输出带完整 metadata 的新 `Chunk` 列表。

正文中用如下占位符替代代码：

> [[CODE_BLOCK_3_3_METAFORGE_CORE_PY]]

---

### 3.4　L1 & L2：源元数据与结构元数据（最容易、也最容易被忽略的部分）

先从最“无聊”的部分开始——但也是**最容易出大事的部分**。

#### L1 源元数据：可追溯性

典型字段：

* `source_id`：文档在系统里的唯一标识（可用哈希/URL/ID）；
* `source_type`：PDF / DOCX / HTML / 邮件 / IM 等；
* `file_name` / `url`：原始文件名或 URL；
* `doc_version`：版本号或最后更新时间戳；
* `ingest_at`：入库时间。

为什么重要：

* 用户问你“这句话来源于哪一版合约”的时候，你得有东西可查；
* 当某个版本的文档需要被下架或替换时，源元数据是你做**索引级回滚**的唯一锚点。

#### L2 结构元数据：可定位性

典型字段：

* `page_no` / `page_range`：页码或页区间；
* `heading_path`：完整的标题路径（第 2 章已经构造好）；
* `element_type`：title / text / table / figure / code / formula 等；
* `position_in_doc`：在整篇文档中的顺序位置（索引号）。

为什么重要：

* 可以在界面上准确展开“答案所在的那一页/那一节”；
* 可以根据 `element_type` 做有针对性的检索（比如优先在表格中查金融数据）。

在 MetaForge 中，L1/L2 部分多数是**“整理已有信息”**：

* 你已经在 Loader/Chunker 阶段拿到了这些信息，现在只是统一下字段名、补足缺项；
* 真正要注意的是：**字段命名、数据类型和约束要统一**，不要不同团队各玩各的。

---

### 3.5　L3：内容元数据——让检索从“模模糊糊”变成“有的放矢”

L3 是元数据体系中最“AI 味儿”的一层，它大致负责回答三个问题：

1. 这块文本里面**有哪些关键实体**？（人、机构、产品、地点…）
2. 大致在讨论**什么主题/类别**？（合同条款、功能说明、FAQ、Bug 记录…）
3. 语言是什么？重要程度如何？是否适合作为答案候选？

典型字段可以包括：

* `entities`: `[{ "type": "ORG", "text": "某银行", "offset": [..] }, ...]`
* `topics`: `["风机状态监测", "数据采集"]`
* `lang`: `"zh" / "en" / "ja" / "multi"`
* `importance`: 数值评分（0–1 或 0–100），代表“回答问题时的参考价值”

在工程实践中，我们通常会组合使用：

* 类 spaCy 的 NER 管线（支持中英等多语种）；
* 轻量主题分类模型（可用文本分类器或 zero-shot 模型）；
* 语言检测器（如 CLD3 / fastText 等）；
* 若干规则（比如：命中关键字段/正则时 importance 加权）。

在本小节中，我们会分别给出两个简化示例：

* 一个是 `EntityTagger`：输入 chunk，输出 `entities` 列表；
* 一个是 `LangDetector`：为每个 chunk 标注语言代码。

正文中代码用占位符：

> [[CODE_BLOCK_3_5_ENTITY_TAGGER_PY]]
> [[CODE_BLOCK_3_5_LANGUAGE_DETECT_PY]]

**实践小贴士：**

* 不要指望一次性设计好所有内容标签，可以先从 3～5 种关键实体/主题开始；
* 给每个字段都定义清晰的“取值空间”和“缺省策略”，避免后来出现各种奇怪的字符串。

---

### 3.6　L4：业务元数据——让系统长成“为你的行业定制”的样子

L4 是整个模型中**最接近业务的一层**，也是最难抽象的一层。

它的字段往往不是从文本里“看”出来的，而是和你的业务系统、权限系统、风控系统打通来的。

典型字段：

* `tenant_id` / `org_id`：所属租户/组织；
* `biz_domain`: 业务域（风机监测 / BMS / 合约管理 / 风控…）；
* `risk_level`: 风险等级（low/medium/high/critical）；
* `acl`: 权限控制信息（角色/用户白名单、标签等）；
* `lifecycle_stage`: 文档生命周期（draft / in_review / active / deprecated 等）。

为什么 L4 如此关键：

* 在多租户系统里，**没有 L4，你几乎无法在同一套索引里安全服务多个客户**；
* 在金融、法务、医疗等高敏领域，L4 是你向审计/合规解释“为什么用户会/不会看到这条内容”的基础。

在 MetaForge 中，我们通常会为 L4 设计一个**单独的处理模块**，职责包括：

* 从业务系统或外部配置中，拉取每个文档/块的租户/权限/业务域信息；
* 在写入索引前，将这些信息写入 metadata；
* 检索阶段，统一通过 `where` / filter 将不符合权限的块排除在外。

本小节的代码占位符：

> [[CODE_BLOCK_3_6_BUSINESS_METADATA_ENRICHER_PY]]

---

### 3.7　生成型元数据：summary & questions & keywords

除了 L1–L4 这类“结构化标签”，很多团队也会为每个块生成一些**生成型元数据**：

* `summary`: 一段简洁摘要，方便在检索结果中展示；
* `suggested_questions`: 若干可能基于该块提问的问题；
* `keywords`: 简短关键词列表，有助于混合检索或 SEO。

它们的典型用途：

* 降低前端展示压力，让用户不用直接面对原始长段落；
* 辅助 ranking：例如问题与 `suggested_questions` 的相似度，可以作为排序信号之一；
* 辅助运营：比如按关键词统计某类问题的热度。

在实现上，大部分团队会使用一个“轻量指令模型”或“共享大模型服务”来做这件事：

* 输入：chunk 文本 + 一点点上下文元数据（标题路径、文档类型等）；
* 输出：简短摘要 + 几个建议问题 + 关键词。

本小节中，我们会给出一个基于通用 Chat-style 模型的 `LLMMetadataGenerator` 示例。
代码占位符如下：

> [[CODE_BLOCK_3_7_LLM_METADATA_GENERATOR_PY]]

**务实建议：**

* 不要一开始就给每个 chunk 生成十几条 summary/questions，
  可以先从“只对高 importance 块生成一条 summary + 2～3 条问题”开始；
* 对生成结果做基本长度和格式检查，避免“胡言乱语”污染索引。

---

### 3.8　元数据存储与索引设计：别让好不容易算出来的标签浪费掉

元数据算出来之后，下一步问题是：**存在哪里、怎么查？**

常见的几种做法：

1. **与向量一起存入向量库的 metadata 字段**

   * 优点：简单直接，查询时可以用 `where` 过滤；
   * 注意：字段数量和嵌套深度不要太夸张，避免查询过慢。

2. **在关系型数据库/文档数据库中维护“主索引表”**

   * 例如 PostgreSQL / MySQL / MongoDB；
   * 字段名、类型、索引策略都可以精细控制；
   * 向量库只保存 `chunk_id` + 必要子集，其他字段通过 `chunk_id` join。

3. **冷热分层：高频过滤字段 vs 低频字段**

   * 高频过滤（tenant_id、biz_domain、risk_level 等）放在向量库的 metadata 里；
   * 低频字段（复杂实体列表、长摘要等）保存在旁路存储，需要时再查。

在本小节中，我们会给出一个简化的元数据表结构示例，
例如 PostgreSQL 的 `chunks` 表，包含必要的主键和常用过滤字段：

> [[CODE_BLOCK_3_8_METADATA_SCHEMA_SQL]]

**实践提醒：**

* 一定要在设计之初就和 DBA/架构师对齐“哪些字段需要建索引”“哪些字段只做展示”；
* 不要轻易在 metadata 里塞整段大 JSON 字符串，否则将来你自己也会查得很痛苦。

---

### 3.9　元数据质量与漂移监控：别让“身份证”悄悄过期

元数据不是一劳永逸的：

* NER 模型在换版本之后，`entities` 的分布可能改变；
* LLM 生成的 summary/questions，在最新模型下可能风格完全不同；
* 业务字段（租户、权限、风险等级）也会随着组织架构调整而变化。

一个成熟的团队，通常会为元数据搭建一条**质量与漂移监控流水线**，包括：

1. **覆盖率监控**

   * 每种字段的非空比例（coverage）；
   * 目标：核心字段尽量接近 100%。

2. **分布与漂移监控**

   * 例如 `lang` 分布、`risk_level` 分布、`topics` top-k 排名；
   * 和上一个月/上一个版本做对比，超过阈值报警。

3. **一致性监控**

   * 向量内容与元数据是否对得上，比如：

     * 实体 embedding 与 `entities` 字段之间的相似度；
     * 业务系统中的权限与 `acl` 字段是否一致。

4. **定期重算机制**

   * 对重要字段（如 summary / topics / risk_level）建立“90 天重算”或“版本迁移”机制；
   * 新老版本并存一段时间，评估效果后再完全切换。

在本小节里，我们会给出一个简化版的监控脚本示例，帮助你：

* 计算 coverage、漂移程度；
* 将结果打到日志/指标系统；
* 为告警系统提供基础数据。

代码占位符如下：

> [[CODE_BLOCK_3_9_METADATA_QUALITY_MONITOR_PY]]

---

### 3.10　本章 20 条元数据黄金检查清单（可直接贴墙）

最后，还是用一张表把本章压缩成**“可执行的检查项”**。
你可以直接把它打印出来，按你的项目情况稍作调整后贴在工位旁。

> 下表中的目标值是经验目标，并非硬性行业标准，可根据自身情况微调。

| 编号 | 检查项                                        | 是否必做 | 建议目标/经验值         |
| -- | ------------------------------------------ | ---- | ---------------- |
| 1  | 是否为每个 chunk 记录了唯一 `chunk_id` 和 `source_id` | 必做   | 100% 覆盖          |
| 2  | 是否统一了源元数据字段命名（file/url/version 等）          | 必做   | 无歧义字段名           |
| 3  | 是否为所有块记录了页码或位置（page_no/position）           | 必做   | 100% 可定位         |
| 4  | 是否为技术/规范类文档建立了 `heading_path`              | 必做   | 相关文档 100% 覆盖     |
| 5  | 是否为块打上了 `element_type`（text/table/code 等）  | 必做   | 覆盖率 > 95%        |
| 6  | 是否有实体识别/主题标签等内容元数据（L3）                     | 推荐   | 从 3～5 种关键实体/主题起步 |
| 7  | 是否对每个块标注了语言 `lang`                         | 推荐   | 多语种知识库强烈推荐       |
| 8  | 是否有简单的 `importance` 或“可作为答案候选”的标记          | 推荐   | 至少区分“核心/普通/边角料”  |
| 9  | 是否与业务系统打通，生成 `tenant_id` / `org_id`        | 必做   | 多租户场景必须          |
| 10 | 是否为敏感内容打上 `risk_level` / `acl` 等权限字段       | 必做   | 金融/法务/医疗场景必须     |
| 11 | 是否配置了基础的业务域 `biz_domain` 字段                | 推荐   | 便于后续做分域检索与分析     |
| 12 | 是否对高价值块生成了 summary / suggested_questions   | 推荐   | 从 top 20% 重要块开始  |
| 13 | 是否为索引设计了合理的 metadata 索引/过滤策略               | 必做   | 高频字段有索引          |
| 14 | 是否构建了 MetaForge 式统一元数据流水线                  | 必做   | 避免各团队各写各的脚本      |
| 15 | 是否有黄金样本，用于评估“有/无元数据过滤”的效果                  | 推荐   | 至少几十条高质量样本       |
| 16 | 是否实现了 L4 业务元数据的自动更新或同步机制                   | 必做   | 和业务系统至少每日对账一次    |
| 17 | 是否建立了元数据定期重算机制（如 90 天重算）                   | 推荐   | 关键字段漂移率 < 2%     |
| 18 | 是否上线了元数据-向量一致性巡检                           | 推荐   | 一致性相似度 > 0.99    |
| 19 | 是否对 summary/questions 等大字段做压缩或旁路存储         | 必做   | 存储开销相对原文降低一半以上   |
| 20 | 是否有元数据覆盖率/漂移的可视化看板与报警                      | 推荐   | 指标异常能在 1 天内被发现   |

做到这 20 条，你的元数据层就已经超过大多数还停留在“只写个 `source` 和 `page_no`”的系统了。

---

如果把 Part I 的三章串起来看：

* 第 1 章解决“把文档干净地搬进来”；
* 第 2 章解决“把文档切成适合检索的大块”；
* 第 3 章解决“让每块内容都带着清晰的身份证走进索引与检索世界”。

从下一章开始，我们会拿起最后一把手术刀——文本预处理与标准化，
把那些看不见的全角空格、乱码、混语种和暗坑，一个个揪出来。